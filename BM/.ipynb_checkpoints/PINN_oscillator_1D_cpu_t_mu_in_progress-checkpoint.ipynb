{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5338c89-4d5c-4a33-ac71-50e4c7e0cd7f",
   "metadata": {},
   "source": [
    "PINN_oscillator_1D_cpu_t_mu_in_progress"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b32a00-32eb-4da8-9568-c8f36bb20b7b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Physics-informed neural networks (PINNs): an introductory crash-course\n",
    "\n",
    "By Ben Moseley, 2022\n",
    "\n",
    "This workshop builds upon my blog post on PINNs: https://benmoseley.blog/my-research/so-what-is-a-physics-informed-neural-network/. \n",
    "\n",
    "Read the seminal PINN papers [here](https://ieeexplore.ieee.org/document/712178) and [here](https://www.sciencedirect.com/science/article/pii/S0021999118307125).\n",
    "\n",
    "\n",
    "## Workshop goals\n",
    "\n",
    "By the end of this workshop, you should be able to:\n",
    "- code a PINN from scratch in PyTorch\n",
    "- understand the different types of scientific tasks PINNs can be used for\n",
    "- understand in more detail how PINNs are trained and how to improve their convergence\n",
    "\n",
    "\n",
    "## Task overview\n",
    "\n",
    "We will be coding a PINN from scratch in PyTorch and using it solve simulation and inversion tasks related to the damped harmonic oscillator.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129d88ca-2050-4f98-8c66-338dd5aa25ad",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Problem overview\n",
    "\n",
    "We are going to use a PINN to solve problems related to the **damped harmonic oscillator**:\n",
    "\n",
    "<img src=\"oscillator.gif\" width=\"500\">\n",
    "\n",
    "We are interested in modelling the displacement of the mass on a spring (green box) over time.\n",
    "\n",
    "This is a canonical physics problem, where the displacement, $u(t)$, of the oscillator as a function of time can be described by the following differential equation:\n",
    "\n",
    "$$\n",
    "m \\dfrac{d^2 u}{d t^2} + \\mu \\dfrac{d u}{d t} + ku = 0~,\n",
    "$$\n",
    "\n",
    "where $m$ is the mass of the oscillator, $\\mu$ is the coefficient of friction and $k$ is the spring constant.\n",
    "\n",
    "We will focus on solving the problem in the **under-damped state**, i.e. where the oscillation is slowly damped by friction (as displayed in the animation above). \n",
    "\n",
    "Mathematically, this occurs when:\n",
    "\n",
    "$$\n",
    "\\delta < \\omega_0~,~~~~~\\mathrm{where}~~\\delta = \\dfrac{\\mu}{2m}~,~\\omega_0 = \\sqrt{\\dfrac{k}{m}}~.\n",
    "$$\n",
    "\n",
    "Furthermore, we consider the following initial conditions of the system:\n",
    "\n",
    "$$\n",
    "u(t=0) = 1~~,~~\\dfrac{d u}{d t}(t=0) = 0~.\n",
    "$$\n",
    "\n",
    "For this particular case, the exact solution is known and given by:\n",
    "\n",
    "$$\n",
    "u(t) = e^{-\\delta t}(2 A \\cos(\\phi + \\omega t))~,~~~~~\\mathrm{with}~~\\omega=\\sqrt{\\omega_0^2 - \\delta^2}~.\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "For a more detailed mathematical description of the harmonic oscillator, check out this blog post: https://beltoforion.de/en/harmonic_oscillator/."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e06139-aa1b-45e3-941c-4807c0a05c5e",
   "metadata": {},
   "source": [
    "# Workflow overview\n",
    "\n",
    "There are **two scientific tasks** related to the harmonic oscillator we will use a PINN for:\n",
    "\n",
    ">First, we will **simulate** the system using a PINN, given its initial conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2a86b5-566d-4c8f-9395-0cfa8d1531cf",
   "metadata": {},
   "source": [
    "## Task 1: train a PINN to simulate the system\n",
    "\n",
    "#### Task\n",
    "\n",
    "The first task is to use a PINN to **simulate** the system.\n",
    "\n",
    "Specifically, our inputs and outputs are:\n",
    "\n",
    "- Inputs: underlying differential equation and the initial conditions of the system\n",
    "- Outputs: estimate of the solution, $u(t)$\n",
    "\n",
    "#### Approach\n",
    "\n",
    "The PINN is trained to directly approximate the solution to the differential equation, i.e.\n",
    "\n",
    "$$\n",
    "u_{\\mathrm{PINN}}(t;\\theta) \\approx u(t)~,\n",
    "$$\n",
    "\n",
    "where $\\theta$ are the free parameters of the PINN.\n",
    "\n",
    "#### Loss function\n",
    "\n",
    "To simulate the system, the PINN is trained with the following loss function:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\theta)= (u_{\\mathrm{PINN}}(t=0;\\theta) - 1)^2 + \\lambda_1 \\left(\\frac{d\\,u_{\\mathrm{PINN}}}{dt}(t=0;\\theta) - 0\\right)^2 + \\frac{\\lambda_2}{N} \\sum^{N}_{i} \\left( \\left[ m\\frac{d^2}{dt^2} + \\mu \\frac{d}{dt} + k \\right] u_{\\mathrm{PINN}}(t_{i};\\theta) Â \\right)^2\n",
    "$$\n",
    "\n",
    "For this task, we use $\\delta=2$, $\\omega_0=20$, and try to learn the solution over the domain $t\\in [0,1]$.\n",
    "\n",
    "#### Notes\n",
    "\n",
    "The first two terms in the loss function represent the **boundary loss**, and tries to ensure that the solution learned by the PINN matches the initial conditions of the system, namely, $u(t=0)=1$ and $u'(t=0)=0$.\n",
    "\n",
    "The second term in the loss function is called the **physics loss**, and and tries to ensure that the PINN solution obeys the underlying differential equation at a set of training points $\\{t_i\\}$ sampled over the entire domain.\n",
    "\n",
    "The hyperparameters, $\\lambda_1$ and $\\lambda_2$, are used to balence the terms in the loss function, to ensure stability during training.\n",
    "\n",
    "Autodifferentiation (`torch.autograd`) is used to calculate the gradients of the PINN with respect to its input required to evaluate the loss function. This is very powerful! \n",
    "\n",
    "For more details on `torch.autograd`, check out [this](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#a-gentle-introduction-to-torch-autograd) tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ad0024-2337-49a7-83d1-7feb564a8dde",
   "metadata": {},
   "source": [
    "Here you can see an overview of the 2 dimensional space (t, mu):\n",
    "\n",
    "<img src=\"excalidraw1.JPG\" width=\"750\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984affd8-1cd5-478c-8357-df169169151d",
   "metadata": {},
   "source": [
    "## Step 1: Definition and initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c22569-040e-41ee-9ea1-3a937c7b7edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import all what you need:\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "from torchsummary import summary # https://pypi.org/project/torch-summary/\n",
    "import torchinfo \n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2085019-11d0-4e4d-b8b1-a4a87e045612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pseudorandom number generator:\n",
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fdb0e9-ec24-4b7e-b4f6-5b509794feec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definition of the physical constants and hyperparameters:\n",
    "mass = 1\n",
    "d, w0 = 2, 20\n",
    "mu, k = 2*d, w0**2            # Este mu NO se utiliza!!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7ea15f-2b87-4fc7-9986-91407f52b886",
   "metadata": {},
   "source": [
    "### Initial, training and test points generation:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a23905-0efb-486b-a59c-54f800cf0c66",
   "metadata": {},
   "source": [
    "#### Initial condition 1: generation of initial points:\n",
    "$$ u(t=0, \\mu) = 1\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b59d538-c9bf-4c3d-8120-6f37d3984851",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Generation of t and mu initial points (Initial condition 1) \n",
    "point_resolution = 40\n",
    "\n",
    "ic1_t_mu = torch.stack([torch.zeros(point_resolution).requires_grad_(True), torch.linspace(1,10, point_resolution).requires_grad_(True)],-1)\n",
    "ic1_scope = torch.ones_like(ic1_t_mu[:,0:1]).requires_grad_(True)\n",
    "\n",
    "#print(f\"Initial condition 1: \\n \\t [t, mu]: \\n{ic1_t_mu}\")\n",
    "# print(f\"\\n Initial condition 1: \\n \\t u(t=0, mu) = 1: \\n {ic1_scope}\")\n",
    "# print(f\"\\n Shapes of [t, mu] and u(t=0, mu) = 1: \\n\\t{ic1_t_mu.shape}, {ic1_scope.shape}\")\n",
    "#ic1_t_mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b926c7-f8cc-4def-ae64-850ee7147c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.linspace(1,10,40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9d81a6-b8b1-48cf-bc28-16c72666bb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.rand(1,10, 40).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a3556d-a8e5-4869-96c5-1ad657294d10",
   "metadata": {},
   "source": [
    "#### Initial condition 2: t and mu training points generation:\n",
    "$$\n",
    "\\dfrac{d u}{d t}(t=0, \\mu) = 0~\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e948f10f-422c-4c8c-b583-ccf88d582af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ic2_t_mu = torch.stack([torch.zeros(point_resolution).requires_grad_(True), torch.linspace(1,10, point_resolution).requires_grad_(True)], -1)\n",
    "ic2_scope = torch.zeros_like(ic2_t_mu[:,0:1]).requires_grad_(True)\n",
    "\n",
    "#print(f\"Initial condition 2: \\n \\t [t, mu]: \\n{ic2_t_mu}\")\n",
    "#print(f\"\\n Initial condition 2: \\n \\t du/dt(t=0, mu) = 0: \\n\\t {ic2_scope}\")\n",
    "#print(f\"\\n Shapes of [t, mu] and du/dt(t=0, mu) = 0: \\n\\t {ic2_t_mu.shape}, {ic2_scope.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c522a7-1779-4393-8aa7-aeae804817db",
   "metadata": {},
   "source": [
    "#### Generation of physical domain training points: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db496d1-a104-4b16-9b91-a4b1aeef5b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate domain physic loss sample points:\n",
    "physic_in_t_mu = [torch.linspace(1e-2,1, point_resolution).requires_grad_(True), torch.linspace(1,10, point_resolution).requires_grad_(True)] # Mejora: se puede utilizar torch.rand\n",
    "physic_domain_t_mu = torch.stack(torch.meshgrid(*physic_in_t_mu, indexing='ij'), -1).view(-1, 2).requires_grad_(True)\n",
    "\n",
    "#print(f\" Point seed list for mesh grid domain points: \\n \\t {physic_in_t_mu}  \\n Shape: {physic_in_t_mu[0].shape}\")\n",
    "#print(f\"Domain training points: \\n \\t: {physic_domain_t_mu}\")\n",
    "#print(f\"Size of Domain training points: \\n \\t {physic_domain_t_mu.size()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58dd45d-654d-4134-88aa-aa2d9a2b035e",
   "metadata": {},
   "source": [
    "#### Generation of testing points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f67b655-9aad-46dd-8d6e-6d296cd9ba90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Generation of t and mu test points within the domain:\n",
    "point_resolution_test = 100\n",
    "\n",
    "# Testing points:\n",
    "test_in_t_mu = [torch.linspace(0,1,point_resolution_test), torch.linspace(1,10,point_resolution_test) ]\n",
    "test_domain_t_mu = torch.stack(torch.meshgrid(*test_in_t_mu, indexing='ij'), -1).view(-1, 2)\n",
    "\n",
    "#print(f\"Point seed list for mesh grid test points: \\n \\t \\n \\t: {test_in_t_mu}\")\n",
    "#print(f\"Test points \\n \\t [t, mu]: \\n \\t {test_domain_t_mu}\")\n",
    "#print(f\"Size of the test domain points: \\n \\t {test_domain_t_mu.size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79002ac2-be30-4ac5-825c-b2398eeb8b29",
   "metadata": {},
   "source": [
    "### Exact solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5790959c-49be-41c4-b8f3-90d01ed69d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exact_solution(d, w0, t):\n",
    "    \"Defines the analytical solution to the under-damped harmonic oscillator problem above.\"\n",
    "    assert d < w0             \n",
    "    w = np.sqrt(w0**2-d**2)\n",
    "    phi = np.arctan(-d/w)\n",
    "    A = 1/(2*np.cos(phi))\n",
    "    cos = torch.cos(phi+w*t)\n",
    "    exp = torch.exp(-d*t)\n",
    "    u = exp*2*A*cos\n",
    "    return u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f120b6b3-492b-4e75-bad7-f87fc0879323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example to check if it works (calculation of the exact solution):\n",
    "test_mu = 5\n",
    "u_exact = exact_solution(test_mu/(2*mass), w0, test_in_t_mu[0])\n",
    "#u_exact.view(-1,1)\n",
    "#u_exact.size()\n",
    "#u_exact.numel()\n",
    "#u_exact.dim()\n",
    "#u_exact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53317ddd-d992-47f4-8545-ec4aae219a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#physic_in_t_mu[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbaecfb-1f86-4d09-a822-b78be35429ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the exact solution:\n",
    "plt.figure(figsize=(10,2.5))\n",
    "plt.plot(\n",
    "        test_in_t_mu[0], \n",
    "        u_exact, \n",
    "        label=\"Exact solution\", \n",
    "        color=\"tab:grey\", \n",
    "        alpha=0.6)\n",
    "plt.scatter(\n",
    "        test_in_t_mu[0], \n",
    "        torch.zeros_like(test_in_t_mu[0]), \n",
    "        s=20, \n",
    "        lw=0, \n",
    "        color=\"tab:green\",\n",
    "        alpha=0.6,\n",
    "        label= \"Seed points (testing)\")\n",
    "plt.scatter(\n",
    "        physic_in_t_mu[0].detach(), \n",
    "        torch.zeros_like(physic_in_t_mu[0]), \n",
    "        s=20, \n",
    "        lw=0, \n",
    "        color=\"tab:red\",\n",
    "        alpha=0.6,\n",
    "        label= \"Training points\")\n",
    "\n",
    "plt.title(f\"Exact solution \\n u(t=(0,1), $\\mu$ = {test_mu}) using {point_resolution_test} seed points\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ae450c-6244-4a69-839f-bad9e6115754",
   "metadata": {},
   "source": [
    "## Step 2: Create Fully Connected Neural Network Class (FCNN):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f65f49-a637-447c-8672-ef2c47f94e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# import torch.nn.init as init\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "\n",
    "class FCN(nn.Module):\n",
    "    def __init__(self, N_INPUT = 2, hidden_layers = [4], N_OUTPUT = 1, activation='Tanh', initialization='Xavier'):\n",
    "        super().__init__()\n",
    "\n",
    "        self.activation_functions = nn.ModuleDict([\n",
    "            [\"Tanh\", nn.Tanh()],\n",
    "            [\"ReLU\", nn.ReLU()],\n",
    "            [\"LeakyReLU\", nn.LeakyReLU()],\n",
    "            [\"Sigmoid\", nn.Sigmoid()],\n",
    "            [\"Softmax\", nn.Softmax(dim=-1)],\n",
    "        ])\n",
    "\n",
    "        if activation not in self.activation_functions:\n",
    "            raise ValueError(\"Unsupported activation function\")\n",
    "\n",
    "        self.activation = self.activation_functions[activation]\n",
    "\n",
    "        self.fch = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(N_INPUT if i == 0 else hidden_size, hidden_size),\n",
    "                self.activation\n",
    "            ) for i, hidden_size in enumerate(hidden_layers[:])   \n",
    "        ])\n",
    "\n",
    "        self.fco = nn.Linear(hidden_layers[-1], N_OUTPUT)\n",
    "\n",
    "        self.initialize_parameters(initialization)\n",
    "\n",
    "        self.plot_weights(fig_size = (10,5), font_size = 8)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = self.activation(self.fci(x))\n",
    "        for layer in self.fch:\n",
    "            x = self.activation(layer(x))\n",
    "        x = self.fco(x)\n",
    "        return x\n",
    "\n",
    "    def initialize_parameters(self, initialization):\n",
    "        \n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                if initialization == 'Uniform':\n",
    "                    init.uniform_(module.weight.data, -0.1, 0.1)\n",
    "                    init.zeros_(module.bias.data)\n",
    "                elif initialization == 'Normal':\n",
    "                    init.normal_(module.weight.data, mean=0, std=0.01)\n",
    "                    init.zeros_(module.bias.data)\n",
    "                elif initialization == 'Xavier':\n",
    "                    init.xavier_uniform_(module.weight.data)\n",
    "                    init.zeros_(module.bias.data)\n",
    "                elif initialization == 'He':\n",
    "                    init.kaiming_uniform_(module.weight.data, mode='fan_in', nonlinearity='relu')\n",
    "                    init.zeros_(module.bias.data)\n",
    "                elif initialization == 'Orthogonal':\n",
    "                    init.orthogonal_(module.weight.data)\n",
    "                    init.zeros_(module.bias.data)\n",
    "                elif initialization == 'Kaiming':\n",
    "                    init.kaiming_uniform_(module.weight.data, mode='fan_in', nonlinearity='leaky_relu')\n",
    "                    init.zeros_(module.bias.data)\n",
    "                else:\n",
    "                    raise ValueError(\"Unsupported initialization type\")\n",
    "    \n",
    "    def plot_weights(self, fig_size = (10,5), font_size = 8):\n",
    "        \n",
    "        self.fig_size = fig_size\n",
    "        self.font_size = font_size\n",
    "        \n",
    "        weights_biases_dict = self.state_dict()\n",
    "        \n",
    "        # Definition of columns, rows and subplots\n",
    "        num_subplots = len(weights_biases_dict)\n",
    "        num_cols = 2\n",
    "        num_rows = (num_subplots + num_cols - 1) // num_cols  \n",
    "        \n",
    "        # For colorbar (collect minimum and maximum values across all tensors)\n",
    "        all_values = np.concatenate([tensor.flatten() for tensor in weights_biases_dict.values()])\n",
    "        min_val = round(all_values.min(), 2)\n",
    "        max_val = round(all_values.max(), 2)\n",
    "        \n",
    "        # Create subplots\n",
    "        fig, axs = plt.subplots(num_rows, num_cols, figsize = self.fig_size)\n",
    "        \n",
    "        # Plot images and add values in each subplot\n",
    "        for i, (key, tensor) in enumerate(weights_biases_dict.items()):\n",
    "            row = i // num_cols\n",
    "            col = i % num_cols\n",
    "            ax = axs[row, col]\n",
    "            if 'weight' in key:\n",
    "                im = ax.imshow(tensor, cmap='viridis',vmin=min_val, vmax=max_val, interpolation='none')\n",
    "                ax.set_title(f'{key}', fontsize = self.font_size)\n",
    "            elif 'bias' in key:\n",
    "                im = ax.imshow(tensor.unsqueeze(0), cmap='viridis', vmin=min_val, vmax=max_val, interpolation='none')\n",
    "                ax.set_title(f'{key}', fontsize = self.font_size)\n",
    "            #ax.axis('off')\n",
    "            \n",
    "            # If the data is 1D (possibly biases), reshape them to (1, len(data))\n",
    "            if len(tensor.shape) == 1:\n",
    "                tensor = tensor.reshape(1, -1)\n",
    "\n",
    "            # Add xticks and yticks\n",
    "            ax.set_xticks(np.arange(0, tensor.shape[1] , step=1))  \n",
    "            ax.tick_params(axis='x', labelsize = self.font_size)\n",
    "            ax.set_yticks(np.arange(0, tensor.shape[0] , step=1))\n",
    "            ax.tick_params(axis='y', labelsize = self.font_size)\n",
    "\n",
    "           \n",
    "            # Add values in the middle of the cell\n",
    "            for y in range(tensor.shape[0]):\n",
    "                for x in range(tensor.shape[1]):\n",
    "                    value = tensor[y, x]\n",
    "                    ax.text(x, y, f'{value:.2f}', fontsize = self.font_size, color='white', ha='center', va='center')\n",
    "            \n",
    "         # Hide extra subplots\n",
    "        for i in range(num_subplots, num_rows * num_cols):\n",
    "            axs.flatten()[i].axis('off')\n",
    "            \n",
    "        # Add a title to the figure\n",
    "        plt.suptitle('Weights and Biases of the Neural Network', fontsize = self.font_size)\n",
    "        \n",
    "        # Add a colorbar\n",
    "            # Set custom ticks and intervals\n",
    "        tick_interval = 0.2\n",
    "        ticks = np.arange(min_val, max_val, step=tick_interval)\n",
    "\n",
    "        cbar_ax = fig.add_axes([0.25, 0.05, 0.5, 0.01])  # [left, bottom, width, height]\n",
    "        cbar = fig.colorbar(im, cax=cbar_ax, orientation='horizontal', ticks = ticks)\n",
    "        cbar.ax.tick_params(labelsize = self.font_size)\n",
    "        \n",
    "        cbar.set_label('Range of Weights and Biases', fontsize= self.font_size) \n",
    "        \n",
    "        # Show the plots\n",
    "        plt.show()\n",
    "\n",
    "class FCN_extended(FCN):\n",
    "    def __init__(self, N_INPUT, hidden_layers, N_OUTPUT, activation='Tanh', initialization='Xavier', original_model_path=None):\n",
    "        super().__init__(N_INPUT, hidden_layers, N_OUTPUT, activation, initialization)\n",
    "\n",
    "        #self.initialize_parameters(initialization)\n",
    "        \n",
    "        self.original_model_path = original_model_path\n",
    "        self.original_state_dict = self.load_original_state_dict()\n",
    "\n",
    "        self.extend_layers_neurons()\n",
    "        \n",
    "    def extend_layers_neurons(self):\n",
    "        #current_model.initialize_parameters(initialization)  # Initialize current model first\n",
    "        for name, param in self.named_parameters():\n",
    "            \n",
    "            if name in self.original_state_dict:                \n",
    "                original_param = self.original_state_dict[name]\n",
    "                \n",
    "                if param.shape == original_param.shape:                    \n",
    "                    param.data.copy_(original_param)   \n",
    "                    \n",
    "                else:\n",
    "                    \n",
    "                    if \"weight\" in name: \n",
    "                        if param.shape != original_param.shape:  \n",
    "                            # Copy matching portion of old weights                     \n",
    "                            param.data[:original_param.size(0), :original_param.size(1)].copy_(original_param)\n",
    "                        \n",
    "                        elif param.size(0) != original_param.size(0): \n",
    "                            # Copy matching portion of old weights                           \n",
    "                            param.data[:original_param.size(0), :].copy_(original_param)                           \n",
    "                        \n",
    "                        elif param.size(0) != original_param.size(0) and \"fc0\" in name:                            \n",
    "                            param.data[0,:original_param.size(1)].copy_(original_param[0]) \n",
    "               \n",
    "                    if \"bias\" in name: \n",
    "                        if param.size(0) != original_param.size(0):  \n",
    "                            # Copy matching portion of old weights\n",
    "                            param.data[:original_param.size(0)].copy_(original_param)\n",
    "                        else:\n",
    "                            param.data.copy_(original_param)     \n",
    "    \n",
    "    def load_original_state_dict(self):\n",
    "        if self.original_model_path is None:\n",
    "            raise ValueError(\"Path to the original model checkpoint is not provided.\")\n",
    "\n",
    "        if not os.path.exists(self.self.model_path):\n",
    "            raise FileNotFoundError(f\"Provided path '{self.self.model_path}' does not exist.\")\n",
    "\n",
    "        if not os.path.isfile(self.self.model_path):\n",
    "            raise ValueError(f\"Provided path '{self.self.model_path}' is not a file.\")\n",
    "\n",
    "        _, ext = os.path.splitext(self.self.model_path)\n",
    "        if ext not in ['.pt', '.pth']:\n",
    "            raise ValueError(\"Provided file is not a valid checkpoint file.\")\n",
    "        \n",
    "        # Load weights and biases from self.model\n",
    "        return torch.load(self.self.model_path)['model_state_dict']\n",
    "\n",
    "class TrainModel:\n",
    "    def __init__(self, model, ic1_t_mu, ic1_scope, ic2_scope, physic_in_t_mu, physic_domain_t_mu, k, test_in_t_mu, test_in_t_constant_mu, u_exact, test_mu = 5, learning_rate = 0.01, num_epochs= 20000, lambda1= 1e-1, lambda2 = 1e-4, checkpoint_interval = 1000, stagnation_amplitude= 0.0001, stagnation_range = 200):\n",
    "        \n",
    "        self.model = model\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_epochs = num_epochs\n",
    "        self.checkpoint_interval = checkpoint_interval\n",
    "        self.stagnation_amplitude = stagnation_amplitude\n",
    "        self.stagnation_range = stagnation_range\n",
    "\n",
    "        self.ic1_t_mu = ic1_t_mu\n",
    "        self.ic1_scope = ic1_scope\n",
    "        self.ic2_scope = ic2_scope\n",
    "        self.physic_in_t_mu = physic_in_t_mu\n",
    "        self.physic_domain_t_mu = physic_domain_t_mu\n",
    "        self.test_in_t_mu = test_in_t_mu\n",
    "        self.test_in_t_constant_mu = test_in_t_constant_mu\n",
    "        self.u_exact = u_exact\n",
    "\n",
    "        self.test_mu = test_mu\n",
    "        self.k = k\n",
    "        self.lambda1 = lambda1\n",
    "        self.lambda2 = lambda2\n",
    "\n",
    "        self.loss_ic1_history = []\n",
    "        self.loss_ic2_history = []\n",
    "        self.loss_differential_equation_history = []\n",
    "        self.loss_total_history = []\n",
    "        self.loss_history = []\n",
    "        self.max_min_range_history = []\n",
    "        \n",
    "        self.train()\n",
    "\n",
    "    def plot_results(self, residuals_ic1, residuals_ic2, residuals_differential_equation, loss_ic1_history, loss_ic2_history, loss_differential_equation_history, loss_total_history, loss_history):\n",
    "              \n",
    "        self.test_predicted = self.model(test_in_t_constant_mu)\n",
    "        \n",
    "        plt.figure(figsize=(10,2.5))\n",
    "        plt.plot(self.test_in_t_mu[0].detach(), \n",
    "                 self.u_exact, \n",
    "                 label=\"Exact solution\", \n",
    "                 color=\"tab:grey\", \n",
    "                 alpha=0.6\n",
    "        )\n",
    "        plt.plot(\n",
    "                self.test_in_t_mu[0], \n",
    "                self.test_predicted[:,0].detach(), \n",
    "                label=\"PINN solution\", \n",
    "                color=\"tab:green\"\n",
    "        )\n",
    "        plt.scatter(\n",
    "                    self.physic_in_t_mu[0].detach(), \n",
    "                    torch.zeros_like(self.physic_in_t_mu[0]), \n",
    "                    s=20, \n",
    "                    lw=0, \n",
    "                    color=\"tab:red\",\n",
    "                    alpha=0.6,\n",
    "                    label= \"Training points\"\n",
    "        )\n",
    "        plt.scatter(\n",
    "                    self.test_in_t_mu[0], \n",
    "                    torch.zeros_like(self.test_in_t_mu[0]), \n",
    "                    s=20, \n",
    "                    lw=0, \n",
    "                    color=\"tab:green\",\n",
    "                    alpha=0.6,\n",
    "                    label= \"Seed points(testing)\"\n",
    "        )\n",
    "        #plt.title(f\"Exact and predicted solution \\n u(t=(0,1), $\\mu$ = {test_mu}), model: {self.model.__class__.__name__}, activation function: {self.model.activation()}, epoch = {self.i} \\n (learning rate: {learning_rate}, lambda1: {lambda1}, lambda2: {lambda2})\")\n",
    "        plt.title(f\"Exact and predicted solution \\n u(t=(0,1), $\\mu$ = {self.test_mu}), model: {self.model.__class__.__name__}, activation function: Tanh, epoch = {self.i} \\n (learning rate: {self.learning_rate}, lambda1: {self.lambda1}, lambda2: {self.lambda2})\")\n",
    "    \n",
    "        plt.grid()\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "    def train(self):\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr= self.learning_rate)\n",
    "\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for self.i in range(1, self.num_epochs + 1):\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "            # compute initial condition 1 loss:\n",
    "            self.ic1_predicted= self.model(self.ic1_t_mu)\n",
    "            \n",
    "            self.residuals_ic1 = self.ic1_predicted - self.ic1_scope\n",
    "            \n",
    "            self.loss_ic1 = torch.mean((self.ic1_predicted - self.ic1_scope)**2)\n",
    "            self.loss_ic1_history.append(self.loss_ic1.item())\n",
    "        \n",
    "        \n",
    "            # compute initial condition 2 loss:\n",
    "            self.du_dtdmu_initial = torch.autograd.grad(outputs = self.ic1_predicted, inputs = self.ic1_t_mu, grad_outputs= torch.ones_like(self.ic1_predicted), create_graph= True)[0]\n",
    "            self.ic2_du_dt, ic2_du_dmu = self.du_dtdmu_initial[:, 0:1], self.du_dtdmu_initial[:,1:2]\n",
    "            \n",
    "            self.residuals_ic2 = self.ic2_du_dt- self.ic2_scope\n",
    "            \n",
    "            self.loss_ic2 = torch.mean((self.ic2_du_dt- self.ic2_scope)**2)\n",
    "            self.loss_ic2_history.append(self.loss_ic2.item())\n",
    "        \n",
    "            # compute physic loss:\n",
    "            self.physic_domain_predicted = self.model(self.physic_domain_t_mu)\n",
    "            self.physic_domain_du_dtdmu = torch.autograd.grad(outputs = self.physic_domain_predicted, inputs = self.physic_domain_t_mu, grad_outputs= torch.ones_like(self.physic_domain_predicted), create_graph= True)[0]\n",
    "            self.physic_domain_d2u_d2t_d2mu = torch.autograd.grad(outputs = self.physic_domain_du_dtdmu[:,0:1], inputs = self.physic_domain_t_mu, grad_outputs= torch.ones_like(self.physic_domain_du_dtdmu[:,0:1]), create_graph= True)[0]\n",
    "            \n",
    "            self.residuals_differential_equation = self.physic_domain_d2u_d2t_d2mu[:,0:1] + self.physic_domain_t_mu[:,1:2] * self.physic_domain_du_dtdmu[:,0:1] + self.k * self.physic_domain_predicted \n",
    "            \n",
    "            self.loss_differential_equation = torch.mean( (self.physic_domain_d2u_d2t_d2mu[:,0:1] + self.physic_domain_t_mu[:,1:2] * self.physic_domain_du_dtdmu[:,0:1] + self.k * self.physic_domain_predicted )**2)\n",
    "            self.loss_differential_equation_history.append(self.loss_differential_equation.item())\n",
    "            \n",
    "            self.loss = self.loss_ic1 + self.lambda1 * self.loss_ic2 + self.lambda2 * self.loss_differential_equation\n",
    "            self.loss_total_history.append(self.loss.item())\n",
    "            self.loss_history.append(self.loss.item())\n",
    "            \n",
    "            if self.i ==1:                \n",
    "                self.initial_loss_value = self.loss.item()        \n",
    "            \n",
    "            self.loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Update loss history and ensure it contains the losses of the last check_range epochs\n",
    "            if len(self.loss_history) >  self.stagnation_range:\n",
    "                self.loss_history.pop(0)  # Remove the oldest loss value\n",
    "            \n",
    "            # Check if the difference between max and min loss in the last 100 epochs is within the threshold\n",
    "            self.max_min_amplitude = max(self.loss_history) - min(self.loss_history)\n",
    "            self.max_min_range_history.append(self.max_min_amplitude)\n",
    "            self.absolut_loss_value = sum(self.loss_history) / len(self.loss_history)\n",
    "\n",
    "            if len(self.loss_history) ==  self.stagnation_range and self.max_min_amplitude <= self.stagnation_amplitude and  self.absolut_loss_value < self.initial_loss_value:\n",
    "                print(f\"Stopping training at epoch {self.i} as the loss stabilized within the threshold.\")\n",
    "                print(f\"max_min_amplitude = {self.max_min_amplitude} \\n absolute_loss_value: {self.absolut_loss_value} \")\n",
    "                break\n",
    "            \n",
    "            if self.i%self.checkpoint_interval == 0:\n",
    "                print(f\"max_min_amplitude = {self.max_min_amplitude}\")\n",
    "                print(f'Decomposition of the loss terms: \\n loss({self.loss}) = loss1({self.loss_ic1}) + {self.lambda1} * loss2({self.loss_ic2}) + {self.lambda2} * loss3({self.loss_differential_equation})')\n",
    "                self.plot_results(self.residuals_ic1, self.residuals_ic2, self.residuals_differential_equation, self.loss_ic1_history, self.loss_ic2_history, self.loss_differential_equation_history, self.loss_total_history, self.loss_history)\n",
    "                torch.save({\n",
    "                            \"epoch\": self.i,\n",
    "                            \"model_state_dict\": self.model.state_dict(),\n",
    "                            \"optimiser_state_dict\": optimizer.state_dict(),\n",
    "                            \"loss\": self.loss,\n",
    "                           },                    \n",
    "                            f\"lr{self.learning_rate}_epoch{self.i}.pth\")\n",
    "                print(f\"Saved the checkpoint corresponding to epoch: {self.i}\")\n",
    "                print(\"*\"*100)\n",
    "        end_time = time.time()\n",
    "        execution_time = (end_time - start_time)\n",
    "        print(f\"Training elapsed time (s): {execution_time}\")\n",
    "        torch.save({\n",
    "                    \"epoch\": self.i,\n",
    "                    \"model_state_dict\": self.model.state_dict(),\n",
    "                    \"optimiser_state_dict\": optimizer.state_dict(),\n",
    "                    \"loss\": self.loss,\n",
    "                    },                    \n",
    "                    f\"original_model.pth\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa88b304-2a28-4e90-9d1c-221701f1e0a3",
   "metadata": {},
   "source": [
    "## STEP 3: Instantiate the Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b93f99c-8c67-4927-ba34-7f0ef69ab5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_input_size = 2\n",
    "original_hidden_layers = [4]\n",
    "original_output_size = 1\n",
    "original_model = FCN(original_input_size, original_hidden_layers, original_output_size, activation='Tanh', initialization='Xavier')\n",
    "#original_model.plot_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bb776b-8eef-4974-8990-69b8518965ac",
   "metadata": {},
   "source": [
    "### Checking the model using test points and a constant mu value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c376b2ed-ef6e-4108-a688-7e268e7172ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mu = 5\n",
    "test_in_t_constant_mu = torch.stack([torch.linspace(0,1,point_resolution_test), test_mu*torch.ones(point_resolution_test)], -1).view(-1,2)\n",
    "#print(f\" Point seed list for mesh grid domain points: \\n \\t {test_in_t_constant_mu}\")\n",
    "#print(f\"Size of Domain training points: \\n \\t {test_in_t_constant_mu.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d8aeef-136e-442e-b752-b420f4c1b339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NO es necesario porque se mantiene constante mu\n",
    "#test_domain_t_constant_mu = torch.stack(torch.meshgrid(*test_in_t_constant_mu, indexing='ij'), -1).view(-1, 2)\n",
    "#test_domain_t_constant_mu.size()\n",
    "#print(f\"Domain training points: \\n \\t: {test_domain_t_constant_mu}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d5f3e9-f7eb-4480-adca-523b7da72a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predicted = original_model(test_in_t_constant_mu)\n",
    "#test_predicted.size()\n",
    "#test_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d41b39a-0941-43a9-b3f3-ac45ffa10a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,2.5))\n",
    "plt.plot(test_in_t_mu[0].detach(), \n",
    "                 u_exact, \n",
    "                 label=\"Exact solution\", \n",
    "                 color=\"tab:grey\", \n",
    "                 alpha=0.6\n",
    ")\n",
    "plt.plot(test_in_t_mu[0], \n",
    "                 test_predicted[:,0].detach(), \n",
    "                 label=\"PINN solution (initial)\", \n",
    "                 color=\"tab:green\"\n",
    ")\n",
    "plt.scatter(\n",
    "        test_in_t_mu[0], \n",
    "        torch.zeros_like(test_in_t_mu[0]), \n",
    "        s=20, \n",
    "        lw=0, \n",
    "        color=\"tab:green\",\n",
    "        alpha=0.6,\n",
    "        label= \"Seed points(testing)\"\n",
    ")\n",
    "plt.scatter(\n",
    "        physic_in_t_mu[0].detach(), \n",
    "        torch.zeros_like(physic_in_t_mu[0]), \n",
    "        s=20, \n",
    "        lw=0, \n",
    "        color=\"tab:purple\",\n",
    "        alpha=0.6,\n",
    "        label= \"Training points\"\n",
    ")\n",
    "### model and activation has to be manually adapted\n",
    "plt.title(f\"Exact and predicted solution for a nn with following architecture: [{original_input_size}, {original_hidden_layers}, {original_output_size}] \\n u(t=(0,1), $\\mu$ = {test_mu}), model: {original_model.__class__.__name__}, activation function: {original_model.activation}, epoch = 1\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a0144e-98d1-4a3c-bd6a-8c63deecf2c4",
   "metadata": {},
   "source": [
    "## STEP 4: Training the PINN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ffb4dd-ddba-4ad8-8718-8fb1efee97b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train original model\n",
    "#train_model = TrainModel(original_model, num_epochs=1000, checkpoint_interval=100, loss_threshold=0.01)\n",
    "#train_model = TrainModel(original_model, ic1_t_mu, ic1_scope, ic2_scope, physic_domain_t_mu, k, learning_rate = 0.0001, num_epochs= 20000, lambda1= 1e-1, lambda2 = 1e-4, checkpoint_interval = 1000, stagnation_amplitude= 0.0001, stagnation_range = 200)\n",
    "train_model = TrainModel(original_model, ic1_t_mu, ic1_scope, ic2_scope, physic_in_t_mu, physic_domain_t_mu, k, test_in_t_mu, test_in_t_constant_mu, u_exact, test_mu = 5, learning_rate = 1e-3, num_epochs= 20000, lambda1= 1, lambda2 = 1e-4, checkpoint_interval = 1000, stagnation_amplitude= 0.00001, stagnation_range = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655b42ba-be0b-4539-a735-f2309186e892",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_model.plot_weights(fig_size = (10,5), font_size = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6649e2-a848-449f-8c69-b5361a65c65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss history as before\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.plot(train_model.loss_history, label='Loss over epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss: last 100 epochs')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231ddaae-922a-4283-bf3f-43f79596794b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 3))\n",
    "plt.plot(train_model.loss_total_history[-1000:], label='Loss over epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss: last 1000 epochs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87673051-6f2c-4746-9fc5-f6fe237946f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 3))\n",
    "plt.plot(train_model.loss_total_history, label='Loss over epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss: whole history')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29db1309-a7e0-4253-8671-626aea0c97c0",
   "metadata": {},
   "source": [
    "## STEP 5: Investigation of the loss terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71126175-8509-4181-84f5-1b26f5d17152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the contribution of every loss term (loss1, loss2 and loss3)\n",
    "fig, (loss1_2, loss3) = plt.subplots(1,2, layout = 'constrained', sharex = True, figsize = (12,5))\n",
    "#fig.suptitle(f\"Decomposition of the loss terms using {original_model.__class__.__name__} model and Tanh #{original_model.activation()}# activation function \\n (learning_rate: {learning_rate}, lambda1: {lambda1}, lambda2: {lambda2})\", fontsize = 14)\n",
    "fig.suptitle(f\"Decomposition of the loss terms using {train_model.__class__.__name__} model and Tanh activation function \\n (learning_rate: {train_model.learning_rate}, lambda1: {train_model.lambda1}, lambda2: {train_model.lambda2})\", fontsize = 14)\n",
    "\n",
    "loss1_2.plot(train_model.loss_ic1_history, label = \"loss1: residuals of u(t=0)=1\", color = \"tab:red\")\n",
    "loss1_2.plot(train_model.loss_ic2_history, label = \"loss2: residuals of du/dt(t=0)=0\", color = \"tab:blue\")\n",
    "loss1_2.set_title(\"loss1: (u(t=0)=1) and loss2: (du/dt(t=0)=0)\")\n",
    "loss1_2.set_xlabel(\"epochs\")\n",
    "loss1_2.set_ylabel(\"residuals\")\n",
    "loss1_2.grid()\n",
    "loss1_2.legend()\n",
    "\n",
    "loss3.plot(train_model.loss_differential_equation_history, label= \"loss3: residuals of the differential equation\", color = \"tab:grey\")\n",
    "loss3.set_title(\"loss3: residuals of the differential equation\")\n",
    "loss3.set_xlabel(\"epochs\")\n",
    "loss3.set_ylabel(\"residuals\")\n",
    "loss3.legend()\n",
    "loss3.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e105f44-a3bc-46f9-81f8-c9b42d4f47e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 3))\n",
    "plt.plot(train_model.max_min_range_history, label='Max min range over epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Max min range')\n",
    "plt.title('Max min range: whole history')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93eacc91-c2df-45b7-920d-221345fc8d30",
   "metadata": {},
   "source": [
    "## STEP 6: Inference/Prediction with a constant mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b762fdbd-eafc-4975-ad71-9c3faefb2865",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mu = 5\n",
    "test_in_t_constant_mu = torch.stack([torch.linspace(0,1,point_resolution_test), test_mu*torch.ones(point_resolution_test)], -1).view(-1,2)\n",
    "test_in_t_constant_mu.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26803c1-3b37-4df4-9309-da226f64f339",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predicted = original_model(test_in_t_constant_mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9de318c-f172-4e67-8527-72e729574f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "u_exact = exact_solution(test_mu/(2*mass), w0, test_in_t_mu[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc8b99d-8ab9-4088-b2f6-e682024e9112",
   "metadata": {},
   "outputs": [],
   "source": [
    "#u_test = pinn(test_t_frict_coeff_total).detach().cpu()\n",
    "plt.figure(figsize=(10,2.5))\n",
    "\n",
    "plt.plot(test_in_t_mu[0].detach(), \n",
    "                 u_exact, \n",
    "                 label=\"Exact solution\", \n",
    "                 color=\"tab:grey\", \n",
    "                 alpha=0.6)\n",
    "plt.plot(test_in_t_mu[0], \n",
    "                 test_predicted[:,0].detach(), \n",
    "                 label=\"PINN solution\", \n",
    "                 color=\"tab:green\")\n",
    "plt.scatter(\n",
    "            physic_in_t_mu[0].detach(), \n",
    "            torch.zeros_like(physic_in_t_mu[0]), \n",
    "            s=20, \n",
    "            lw=0, \n",
    "            color=\"tab:red\",\n",
    "            alpha=0.6,\n",
    "            label= \"Training points\")\n",
    "plt.scatter(\n",
    "            test_in_t_mu[0], \n",
    "            torch.zeros_like(test_in_t_mu[0]), \n",
    "            s=20, \n",
    "            lw=0, \n",
    "            color=\"tab:green\",\n",
    "            alpha=0.6,\n",
    "            label= \"Seed points(testing)\")\n",
    "plt.title(f\"Inference Case: Exact and predicted solution \\n u(t=(0,1), $\\mu$ = {train_model.test_mu}), model: {train_model.__class__.__name__}, activation function: Tanh \\n (learning rate: {train_model.learning_rate}, lambda1: {train_model.lambda1}, lambda2: {train_model.lambda2})\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a58084b-7591-4ddd-a10f-d78f8d9d26f2",
   "metadata": {},
   "source": [
    "# STEP 7: extension of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b82546-991c-483c-ba5d-bf4422a2b80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create extended model\n",
    "extended_input_size = 2\n",
    "extended_hidden_layers = [4,4]\n",
    "extended_output_size = 1\n",
    "extended_model = FCN_extended(extended_input_size, extended_hidden_layers, extended_output_size, activation='Tanh', initialization='Xavier', original_model_path='original_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635b6a23-e6ef-4bca-a419-7b6480fe4303",
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_model.plot_weights(fig_size = (10,5), font_size = 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d71ff1-0970-4ef1-bcdb-c03c762bed09",
   "metadata": {},
   "source": [
    "## Inference/Prediction for checking a correct initialization of the extended model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75630ec-a26c-4cdb-9b55-ccbe931d9d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mu = 5\n",
    "test_in_t_constant_mu = torch.stack([torch.linspace(0,1,point_resolution_test), test_mu*torch.ones(point_resolution_test)], -1).view(-1,2)\n",
    "test_in_t_constant_mu.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004e6cd7-4e79-48f9-a493-4a40c5a422f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predicted_extended_model = extended_model(test_in_t_constant_mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61aa438a-6ffd-4416-b204-a2058729aa46",
   "metadata": {},
   "outputs": [],
   "source": [
    "u_exact = exact_solution(test_mu/(2*mass), w0, test_in_t_mu[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bf49fd-09cd-4d71-a21f-2a17bc7da6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#u_test = pinn(test_t_frict_coeff_total).detach().cpu()\n",
    "plt.figure(figsize=(10,2.5))\n",
    "\n",
    "plt.plot(test_in_t_mu[0].detach(), \n",
    "                 u_exact, \n",
    "                 label=\"Exact solution\", \n",
    "                 color=\"tab:grey\", \n",
    "                 alpha=0.6)\n",
    "plt.plot(test_in_t_mu[0], \n",
    "                 test_predicted_extended_model[:,0].detach(), \n",
    "                 label=\"PINN solution\", \n",
    "                 color=\"tab:green\")\n",
    "plt.scatter(\n",
    "            physic_in_t_mu[0].detach(), \n",
    "            torch.zeros_like(physic_in_t_mu[0]), \n",
    "            s=20, \n",
    "            lw=0, \n",
    "            color=\"tab:red\",\n",
    "            alpha=0.6,\n",
    "            label= \"Training points\")\n",
    "plt.scatter(\n",
    "            test_in_t_mu[0], \n",
    "            torch.zeros_like(test_in_t_mu[0]), \n",
    "            s=20, \n",
    "            lw=0, \n",
    "            color=\"tab:green\",\n",
    "            alpha=0.6,\n",
    "            label= \"Seed points(testing)\")\n",
    "plt.title(f\"Inference Case: Exact and predicted solution \\n u(t=(0,1), $\\mu$ = {train_model.test_mu}), model: {train_model.__class__.__name__}, activation function: Tanh \\n (learning rate: {train_model.learning_rate}, lambda1: {train_model.lambda1}, lambda2: {train_model.lambda2})\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdec089-8aa0-40b8-a5dc-a32593ed6e96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591e3eb6-2fa9-44d7-9d3d-1677165381d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed800402-acca-4f9f-845a-bfa24164dd83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9cce1a-3f86-41a9-9026-f1449b79246f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2409edbc-a681-4686-865c-88b74f1efea5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb65159a-3c33-4842-9dad-07f5a4b23745",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffbcf25-0b53-4711-893a-d885cd2974f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b26c93-fcfb-4306-885a-472da1cee1ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06450750-462c-4a6d-828a-921de28ee5f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
