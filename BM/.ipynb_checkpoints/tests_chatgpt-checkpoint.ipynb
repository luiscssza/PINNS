{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c7b585-85ca-4aec-b02a-63c992f545cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the neural network architecture\n",
    "class DampedOscillatorPINN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DampedOscillatorPINN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(1, 50),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(50, 50),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(50, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, t):\n",
    "        return self.net(t)\n",
    "\n",
    "# Modified function to compute both loss and residuals\n",
    "def compute_loss_and_residuals(model, t, omega_0, zeta):\n",
    "    t.requires_grad = True\n",
    "    x = model(t)\n",
    "    dx_dt = torch.autograd.grad(x, t, torch.ones_like(x), create_graph=True)[0]\n",
    "    d2x_dt2 = torch.autograd.grad(dx_dt, t, torch.ones_like(dx_dt), create_graph=True)[0]\n",
    "    residuals = d2x_dt2 + 2*zeta*omega_0*dx_dt + omega_0**2*x\n",
    "    loss = torch.mean(residuals**2)\n",
    "    return loss, residuals\n",
    "\n",
    "# Model, optimizer, and training loop setup\n",
    "model = DampedOscillatorPINN()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "omega_0 = 1.0  # Natural frequency\n",
    "zeta = 0.1    # Damping ratio\n",
    "\n",
    "# Initialization\n",
    "losses = []\n",
    "epoch_residuals = []\n",
    "\n",
    "# Adjusted training loop\n",
    "for epoch in range(5000):\n",
    "    t = torch.rand(100, 1) * 10  # Generate random time points\n",
    "    loss, residuals = compute_loss_and_residuals(model, t, omega_0, zeta)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 100 == 0:  # Adjust based on your preference for data collection frequency\n",
    "        losses.append(loss.item())\n",
    "        epoch_residuals.append(residuals.mean().item())  # Example: storing mean residual\n",
    "\n",
    "# After training, plot the collected data\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(losses, label='Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epoch_residuals, label='Mean Residual')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Mean Residual')\n",
    "plt.title('Mean Residual Over Epochs')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "495baa4e-495e-4049-8e3d-bd266f035076",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T07:47:33.184647Z",
     "iopub.status.busy": "2024-02-28T07:47:33.182545Z",
     "iopub.status.idle": "2024-02-28T07:47:41.703799Z",
     "shell.execute_reply": "2024-02-28T07:47:41.703279Z",
     "shell.execute_reply.started": "2024-02-28T07:47:33.184598Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the neural network architecture\n",
    "class DampedOscillatorPINN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DampedOscillatorPINN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(1, 50),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(50, 50),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(50, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, t):\n",
    "        return self.net(t)\n",
    "\n",
    "# Modified function to compute both loss and residuals\n",
    "def compute_loss_and_residuals(model, t, omega_0, zeta):\n",
    "    t.requires_grad = True\n",
    "    x = model(t)\n",
    "    dx_dt = torch.autograd.grad(x, t, torch.ones_like(x), create_graph=True)[0]\n",
    "    d2x_dt2 = torch.autograd.grad(dx_dt, t, torch.ones_like(dx_dt), create_graph=True)[0]\n",
    "    residuals = d2x_dt2 + 2*zeta*omega_0*dx_dt + omega_0**2*x\n",
    "    loss = torch.mean(residuals**2)\n",
    "    return loss, residuals\n",
    "\n",
    "# Model, optimizer, and training loop setup\n",
    "model = DampedOscillatorPINN()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "omega_0 = 1.0  # Natural frequency\n",
    "zeta = 0.1    # Damping ratio\n",
    "\n",
    "\n",
    "# Initialization for residuals collection\n",
    "all_residuals = []\n",
    "selected_epoch = 5000  # Example: Collecting residuals at the last epoch\n",
    "\n",
    "for epoch in range(selected_epoch):\n",
    "    t = torch.rand(100, 1) * 10  # Generate random time points within the domain\n",
    "    optimizer.zero_grad()\n",
    "    loss, residuals = compute_loss_and_residuals(model, t, omega_0, zeta)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch == selected_epoch - 1:  # Collect residuals at the specified epoch\n",
    "        all_residuals.append(residuals.detach().numpy())\n",
    "\n",
    "# After training, plot the collected residuals for the specified epoch\n",
    "# Assuming `t` still holds the training points used in the last epoch\n",
    "plt.scatter(t.detach().numpy(), all_residuals[0], alpha=0.6)\n",
    "plt.xlabel('Training Points (t)')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residuals at Training Points')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edba5834-a06d-44b2-a6aa-c7fb375245fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming the rest of the model setup is as before\n",
    "\n",
    "# Initialization for residuals collection\n",
    "all_residuals = []\n",
    "selected_epoch = 5000  # Example: Collecting residuals at the last epoch\n",
    "\n",
    "for epoch in range(selected_epoch):\n",
    "    t = torch.rand(100, 1) * 10  # Generate random time points within the domain\n",
    "    optimizer.zero_grad()\n",
    "    loss, residuals = compute_loss_and_residuals(model, t, omega_0, zeta)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch == selected_epoch - 1:  # Collect residuals at the specified epoch\n",
    "        all_residuals.append(residuals.detach().numpy())\n",
    "\n",
    "# After training, plot the collected residuals for the specified epoch\n",
    "# Assuming `t` still holds the training points used in the last epoch\n",
    "plt.scatter(t.detach().numpy(), all_residuals[0], alpha=0.6)\n",
    "plt.xlabel('Training Points (t)')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residuals at Training Points')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df13307-7587-4330-af64-fcfc017b5e8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e8da46-ca62-4ee3-b9ef-fa4659087f38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bad782-f2b8-4abc-86c3-81232136f722",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138a1f2e-68c7-4cc5-8c69-ffb8044c3cf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d94f10-7676-422a-8957-94ecf85b59a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d6e4a3-6b0a-4ced-8037-a408eba22d59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25443f7-d90b-409d-8269-6db300b4ed32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have the weights and biases for the original network\n",
    "# Replace these with your actual values\n",
    "original_weights = {\n",
    "    'hidden_layer1': torch.randn(8, 2),\n",
    "    'hidden_layer2': torch.randn(8, 8),\n",
    "    'output_layer': torch.randn(1, 8),\n",
    "}\n",
    "\n",
    "original_biases = {\n",
    "    'hidden_layer1': torch.randn(8),\n",
    "    'hidden_layer2': torch.randn(8),\n",
    "    'output_layer': torch.randn(1),\n",
    "}\n",
    "\n",
    "# Define the architecture of the new network\n",
    "new_input_size = 2\n",
    "new_hidden_layers = [8, 8]\n",
    "new_output_size = 1\n",
    "\n",
    "# Create a new model with the same architecture as the original one\n",
    "class NewModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layers, output_size):\n",
    "        super(NewModel, self).__init__()\n",
    "        \n",
    "        # Input layer\n",
    "        self.input_layer = nn.Linear(input_size, hidden_layers[0])\n",
    "        \n",
    "        # Hidden layers with weights and biases from the original model\n",
    "        self.hidden_layers = nn.ModuleList([\n",
    "            nn.Linear(hidden_layers[i], hidden_layers[i+1]) for i in range(len(hidden_layers)-1)\n",
    "        ])\n",
    "        \n",
    "        # Output layer\n",
    "        self.output_layer = nn.Linear(hidden_layers[-1], output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.input_layer(x))\n",
    "        for layer in self.hidden_layers:\n",
    "            x = torch.relu(layer(x))\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "# Create an instance of the new model\n",
    "new_model = NewModel(new_input_size, new_hidden_layers, new_output_size)\n",
    "\n",
    "# Set the weights and biases from the original model\n",
    "new_model.input_layer.weight.data = torch.transpose(original_weights['hidden_layer1'], 0, 1)\n",
    "new_model.input_layer.bias.data = original_biases['hidden_layer1']\n",
    "\n",
    "for i, layer in enumerate(new_model.hidden_layers):\n",
    "    layer.weight.data = torch.transpose(original_weights[f'hidden_layer{i+2}'], 0, 1)\n",
    "    layer.bias.data = original_biases[f'hidden_layer{i+2}']\n",
    "\n",
    "new_model.output_layer.weight.data = torch.transpose(original_weights['output_layer'], 0, 1)\n",
    "new_model.output_layer.bias.data = original_biases['output_layer']\n",
    "\n",
    "# Visualize the differences in weights and biases\n",
    "def visualize_difference(original, new, layer_name):\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    \n",
    "    # Plot weights\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title(f'{layer_name} Weights')\n",
    "    plt.imshow(original, cmap='viridis')\n",
    "    plt.colorbar(label='Weight Values')\n",
    "    plt.xlabel('Neurons in Previous Layer')\n",
    "    plt.ylabel('Neurons in Current Layer')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.title(f'Difference in {layer_name} Weights')\n",
    "    plt.imshow(new - original, cmap='coolwarm')\n",
    "    plt.colorbar(label='Weight Difference')\n",
    "    plt.xlabel('Neurons in Previous Layer')\n",
    "    plt.ylabel('Neurons in Current Layer')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize differences for each layer\n",
    "for layer_name, original_weight in original_weights.items():\n",
    "    new_weight = getattr(new_model, layer_name).weight.data\n",
    "    visualize_difference(original_weight, new_weight, layer_name)\n",
    "\n",
    "# Visualize differences for biases\n",
    "for layer_name, original_bias in original_biases.items():\n",
    "    new_bias = getattr(new_model, layer_name).bias.data\n",
    "    visualize_difference(original_bias.unsqueeze(0), new_bias.unsqueeze(0), layer_name + ' Bias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21c9143-f857-489d-b2e3-5002526a88b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_weights(ax, weights, title):\n",
    "    im = ax.imshow(weights, cmap='coolwarm', interpolation='none')\n",
    "\n",
    "    # Add numbers to the cells\n",
    "    for i in range(weights.shape[0]):\n",
    "        for j in range(weights.shape[1]):\n",
    "            ax.text(j, i, f'{weights[i, j]:.2f}', color='white', ha='center', va='center')\n",
    "\n",
    "    ax.set_title(title)\n",
    "    return im\n",
    "\n",
    "# Define the neural network architecture\n",
    "layer_sizes = [2, 4, 8, 4, 1]\n",
    "\n",
    "# Initialize random weights\n",
    "np.random.seed(42)\n",
    "weights = [np.random.randn(layer_sizes[i], layer_sizes[i+1]) for i in range(len(layer_sizes)-1)]\n",
    "\n",
    "# Create a figure for visualization\n",
    "fig, axes = plt.subplots(1, len(layer_sizes)-1, figsize=(15, 5))\n",
    "\n",
    "# Plot the weights for each layer using a for-loop\n",
    "for i, ax in enumerate(axes):\n",
    "    im = plot_weights(ax, weights[i], f'Weights Layer {i+1}')\n",
    "\n",
    "    # Show the colorbars\n",
    "    fig.colorbar(im, ax=ax, orientation='horizontal')\n",
    "\n",
    "# Show the plots\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f79653-125e-40e6-b8fb-4e54d88d82ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.fc3 = nn.Linear(hidden_size2, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "def plot_weights(ax, weights, title):\n",
    "    im = ax.imshow(weights, cmap='viridis', interpolation='none')\n",
    "\n",
    "    # Add numbers to the cells\n",
    "    for i in range(weights.shape[0]):\n",
    "        for j in range(weights.shape[1]):\n",
    "            ax.text(j, i, f'{weights[i, j]:.2f}', color='white', ha='center', va='center')\n",
    "\n",
    "    ax.set_title(title)\n",
    "    return im\n",
    "\n",
    "# Define the neural network architecture\n",
    "input_size = 2\n",
    "hidden_size1 = 4\n",
    "hidden_size2 = 4\n",
    "output_size = 1\n",
    "\n",
    "# Instantiate the PyTorch model\n",
    "model = SimpleNN(input_size, hidden_size1, hidden_size2, output_size)\n",
    "\n",
    "# Load weights from the model\n",
    "state_dict = model.state_dict()\n",
    "\n",
    "# Extract weights from state_dict\n",
    "weights = [state_dict[key].numpy() for key in state_dict.keys()]\n",
    "\n",
    "# Create a figure for visualization\n",
    "fig, axes = plt.subplots(1, len(weights), figsize=(15, 5))\n",
    "\n",
    "# Plot the weights for each layer using a for-loop\n",
    "for i, ax in enumerate(axes):\n",
    "    im = plot_weights(ax, weights[i], f'Weights Layer {i+1}')\n",
    "\n",
    "    # Show the colorbars\n",
    "    fig.colorbar(im, ax=ax, orientation='vertical')\n",
    "\n",
    "# Show the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ecc82c-fb1d-4d91-b682-fc5009231f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e2b93b-bc9c-4372-81a1-57ff6125a442",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.fc3 = nn.Linear(hidden_size2, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "def plot_weights(ax, weights, title):\n",
    "    if len(weights.shape) == 1:\n",
    "        # If the weights are 1D (possibly biases), reshape them to (1, len(weights))\n",
    "        weights = weights.reshape(1, -1)\n",
    "    im = ax.imshow(weights, cmap='viridis', interpolation='none')\n",
    "\n",
    "    # Add numbers to the cells\n",
    "    for i in range(weights.shape[0]):\n",
    "        for j in range(weights.shape[1]):\n",
    "            ax.text(j, i, f'{weights[i, j]:.2f}', color='black', ha='center', va='center')\n",
    "\n",
    "    ax.set_title(title)\n",
    "    return im\n",
    "\n",
    "# Define the neural network architecture\n",
    "input_size = 2\n",
    "hidden_size1 = 4\n",
    "hidden_size2 = 4\n",
    "output_size = 1\n",
    "\n",
    "# Instantiate the PyTorch model\n",
    "model = SimpleNN(input_size, hidden_size1, hidden_size2, output_size)\n",
    "\n",
    "# Load weights from the model\n",
    "state_dict = model.state_dict()\n",
    "\n",
    "# Extract weights from state_dict\n",
    "weights = [state_dict[key].numpy() for key in state_dict.keys()]\n",
    "\n",
    "# Create a figure for visualization\n",
    "fig, axes = plt.subplots(1, len(weights), figsize=(15, 5))\n",
    "\n",
    "# Plot the weights for each layer using a for-loop\n",
    "for i, ax in enumerate(axes):\n",
    "    im = plot_weights(ax, weights[i], f'Weights Layer {i+1}')\n",
    "\n",
    "    # Show the colorbars\n",
    "    fig.colorbar(im, ax=ax, orientation='horizontal')\n",
    "\n",
    "# Show the plots\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7fd272-7286-4fc7-bc0c-10930b19f7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf248e3-097e-4154-a886-d31df6d9f096",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf4598e-ae08-4009-9a5b-27915ec1d3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.fc3 = nn.Linear(hidden_size2, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "def plot_weights(ax, weights, title):\n",
    "    if len(weights.shape) == 1:\n",
    "        # If the weights are 1D (possibly biases), reshape them to (1, len(weights))\n",
    "        weights = weights.reshape(1, -1)\n",
    "    im = ax.imshow(weights, cmap='viridis', interpolation='none')\n",
    "\n",
    "    # Add numbers to the cells\n",
    "    for i in range(weights.shape[0]):\n",
    "        for j in range(weights.shape[1]):\n",
    "            ax.text(j, i, f'{weights[i, j]:.2f}', color='white', ha='center', va='center')\n",
    "\n",
    "    ax.set_title(title)\n",
    "    return im\n",
    "\n",
    "# Define the neural network architecture\n",
    "input_size = 2\n",
    "hidden_size1 = 4\n",
    "hidden_size2 = 4\n",
    "output_size = 1\n",
    "\n",
    "# Instantiate the PyTorch model\n",
    "model = SimpleNN(input_size, hidden_size1, hidden_size2, output_size)\n",
    "\n",
    "# Load weights and biases from the model\n",
    "state_dict = model.state_dict()\n",
    "\n",
    "# Extract weights and biases from state_dict\n",
    "weights = [state_dict[key].numpy() for key in state_dict.keys()]\n",
    "\n",
    "# Create a figure for visualization\n",
    "fig, axes = plt.subplots(1, len(weights), figsize=(15, 5))\n",
    "\n",
    "# Plot the weights and biases for each layer using a for-loop\n",
    "for i, ax in enumerate(axes):\n",
    "    im = plot_weights(ax, weights[i], f'Layer {i+1}')\n",
    "\n",
    "    # Show the colorbars\n",
    "    #fig.colorbar(im, ax=ax, orientation='vertical')\n",
    "\n",
    "# Show the plots\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16750952-921f-4553-b7d8-1cd1b33e71b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.fc3 = nn.Linear(hidden_size2, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "def plot_weights(ax, weights, title, labels):\n",
    "    if len(weights.shape) == 1:\n",
    "        # If the weights are 1D (possibly biases), reshape them to (1, len(weights))\n",
    "        weights = weights.reshape(1, -1)\n",
    "    im = ax.imshow(weights, cmap='viridis', interpolation='none')\n",
    "\n",
    "    # Add numbers and labels to the cells\n",
    "    for i in range(weights.shape[0]):\n",
    "        for j in range(weights.shape[1]):\n",
    "            ax.text(j, i, f'{weights[i, j]:.2f}\\n{labels[i, j]}', color='white', ha='center', va='center')\n",
    "\n",
    "    ax.set_title(title)\n",
    "    return im\n",
    "\n",
    "# Define the neural network architecture\n",
    "input_size = 2\n",
    "hidden_size1 = 4\n",
    "hidden_size2 = 4\n",
    "output_size = 1\n",
    "\n",
    "# Instantiate the PyTorch model\n",
    "model = SimpleNN(input_size, hidden_size1, hidden_size2, output_size)\n",
    "\n",
    "# Load weights and biases from the model\n",
    "state_dict = model.state_dict()\n",
    "\n",
    "# Extract weights and biases from state_dict\n",
    "weights = [state_dict[key].numpy() for key in state_dict.keys()]\n",
    "\n",
    "# Create labels to indicate weights or biases\n",
    "labels = np.array([['Weight' if 'weight' in key else 'Bias' for key in state_dict.keys()]])\n",
    "\n",
    "# Create a figure for visualization\n",
    "fig, axes = plt.subplots(1, len(weights), figsize=(15, 5))\n",
    "\n",
    "# Plot the weights and biases for each layer using a for-loop\n",
    "for i, ax in enumerate(axes):\n",
    "    im = plot_weights(ax, weights[i], f'Layer {i+1}', labels)\n",
    "\n",
    "    # Show the colorbars\n",
    "    fig.colorbar(im, ax=ax, orientation='vertical')\n",
    "\n",
    "# Show the plots\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb78ada7-6e69-4a32-a1d9-80b36cc856e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.fc3 = nn.Linear(hidden_size2, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "def plot_weights(ax, weights, title, labels):\n",
    "    if len(weights.shape) == 1:\n",
    "        # If the weights are 1D (possibly biases), reshape them to (1, len(weights))\n",
    "        weights = weights.reshape(1, -1)\n",
    "    im = ax.imshow(weights, cmap='viridis', interpolation='none')\n",
    "\n",
    "    # Add numbers and labels to the cells\n",
    "    for i in range(weights.shape[0]):\n",
    "        for j in range(weights.shape[1]):\n",
    "            ax.text(j, i, f'{weights[i, j]:.2f}\\n{labels[i, j]}', color='white', ha='center', va='center')\n",
    "\n",
    "    ax.set_title(title)\n",
    "    return im\n",
    "\n",
    "# Define the neural network architecture\n",
    "input_size = 2\n",
    "hidden_size1 = 4\n",
    "hidden_size2 = 4\n",
    "output_size = 1\n",
    "\n",
    "# Instantiate the PyTorch model\n",
    "model = SimpleNN(input_size, hidden_size1, hidden_size2, output_size)\n",
    "\n",
    "# Load weights and biases from the model\n",
    "state_dict = model.state_dict()\n",
    "\n",
    "# Extract weights and biases from state_dict\n",
    "weights = [state_dict[key].numpy() for key in state_dict.keys()]\n",
    "\n",
    "# Create labels to indicate weights or biases\n",
    "labels = np.array([['Weight' if 'weight' in key else 'Bias' for key in state_dict.keys()]])  # Note the extra square brackets\n",
    "\n",
    "# Create a figure for visualization\n",
    "fig, axes = plt.subplots(1, len(weights), figsize=(15, 5))\n",
    "\n",
    "# Plot the weights and biases for each layer using a for-loop\n",
    "for i, ax in enumerate(axes):\n",
    "    im = plot_weights(ax, weights[i], f'Layer {i+1}', labels)\n",
    "\n",
    "    # Show the colorbars\n",
    "    fig.colorbar(im, ax=ax, orientation='vertical')\n",
    "\n",
    "# Show the plots\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad298fdb-673c-42db-af69-79ab371b8803",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.fc3 = nn.Linear(hidden_size2, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "def plot_weights(ax, weights, title, labels):\n",
    "    if len(weights.shape) == 1:\n",
    "        # If the weights are 1D (possibly biases), reshape them to (1, len(weights))\n",
    "        weights = weights.reshape(1, -1)\n",
    "    im = ax.imshow(weights, cmap='viridis', interpolation='none')\n",
    "\n",
    "    # Add numbers and labels to the cells\n",
    "    for i in range(weights.shape[0]):\n",
    "        for j in range(weights.shape[1]):\n",
    "            ax.text(j, i, f'{weights[i, j]:.2f}\\n{labels[i, j]}', color='white', ha='center', va='center')\n",
    "\n",
    "    ax.set_title(title)\n",
    "    return im\n",
    "\n",
    "# Define the neural network architecture\n",
    "input_size = 2\n",
    "hidden_size1 = 4\n",
    "hidden_size2 = 4\n",
    "output_size = 1\n",
    "\n",
    "# Instantiate the PyTorch model\n",
    "model = SimpleNN(input_size, hidden_size1, hidden_size2, output_size)\n",
    "\n",
    "# Load weights and biases from the model\n",
    "state_dict = model.state_dict()\n",
    "\n",
    "# Extract weights and biases from state_dict\n",
    "weights = [state_dict[key].numpy() for key in state_dict.keys()]\n",
    "\n",
    "# Create labels to indicate weights or biases\n",
    "labels = np.array([['Weight' if 'weight' in key else 'Bias' for key in state_dict.keys()]])\n",
    "\n",
    "# Create a figure for visualization\n",
    "fig, axes = plt.subplots(1, len(weights), figsize=(15, 5))\n",
    "\n",
    "# Plot the weights and biases for each layer using a for-loop\n",
    "for i, ax in enumerate(axes):\n",
    "    im = plot_weights(ax, weights[i], f'Layer {i+1}', labels)\n",
    "\n",
    "    # Show the colorbars\n",
    "    fig.colorbar(im, ax=ax, orientation='vertical')\n",
    "\n",
    "# Show the plots\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e298ec2-8613-47d2-bb4d-1df53fa9e736",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc60fb4a-401c-4709-955c-196ca5eae7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f670323e-a753-4f5f-8a01-1339a3104cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the architecture of the original network\n",
    "original_input_size = 2\n",
    "original_hidden_layers = [8, 8]\n",
    "original_output_size = 1\n",
    "\n",
    "# Create the original model with Xavier initialization\n",
    "class OriginalModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layers, output_size):\n",
    "        super(OriginalModel, self).__init__()\n",
    "        \n",
    "        # Input layer\n",
    "        self.input_layer = nn.Linear(input_size, hidden_layers[0])\n",
    "        init.xavier_uniform_(self.input_layer.weight.data, gain=nn.init.calculate_gain('relu'))\n",
    "        init.zeros_(self.input_layer.bias.data)\n",
    "        \n",
    "        # Hidden layers with Xavier initialization\n",
    "        self.hidden_layers = nn.ModuleList([\n",
    "            nn.Linear(hidden_layers[i], hidden_layers[i+1]) for i in range(len(hidden_layers)-1)\n",
    "        ])\n",
    "        for layer in self.hidden_layers:\n",
    "            init.xavier_uniform_(layer.weight.data, gain=nn.init.calculate_gain('relu'))\n",
    "            init.zeros_(layer.bias.data)\n",
    "        \n",
    "        # Output layer with Xavier initialization\n",
    "        self.output_layer = nn.Linear(hidden_layers[-1], output_size)\n",
    "        init.xavier_uniform_(self.output_layer.weight.data, gain=nn.init.calculate_gain('linear'))\n",
    "        init.zeros_(self.output_layer.bias.data)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.input_layer(x))\n",
    "        for layer in self.hidden_layers:\n",
    "            x = torch.relu(layer(x))\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "# Create an instance of the original model\n",
    "original_model = OriginalModel(original_input_size, original_hidden_layers, original_output_size)\n",
    "\n",
    "# Define the architecture of the new network with increased neurons\n",
    "new_input_size = 2\n",
    "new_hidden_layers = [16, 16]  # Increase the number of neurons\n",
    "new_output_size = 1\n",
    "\n",
    "# Create a new model with the same architecture as the original one\n",
    "class NewModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layers, output_size):\n",
    "        super(NewModel, self).__init__()\n",
    "        \n",
    "        # Input layer\n",
    "        self.input_layer = nn.Linear(input_size, hidden_layers[0])\n",
    "        \n",
    "        # Hidden layers with weights and biases from the original model\n",
    "        self.hidden_layers = nn.ModuleList([\n",
    "            nn.Linear(hidden_layers[i], hidden_layers[i+1]) for i in range(len(hidden_layers)-1)\n",
    "        ])\n",
    "        \n",
    "        # Output layer\n",
    "        self.output_layer = nn.Linear(hidden_layers[-1], output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.input_layer(x))\n",
    "        for layer in self.hidden_layers:\n",
    "            x = torch.relu(layer(x))\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "# Create an instance of the new model\n",
    "new_model = NewModel(new_input_size, new_hidden_layers, new_output_size)\n",
    "\n",
    "# Set the weights from the original model using He initialization\n",
    "def initialize_weights_he(model):\n",
    "    for layer in model.modules():\n",
    "        if isinstance(layer, nn.Linear):\n",
    "            init.kaiming_uniform_(layer.weight.data, mode='fan_in', nonlinearity='relu')\n",
    "            init.zeros_(layer.bias.data)\n",
    "\n",
    "# Initialize weights of the new model using He initialization\n",
    "initialize_weights_he(new_model)\n",
    "\n",
    "# Visualize the differences in weights and biases\n",
    "def visualize_difference(original, new, layer_name):\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    \n",
    "    # Plot weights\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title(f'{layer_name} Weights')\n",
    "    plt.imshow(original, cmap='viridis')\n",
    "    plt.colorbar(label='Weight Values')\n",
    "    plt.xlabel('Neurons in Previous Layer')\n",
    "    plt.ylabel('Neurons in Current Layer')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.title(f'Difference in {layer_name} Weights')\n",
    "    plt.imshow(new - original, cmap='coolwarm')\n",
    "    plt.colorbar(label='Weight Difference')\n",
    "    plt.xlabel('Neurons in Previous Layer')\n",
    "    plt.ylabel('Neurons in Current Layer')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize differences for each layer\n",
    "for layer_name, original_weight in original_model.state_dict().items():\n",
    "    new_weight = new_model.state_dict()[layer_name]\n",
    "    visualize_difference(original_weight, new_weight, layer_name)\n",
    "\n",
    "# Visualize differences for biases\n",
    "for layer_name, original_bias in original_model.state_dict().items():\n",
    "    if 'bias' in layer_name:\n",
    "        new_bias = new_model.state_dict()[layer_name]\n",
    "        visualize_difference(original_bias.unsqueeze(0), new_bias.unsqueeze(0), layer_name + ' Bias')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8807ef4d-b443-481d-9d9a-eaae0d389ab7",
   "metadata": {},
   "source": [
    "# ADAPTIVE SCHEME II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3695ebe-797f-4ff5-a2d8-510dd00f7bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the original model\n",
    "class OriginalModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(OriginalModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(2, 4)\n",
    "        self.fc2 = nn.Linear(4, 4)\n",
    "        self.output = nn.Linear(4, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "# Train and save the original model\n",
    "original_model = OriginalModel()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(original_model.parameters(), lr=0.01)\n",
    "\n",
    "# Dummy data for training\n",
    "input_data = torch.rand((100, 2))\n",
    "target_data = torch.rand((100, 1))\n",
    "\n",
    "original_weights = []  # List to store original model weights\n",
    "\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    output = original_model(input_data)\n",
    "    loss = criterion(output, target_data)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Store the weights for visualization\n",
    "    original_weights.append(original_model.fc1.weight.data.numpy().flatten())\n",
    "\n",
    "# Save the weights and biases\n",
    "torch.save(original_model.state_dict(), 'original_model.pth')\n",
    "\n",
    "# Define the new model with an additional hidden layer\n",
    "class NewModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NewModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(2, 4)\n",
    "        self.fc2 = nn.Linear(4, 4)\n",
    "        self.new_layer = nn.Linear(4, 4)  # Additional hidden layer\n",
    "        self.output = nn.Linear(4, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.new_layer(x))\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the new model with the weights and biases of the original model\n",
    "new_model = NewModel()\n",
    "new_model.load_state_dict(torch.load('original_model.pth'))\n",
    "\n",
    "# Check the weights of the new model\n",
    "new_weights = []\n",
    "for epoch in range(10):  # Only for visualization, you can further train as needed\n",
    "    # Store the weights for visualization\n",
    "    new_weights.append(new_model.fc1.weight.data.numpy().flatten())\n",
    "\n",
    "# Plotting the weights\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(original_weights)\n",
    "plt.title('Original Model Weights')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Weight Values')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(new_weights)\n",
    "plt.title('New Model Weights (Initialized from Original)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Weight Values')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad98b63-762c-45c1-b7cc-9c3c88f57168",
   "metadata": {},
   "source": [
    "# Initialization and activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eba85e4-23a0-4a24-9e9d-57a9f153d72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "\n",
    "class FCN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_sizes, activation='relu', initialization='xavier'):\n",
    "        super(FCN, self).__init__()\n",
    "\n",
    "        # Define input and output layers\n",
    "        self.input_layer = nn.Linear(input_size, hidden_sizes[0])\n",
    "        self.output_layer = nn.Linear(hidden_sizes[-1], output_size)\n",
    "\n",
    "        # Define hidden layers\n",
    "        self.hidden_layers = nn.ModuleList([\n",
    "            nn.Linear(in_size, out_size) for in_size, out_size in zip(hidden_sizes[:-1], hidden_sizes[1:])\n",
    "        ])\n",
    "\n",
    "        # Define ModuleDict for activation functions\n",
    "        self.activations = nn.ModuleDict({\n",
    "            'relu': nn.ReLU(),\n",
    "            'tanh': nn.Tanh(),\n",
    "        })\n",
    "\n",
    "        # Initialize layers\n",
    "        self.init_weights(initialization)\n",
    "\n",
    "        # Set activation function\n",
    "        if activation not in self.activations:\n",
    "            raise ValueError(\"Unsupported activation function\")\n",
    "        self.activation = self.activations[activation]\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_layer(x)\n",
    "\n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            x = self.activation(hidden_layer(x))\n",
    "\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "    def init_weights(self, initialization):\n",
    "        # Initialize input layer\n",
    "        if initialization == 'xavier':\n",
    "            init.xavier_uniform_(self.input_layer.weight)\n",
    "        elif initialization == 'kaiming':\n",
    "            init.kaiming_uniform_(self.input_layer.weight, nonlinearity='relu')\n",
    "        elif initialization == 'zeros':\n",
    "            init.zeros_(self.input_layer.weight)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported initialization type\")\n",
    "\n",
    "        # Initialize hidden layers\n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            if initialization == 'xavier':\n",
    "                init.xavier_uniform_(hidden_layer.weight)\n",
    "            elif initialization == 'kaiming':\n",
    "                init.kaiming_uniform_(hidden_layer.weight, nonlinearity='relu')\n",
    "            elif initialization == 'zeros':\n",
    "                init.zeros_(hidden_layer.weight)\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported initialization type\")\n",
    "\n",
    "        # Initialize output layer\n",
    "        if initialization == 'xavier':\n",
    "            init.xavier_uniform_(self.output_layer.weight)\n",
    "        elif initialization == 'kaiming':\n",
    "            init.kaiming_uniform_(self.output_layer.weight, nonlinearity='relu')\n",
    "        elif initialization == 'zeros':\n",
    "            init.zeros_(self.output_layer.weight)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported initialization type\")\n",
    "\n",
    "# Example usage:\n",
    "input_size = 10\n",
    "output_size = 5\n",
    "hidden_sizes = [20, 30, 15, 10, 25]  # You can specify any number of neurons in each hidden layer\n",
    "\n",
    "# Create an instance of FCN with ReLU activation and Xavier initialization\n",
    "model = FCN(input_size, output_size, hidden_sizes, activation='relu', initialization='xavier')\n",
    "\n",
    "# Check the model architecture\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee48140-fe23-422a-9c95-d78e16dd4743",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-08T11:22:43.139375Z",
     "iopub.status.busy": "2024-02-08T11:22:43.138785Z",
     "iopub.status.idle": "2024-02-08T11:22:43.145227Z",
     "shell.execute_reply": "2024-02-08T11:22:43.144152Z",
     "shell.execute_reply.started": "2024-02-08T11:22:43.139338Z"
    }
   },
   "source": [
    "## Visualization of the weights and biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a0db1a-1f39-47fb-abce-b6acbe6d6608",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.fc3 = nn.Linear(hidden_size2, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "def plot_weights(ax, weights, title):\n",
    "    if len(weights.shape) == 1:\n",
    "        # If the weights are 1D (possibly biases), reshape them to (1, len(weights))\n",
    "        weights = weights.reshape(1, -1)\n",
    "    im = ax.imshow(weights, cmap='viridis', interpolation='none')\n",
    "\n",
    "    # Add numbers to the cells\n",
    "    for i in range(weights.shape[0]):\n",
    "        for j in range(weights.shape[1]):\n",
    "            ax.text(j, i, f'{weights[i, j]:.2f}', color='white', ha='center', va='center')\n",
    "\n",
    "    ax.set_title(title)\n",
    "    return im\n",
    "\n",
    "# Define the neural network architecture\n",
    "input_size = 2\n",
    "hidden_size1 = 4\n",
    "hidden_size2 = 4\n",
    "output_size = 1\n",
    "\n",
    "# Instantiate the PyTorch model\n",
    "model = SimpleNN(input_size, hidden_size1, hidden_size2, output_size)\n",
    "\n",
    "# Load weights and biases from the model\n",
    "state_dict = model.state_dict()\n",
    "\n",
    "# Extract weights and biases from state_dict\n",
    "weights = [state_dict[key].numpy() for key in state_dict.keys()]\n",
    "\n",
    "# Create a figure for visualization\n",
    "fig, axes = plt.subplots(1, len(weights), figsize=(15, 5))\n",
    "\n",
    "# Plot the weights and biases for each layer using a for-loop\n",
    "for i, ax in enumerate(axes):\n",
    "    im = plot_weights(ax, weights[i], f'Layer {i+1}')\n",
    "\n",
    "    # Show the colorbars\n",
    "    #fig.colorbar(im, ax=ax, orientation='vertical')\n",
    "\n",
    "# Show the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4c02d2-c029-474c-b95c-1c7f2cb00ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.fc3 = nn.Linear(hidden_size2, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "def plot_weights(ax, weights, title):\n",
    "    if len(weights.shape) == 1:\n",
    "        # If the weights are 1D (possibly biases), reshape them to (1, len(weights))\n",
    "        weights = weights.reshape(1, -1)\n",
    "    im = ax.imshow(weights, cmap='viridis', interpolation='none')\n",
    "\n",
    "    # Add numbers to the cells\n",
    "    for i in range(weights.shape[0]):\n",
    "        for j in range(weights.shape[1]):\n",
    "            ax.text(j, i, f'{weights[i, j]:.2f}', color='white', ha='center', va='center')\n",
    "\n",
    "    ax.set_title(title)\n",
    "\n",
    "    return im\n",
    "\n",
    "# Define the neural network architecture\n",
    "input_size = 2\n",
    "hidden_size1 = 4\n",
    "hidden_size2 = 4\n",
    "output_size = 1\n",
    "\n",
    "# Instantiate the PyTorch model\n",
    "model = SimpleNN(input_size, hidden_size1, hidden_size2, output_size)\n",
    "\n",
    "# Load weights and biases from the model\n",
    "state_dict = model.state_dict()\n",
    "\n",
    "# Filter out keys containing \"bias\"\n",
    "weights_dict = {key: value for key, value in state_dict.items() if 'bias' not in key}\n",
    "\n",
    "# Create a figure for visualization\n",
    "fig, axes = plt.subplots(1, len(weights_dict), figsize=(15, 5))\n",
    "\n",
    "# Plot the weights for each layer using a for-loop\n",
    "for i, (key, weights) in enumerate(weights_dict.items()):\n",
    "    im = plot_weights(axes[i], weights.numpy(), f'{key}')\n",
    "\n",
    "# Show the plots\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7d1c31-0eca-4e88-8092-6042fef13bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.fc3 = nn.Linear(hidden_size2, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "def plot_weights(ax, weights, title):\n",
    "    if len(weights.shape) == 1:\n",
    "        # If the weights are 1D (possibly biases), reshape them to (1, len(weights))\n",
    "        weights = weights.reshape(1, -1)\n",
    "    im = ax.imshow(weights, cmap='viridis', interpolation='none')\n",
    "\n",
    "    # Add numbers to the cells\n",
    "    for i in range(weights.shape[0]):\n",
    "        for j in range(weights.shape[1]):\n",
    "            ax.text(j, i, f'{weights[i, j]:.2f}', color='white', ha='center', va='center')\n",
    "\n",
    "    ax.set_title(title)\n",
    "    return im\n",
    "\n",
    "# Define the neural network architecture\n",
    "input_size = 2\n",
    "hidden_size1 = 4\n",
    "hidden_size2 = 4\n",
    "output_size = 1\n",
    "\n",
    "# Instantiate the PyTorch model\n",
    "model = SimpleNN(input_size, hidden_size1, hidden_size2, output_size)\n",
    "\n",
    "# Load weights and biases from the model\n",
    "state_dict = model.state_dict()\n",
    "\n",
    "# Combine both dictionaries for weights and biases into a single dictionary\n",
    "weights_biases_dict = {key: value for key, value in state_dict.items()}\n",
    "\n",
    "# Create a figure for visualization\n",
    "fig, axes = plt.subplots(1, len(weights_biases_dict), figsize=(15, 5))\n",
    "\n",
    "# Plot the weights and biases for each layer\n",
    "for i, (key, data) in enumerate(weights_biases_dict.items()):\n",
    "    if 'bias' in key:\n",
    "        im = plot_weights(axes[i], data.numpy().reshape(-1, 1), f'{key} (Biases)')\n",
    "    elif 'weight' in key:\n",
    "        im = plot_weights(axes[i], data.numpy(), f'{key} (Weights)')\n",
    "# Add a colorbar for all the plots\n",
    "divider = make_axes_locatable(axes[-3])\n",
    "cax = divider.append_axes(\"bottom\", size=\"5%\", pad=0.5)\n",
    "cbar = plt.colorbar(im, cax=cax, orientation='horizontal')\n",
    "cbar.set_label('Value')\n",
    "# Show the plots\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cbb7d7-f8e6-4142-b597-9b17f958b523",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.fc3 = nn.Linear(hidden_size2, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "def plot_weights(ax, weights, title):\n",
    "    if len(weights.shape) == 1:\n",
    "        # If the weights are 1D (possibly biases), reshape them to (1, len(weights))\n",
    "        weights = weights.reshape(1, -1)\n",
    "    im = ax.imshow(weights, cmap='viridis', interpolation='none')\n",
    "\n",
    "    # Add numbers to the cells\n",
    "    for i in range(weights.shape[0]):\n",
    "        for j in range(weights.shape[1]):\n",
    "            ax.text(j, i, f'{weights[i, j]:.2f}', color='white', ha='center', va='center')\n",
    "\n",
    "    ax.set_title(title)\n",
    "    return im\n",
    "\n",
    "# Define the neural network architecture\n",
    "input_size = 2\n",
    "hidden_size1 = 4\n",
    "hidden_size2 = 4\n",
    "output_size = 1\n",
    "\n",
    "# Instantiate the PyTorch model\n",
    "model = SimpleNN(input_size, hidden_size1, hidden_size2, output_size)\n",
    "\n",
    "# Load weights and biases from the model\n",
    "state_dict = model.state_dict()\n",
    "\n",
    "# Combine both dictionaries for weights and biases into a single dictionary\n",
    "weights_biases_dict = {key: value for key, value in state_dict.items()}\n",
    "\n",
    "# Create a figure for visualization\n",
    "fig, axes = plt.subplots(1, len(weights_biases_dict), figsize=(15, 5))\n",
    "\n",
    "# Plot the weights and biases for each layer\n",
    "max_intensity = 0  # Track the maximum intensity across all plots\n",
    "for i, (key, data) in enumerate(weights_biases_dict.items()):\n",
    "    if 'bias' in key:\n",
    "        im = plot_weights(axes[i], data.numpy().reshape(-1, 1), f'{key} (Biases)')\n",
    "    elif 'weight' in key:\n",
    "        im = plot_weights(axes[i], data.numpy(), f'{key} (Weights)')\n",
    "    \n",
    "    max_intensity = max(max_intensity, np.max(np.abs(data.numpy())))  # Update max intensity\n",
    "\n",
    "# Add a colorbar below all the plots\n",
    "divider = make_axes_locatable(axes[-1])\n",
    "cax = divider.append_axes(\"bottom\", size=\"10%\", pad=0.5)\n",
    "cbar = plt.colorbar(im, cax=cax, orientation='horizontal')\n",
    "cbar.set_label('Value')\n",
    "\n",
    "# Normalize the color bar based on the maximum intensity across all plots\n",
    "cbar.set_clim(-max_intensity, max_intensity)\n",
    "\n",
    "# Show the plots\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fb2b5d-6a51-4366-82a4-6b8e2d768433",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.fc3 = nn.Linear(hidden_size2, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def plot_weights(self):\n",
    "        state_dict = self.state_dict()\n",
    "        weights_biases_dict = {key: value for key, value in state_dict.items()}\n",
    "        \n",
    "        # Create a figure for visualization\n",
    "        num_plots = len(weights_biases_dict)\n",
    "        fig, axes = plt.subplots(1, num_plots, figsize=(5*num_plots, 5))\n",
    "\n",
    "        # Plot the weights and biases for each layer\n",
    "        #max_intensity = 0  # Track the maximum intensity across all plots\n",
    "        for i, (key, data) in enumerate(weights_biases_dict.items()):\n",
    "            ax = axes[i] if num_plots > 1 else axes\n",
    "            if 'bias' in key:\n",
    "                title = f'{key} (Biases)'\n",
    "            elif 'weight' in key:\n",
    "                title = f'{key} (Weights)'\n",
    "                \n",
    "            if len(data.shape) == 1:\n",
    "                # If the data is 1D (possibly biases), reshape them to (1, len(data))\n",
    "                data = data.reshape(1, -1)\n",
    "                \n",
    "            im = ax.imshow(data.numpy(), cmap='viridis', interpolation='none')\n",
    "\n",
    "            # Add numbers to the cells\n",
    "            for i in range(data.shape[0]):\n",
    "                for j in range(data.shape[1]):\n",
    "                    ax.text(j, i, f'{data[i, j]:.2f}', color='white', ha='center', va='center')\n",
    "\n",
    "            ax.set_title(title)\n",
    "            #max_intensity = max(max_intensity, np.max(np.abs(data.numpy())))  # Update max intensity\n",
    "\n",
    "        # Add a colorbar below all the plots\n",
    "        #divider = make_axes_locatable(ax)\n",
    "        #cax = divider.append_axes(\"bottom\", size=\"5%\", pad=0.5)\n",
    "        #cbar = plt.colorbar(im, cax=cax, orientation='horizontal')\n",
    "        #cbar.set_label('Value')\n",
    "\n",
    "        # Normalize the color bar based on the maximum intensity across all plots\n",
    "        #cbar.set_clim(-max_intensity, max_intensity)\n",
    "\n",
    "        # Show the plots\n",
    "        plt.show()\n",
    "\n",
    "# Define the neural network architecture\n",
    "input_size = 2\n",
    "hidden_size1 = 4\n",
    "hidden_size2 = 4\n",
    "output_size = 1\n",
    "\n",
    "# Instantiate the PyTorch model\n",
    "model = SimpleNN(input_size, hidden_size1, hidden_size2, output_size)\n",
    "\n",
    "# Call the plot_weights method\n",
    "model.plot_weights()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be38e34b-a6a9-4189-a187-877cf854f2fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b66da6-c48d-4b1d-8bbb-ffc11e2b16f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class FCN(nn.Module):\n",
    "    def __init__(self, N_INPUT, N_OUTPUT, hidden_layers, activation='Tanh', initialization='He'):\n",
    "        super().__init__()\n",
    "\n",
    "        self.activation_functions = nn.ModuleDict([\n",
    "            [\"Tanh\", nn.Tanh()],\n",
    "            [\"ReLU\", nn.ReLU()],\n",
    "            [\"LeakyReLU\", nn.LeakyReLU()],\n",
    "            [\"Sigmoid\", nn.Sigmoid()],\n",
    "            [\"Softmax\", nn.Softmax(dim=-1)],\n",
    "        ])\n",
    "\n",
    "        if activation not in self.activation_functions:\n",
    "            raise ValueError(\"Unsupported activation function\")\n",
    "\n",
    "        self.activation = self.activation_functions[activation]\n",
    "\n",
    "        self.fcs = nn.Sequential(\n",
    "            nn.Linear(N_INPUT, hidden_layers[0]),\n",
    "            self.activation\n",
    "        )\n",
    "\n",
    "        self.fch = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(hidden_layers[i], hidden_layers[i + 1]),\n",
    "                self.activation\n",
    "            ) for i in range(len(hidden_layers) - 1)\n",
    "        ])\n",
    "\n",
    "        self.fce = nn.Linear(hidden_layers[-1], N_OUTPUT)\n",
    "\n",
    "        self.initialize_parameters(initialization)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fcs(x)\n",
    "        for layer in self.fch:\n",
    "            x = self.activation(layer(x))\n",
    "        x = self.fce(x)\n",
    "        return x\n",
    "    \n",
    "    def initialize_parameters(self, initialization):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                if initialization == 'Uniform':\n",
    "                    init.uniform_(module.weight.data, -0.1, 0.1)\n",
    "                    init.zeros_(module.bias.data)\n",
    "                elif initialization == 'Normal':\n",
    "                    init.normal_(module.weight.data, mean=0, std=0.01)\n",
    "                    init.zeros_(module.bias.data)\n",
    "                elif initialization == 'Xavier':\n",
    "                    init.xavier_uniform_(module.weight.data)\n",
    "                    init.zeros_(module.bias.data)\n",
    "                elif initialization == 'He':\n",
    "                    init.kaiming_uniform_(module.weight.data, mode='fan_in', nonlinearity='relu')\n",
    "                    init.zeros_(module.bias.data)\n",
    "                elif initialization == 'Orthogonal':\n",
    "                    init.orthogonal_(module.weight.data)\n",
    "                    init.zeros_(module.bias.data)\n",
    "                elif initialization == 'Kaiming':\n",
    "                    init.kaiming_uniform_(module.weight.data, mode='fan_in', nonlinearity='leaky_relu')\n",
    "                    init.zeros_(module.bias.data)\n",
    "                else:\n",
    "                    raise ValueError(\"Unsupported initialization type\")\n",
    "\n",
    "    def plot_weights(self):\n",
    "        state_dict = self.state_dict()\n",
    "        weights_biases_dict = {key: value for key, value in state_dict.items()}\n",
    "        \n",
    "        # Create a figure for visualization\n",
    "        num_subplots = len(weights_biases_dict)\n",
    "        num_cols = 2\n",
    "        num_rows = (num_subplots + num_cols -1) // num_cols\n",
    "        #fig, axes = plt.subplots(1, num_plots, figsize=(5*num_plots, 5))\n",
    "        fig, axes = plt.subplots(num_rows,\n",
    "                                 ncols, \n",
    "                                 figsize=(5*num_plots, 5)\n",
    "                                )\n",
    "\n",
    "        # Plot the weights and biases for each layer\n",
    "        #max_intensity = 0  # Track the maximum intensity across all plots\n",
    "        for i, (key, data) in enumerate(weights_biases_dict.items()):\n",
    "            ax = axes[i] if num_plots > 1 else axes\n",
    "            if 'bias' in key:\n",
    "                title = f'{key} (Biases)'\n",
    "            elif 'weight' in key:\n",
    "                title = f'{key} (Weights)'\n",
    "                \n",
    "            if len(data.shape) == 1:\n",
    "                # If the data is 1D (possibly biases), reshape them to (1, len(data))\n",
    "                data = data.reshape(1, -1)\n",
    "                \n",
    "            im = ax.imshow(data.numpy(), cmap='viridis', interpolation='none')\n",
    "\n",
    "            # Add numbers to the cells\n",
    "            for i in range(data.shape[0]):\n",
    "                for j in range(data.shape[1]):\n",
    "                    ax.text(j, i, f'{data[i, j]:.2f}', color='white', ha='center', va='center')\n",
    "\n",
    "            ax.set_title(title)\n",
    "            #max_intensity = max(max_intensity, np.max(np.abs(data.numpy())))  # Update max intensity\n",
    "\n",
    "        # Add a colorbar below all the plots\n",
    "        #divider = make_axes_locatable(ax)\n",
    "        #cax = divider.append_axes(\"bottom\", size=\"5%\", pad=0.5)\n",
    "        #cbar = plt.colorbar(im, cax=cax, orientation='horizontal')\n",
    "        #cbar.set_label('Value')\n",
    "\n",
    "        # Normalize the color bar based on the maximum intensity across all plots\n",
    "        #cbar.set_clim(-max_intensity, max_intensity)\n",
    "\n",
    "        # Show the plots\n",
    "        plt.show()\n",
    "\n",
    "    def visualize_difference(self, original, new, layer_name):\n",
    "        plt.figure(figsize=(10, 4))\n",
    "\n",
    "        # Plot weights\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.title(f'{layer_name} Weights')\n",
    "        plt.imshow(original, cmap='viridis')\n",
    "        plt.colorbar(label='Weight Values')\n",
    "        plt.xlabel('Neurons in Previous Layer')\n",
    "        plt.ylabel('Neurons in Current Layer')\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.title(f'Difference in {layer_name} Weights')\n",
    "        plt.imshow(new - original, cmap='coolwarm')\n",
    "        plt.colorbar(label='Weight Difference')\n",
    "        plt.xlabel('Neurons in Previous Layer')\n",
    "        plt.ylabel('Neurons in Current Layer')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Step 1: Create the original model\n",
    "original_input_size = 2\n",
    "original_hidden_layers = [8, 8]\n",
    "original_output_size = 1\n",
    "original_model = FCN(original_input_size, original_output_size, original_hidden_layers, activation='Tanh', initialization='He')\n",
    "original_model.plot_weights()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fd8aa2-150b-451a-b4d1-331a33e8036e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b27d860-509c-4130-97d7-2c6d64eca1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot each tensor using Matplotlib\n",
    "for idx, (key, tensor) in enumerate(tensor_dict.items()):\n",
    "    row = idx // num_cols\n",
    "    col = idx % num_cols\n",
    "    axs[row, col].imshow(tensor, cmap='gray')  # Adjust colormap as needed\n",
    "    axs[row, col].set_title(key)\n",
    "    axs[row, col].axis('off')  # Turn off axis\n",
    "    axs[row, col].set_aspect('auto')  # Set aspect ratio to auto\n",
    "# Plot data in each subplot\n",
    "for i, ax in enumerate(axs.flat):\n",
    "    if i < num_subplots:\n",
    "        ax.plot(x, y)\n",
    "        ax.set_title(f\"Subplot {i+1}\")\n",
    "    else:\n",
    "        ax.axis('off')  # Hide extra subplots\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5239886-74b9-44e3-bfc5-21baa07774ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = original_model.state_dict()\n",
    "weights_biases_dict = {key: value for key, value in state_dict.items()}\n",
    "type(weights_biases_dict['fcs.0.weight'].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d48ba9-36a0-4281-9354-545452915d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure for visualization\n",
    "num_plots = len(weights_biases_dict)\n",
    "#fig, axes = plt.subplots(1, num_plots, figsize=(5*num_plots, 5))\n",
    "fig, axes = plt.subplots(nrows = (num_plots +1)//2,\n",
    "                                 ncols = 2, \n",
    "                                 figsize=(5*num_plots, 5)\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6772df81-9738-400c-b3ea-fe2fe7246695",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 2: Visualize the weights and biases of the original model\n",
    "for layer_name, param in original_model.named_parameters():\n",
    "    original_weight = param.detach().numpy()\n",
    "    original_model.visualize_difference(original_weight, original_weight, layer_name)\n",
    "\n",
    "# Step 3: Create the extended model with 16 neurons in hidden layers and ReLU activation\n",
    "extended_hidden_layers = [16, 16]\n",
    "extended_model = FCN(original_input_size, original_output_size, extended_hidden_layers, activation='ReLU', initialization='He')\n",
    "\n",
    "# Step 4: Visualize the weights and biases of the extended model\n",
    "for layer_name, param in extended_model.named_parameters():\n",
    "    extended_weight = param.detach().numpy()\n",
    "    extended_model.visualize_difference(extended_weight, extended_weight, layer_name)\n",
    "\n",
    "# Step 5: Overwrite the weights and biases of the extended model with the original model\n",
    "for original_param, extended_param in zip(original_model.parameters(), extended_model.parameters()):\n",
    "    extended_param.data.copy_(original_param.data)\n",
    "\n",
    "# Step 6: Visualize the weights and biases of the extended model after overwriting\n",
    "for layer_name, param in extended_model.named_parameters():\n",
    "    extended_weight = param.detach().numpy()\n",
    "    extended_model.visualize_difference(original_weight, extended_weight, layer_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d710f3-6544-460d-ae4b-8baa198f9685",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.fc3 = nn.Linear(hidden_size2, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def plot_weights(self):\n",
    "        state_dict = self.state_dict()\n",
    "        \n",
    "        num_plots = len(state_dict)\n",
    "        fig, axes = plt.subplots(1, num_plots, figsize=(5*num_plots, 5))\n",
    "\n",
    "        max_intensity = 0  \n",
    "        for i, (key, data) in enumerate(state_dict.items()):\n",
    "            ax = axes[i] if num_plots > 1 else axes\n",
    "            title = f'{key} (Biases)' if 'bias' in key else f'{key} (Weights)'\n",
    "            \n",
    "            if len(data.shape) == 1:\n",
    "                data = data.reshape(1, -1)\n",
    "                \n",
    "            im = ax.imshow(data.numpy(), cmap='viridis', interpolation='none')\n",
    "\n",
    "            for i in range(data.shape[0]):\n",
    "                for j in range(data.shape[1]):\n",
    "                    ax.text(j, i, f'{data[i, j]:.2f}', color='white', ha='center', va='center')\n",
    "\n",
    "            ax.set_title(title)\n",
    "            max_intensity = max(max_intensity, np.max(np.abs(data.numpy()))) \n",
    "\n",
    "        divider = make_axes_locatable(ax)\n",
    "        cax = divider.append_axes(\"bottom\", size=\"5%\", pad=0.5)\n",
    "        cbar = plt.colorbar(im, cax=cax, orientation='horizontal')\n",
    "        cbar.set_label('Value')\n",
    "        cbar.set_clim(-max_intensity, max_intensity)\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "# Define the neural network architecture\n",
    "input_size = 2\n",
    "hidden_size1 = 4\n",
    "hidden_size2 = 4\n",
    "output_size = 1\n",
    "\n",
    "# Instantiate the PyTorch model\n",
    "model = SimpleNN(input_size, hidden_size1, hidden_size2, output_size)\n",
    "\n",
    "# Call the plot_weights method\n",
    "model.plot_weights()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ac6b86-d3f3-4d2b-a8a4-28c6ce31ce00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Example dictionary with tensors as values\n",
    "tensor_dict = {\n",
    "    'tensor1': np.random.rand(10, 10),\n",
    "    'tensor2': np.random.rand(10, 10)\n",
    "}\n",
    "\n",
    "# Determine the number of subplots needed\n",
    "num_subplots = len(tensor_dict)\n",
    "\n",
    "# Create subplots\n",
    "fig, axs = plt.subplots(1, num_subplots, figsize=(10, 5))\n",
    "\n",
    "# Plot each tensor using Matplotlib\n",
    "for idx, (key, tensor) in enumerate(tensor_dict.items()):\n",
    "    axs[idx].imshow(tensor, cmap='gray')  # Adjust colormap as needed\n",
    "    axs[idx].set_title(key)\n",
    "    axs[idx].axis('off')  # Turn off axis\n",
    "    axs[idx].set_aspect('auto')  # Set aspect ratio to auto\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f808dce-bc84-4e03-82b3-74667b6c7500",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Example dictionary with tensors as values\n",
    "tensor_dict = {\n",
    "    'tensor1': np.random.rand(10, 10),\n",
    "    'tensor2': np.random.rand(10, 10),\n",
    "    'tensor3': np.random.rand(10, 10),\n",
    "    'tensor4': np.random.rand(10, 10),\n",
    "    'tensor5': np.random.rand(10, 10)\n",
    "}\n",
    "\n",
    "# Determine the number of subplots needed\n",
    "num_subplots = len(tensor_dict)\n",
    "\n",
    "# Create subplots with 3 rows and 2 columns\n",
    "num_rows = 3\n",
    "num_cols = 2\n",
    "fig, axs = plt.subplots(num_rows, num_cols, figsize=(10, 10))\n",
    "\n",
    "# Plot each tensor using Matplotlib\n",
    "for idx, (key, tensor) in enumerate(tensor_dict.items()):\n",
    "    row = idx // num_cols\n",
    "    col = idx % num_cols\n",
    "    axs[row, col].imshow(tensor, cmap='gray')  # Adjust colormap as needed\n",
    "    axs[row, col].set_title(key)\n",
    "    axs[row, col].axis('off')  # Turn off axis\n",
    "    axs[row, col].set_aspect('auto')  # Set aspect ratio to auto\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8d6cf6-01b8-42f3-b59e-bb3765a547df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Example dictionary with tensors as values\n",
    "tensor_dict = {\n",
    "    'tensor1': np.random.rand(10, 10),\n",
    "    'tensor2': np.random.rand(10, 10),\n",
    "    'tensor3': np.random.rand(10, 10),\n",
    "    'tensor4': np.random.rand(10, 10),\n",
    "    'tensor5': np.random.rand(10, 10)\n",
    "}\n",
    "\n",
    "# Determine the number of subplots needed\n",
    "num_subplots = len(tensor_dict)\n",
    "\n",
    "# Create subplots with 3 rows and 2 columns\n",
    "num_rows = 3\n",
    "num_cols = 2\n",
    "fig, axs = plt.subplots(num_rows, num_cols, figsize=(10, 10))\n",
    "type(axs)\n",
    "# Plot each tensor using Matplotlib\n",
    "for idx, (key, tensor) in enumerate(tensor_dict.items()):\n",
    "    if idx < num_rows * num_cols:\n",
    "        row = idx // num_cols\n",
    "        col = idx % num_cols\n",
    "        axs[row, col].imshow(tensor, cmap='gray')  # Adjust colormap as needed\n",
    "        axs[row, col].set_title(key)\n",
    "        axs[row, col].axis('off')  # Turn off axis\n",
    "        axs[row, col].set_aspect('auto')  # Set aspect ratio to auto\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n",
    "axs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8680a61-cab7-479b-a4b9-1bd8cf58f5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "axs[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e65a30b-767e-48fd-9f2e-b1bf32009626",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Example data\n",
    "x = np.linspace(0, 2*np.pi, 100)\n",
    "y = np.sin(x)\n",
    "\n",
    "# Create odd number of subplots\n",
    "num_subplots = 5\n",
    "num_cols = 2\n",
    "num_rows = (num_subplots + num_cols - 1) // num_cols  # Calculate number of rows needed\n",
    "\n",
    "# Create subplots\n",
    "fig, axs = plt.subplots(num_rows, num_cols, figsize=(10, 8))\n",
    "\n",
    "# Plot data in each subplot\n",
    "for i, ax in enumerate(axs.flat):\n",
    "    if i < num_subplots:\n",
    "        ax.plot(x, y)\n",
    "        ax.set_title(f\"Subplot {i+1}\")\n",
    "    else:\n",
    "        ax.axis('off')  # Hide extra subplots\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893e7c03-eb99-4d14-8977-15a00c4068b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Example data\n",
    "data = np.random.rand(28, 28, 5)  # Example image data, assuming grayscale images of size 28x28\n",
    "\n",
    "# Create odd number of subplots\n",
    "num_subplots = 5\n",
    "num_cols = 3\n",
    "num_rows = (num_subplots + num_cols - 1) // num_cols  # Calculate number of rows needed\n",
    "\n",
    "# Create subplots\n",
    "fig, axs = plt.subplots(num_rows, num_cols, figsize=(10, 8))\n",
    "\n",
    "# Plot images in each subplot\n",
    "for i, ax in enumerate(axs.flat):\n",
    "    if i < num_subplots:\n",
    "        ax.imshow(data[:, :, i], cmap='gray')  # Assuming grayscale images\n",
    "        ax.set_title(f\"Subplot {i+1}\")\n",
    "        ax.axis('off')\n",
    "    else:\n",
    "        ax.axis('off')  # Hide extra subplots\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400d03c2-fb59-4107-a25f-b86e03c77d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Example dictionary with tensors as values\n",
    "tensor_dict = {\n",
    "    'tensor1': np.random.rand(28, 28),\n",
    "    'tensor2': np.random.rand(28, 28),\n",
    "    'tensor3': np.random.rand(28, 28),\n",
    "    'tensor4': np.random.rand(28, 28),\n",
    "    'tensor5': np.random.rand(28, 28)\n",
    "}\n",
    "\n",
    "# Create odd number of subplots\n",
    "num_subplots = len(tensor_dict)\n",
    "num_cols = 2\n",
    "num_rows = (num_subplots + num_cols - 1) // num_cols  # Calculate number of rows needed\n",
    "\n",
    "# Create subplots\n",
    "fig, axs = plt.subplots(num_rows, num_cols, figsize=(10, 8))\n",
    "\n",
    "# Plot images in each subplot\n",
    "for i, (key, tensor) in enumerate(tensor_dict.items()):\n",
    "    row = i // num_cols\n",
    "    col = i % num_cols\n",
    "    axs[row, col].imshow(tensor, cmap='gray')  # Assuming grayscale tensors\n",
    "    axs[row, col].set_title(key)\n",
    "    axs[row, col].axis('off')\n",
    "\n",
    "# Hide extra subplots\n",
    "for i in range(num_subplots, num_rows * num_cols):\n",
    "    axs.flatten()[i].axis('off')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e3cc7a-a031-4787-80e1-e65ddf322412",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Example dictionary with tensors as values\n",
    "tensor_dict = {\n",
    "    'tensor1': np.random.rand(2, 1),\n",
    "    'tensor2': np.random.rand(1, 4),\n",
    "    'tensor3': np.random.rand(4, 4),\n",
    "    'tensor4': np.random.rand(1, 4),\n",
    "    'tensor5': np.random.rand(1, 1)\n",
    "}\n",
    "\n",
    "# Create odd number of subplots\n",
    "num_subplots = len(tensor_dict)\n",
    "num_cols = 2\n",
    "num_rows = (num_subplots + num_cols - 1) // num_cols  # Calculate number of rows needed\n",
    "\n",
    "# Create subplots\n",
    "fig, axs = plt.subplots(num_rows, num_cols, figsize=(15, 15))\n",
    "\n",
    "# Plot images and add values in each subplot\n",
    "for i, (key, tensor) in enumerate(tensor_dict.items()):\n",
    "    row = i // num_cols\n",
    "    col = i % num_cols\n",
    "    ax = axs[row, col]\n",
    "    print(\"*\"*50)\n",
    "    print(i)\n",
    "    print(key)\n",
    "    print(tensor)\n",
    "    print(\"*\"*10)\n",
    "    print(f\"tensor shape: {tensor.shape}\")\n",
    "    print(tensor.ndim)\n",
    "    print(\"Now tensor\")\n",
    "    ax.imshow(tensor, cmap='viridis', interpolation='none')  # Assuming grayscale tensors\n",
    "    print(\"printed\")\n",
    "    print(\"*\"*50)\n",
    "    ax.set_title(key)\n",
    "    ax.axis('off')\n",
    "\n",
    "    # Add values in the middle of the cell\n",
    "    for y in range(tensor.shape[0]):\n",
    "        for x in range(tensor.shape[1]):\n",
    "            value = tensor[y, x]\n",
    "            ax.text(x, y, f'{value:.2f}', fontsize = 8, color='red', ha='center', va='center')\n",
    "\n",
    "# Hide extra subplots\n",
    "for i in range(num_subplots, num_rows * num_cols):\n",
    "    axs.flatten()[i].axis('off')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n",
    "type(tensor_dict)\n",
    "print(tensor_dict[\"tensor1\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3747a575-9b1b-46f9-97f0-4a6eace57ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tensor_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8629d476-85b0-4ff5-866d-6b25a01567cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(tensor_dict[\"tensor1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a7ee9d-35f5-42a7-9316-88ddf9703c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.fc3 = nn.Linear(hidden_size2, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def plot_weights(self):\n",
    "        weights_biases_dict = self.state_dict()\n",
    "        #weights_biases_dict = {key: value for key, value in state_dict.items()}\n",
    "        \n",
    "        # Definition of columns, rows and subplots\n",
    "        num_subplots = len(weights_biases_dict)\n",
    "        num_cols = 2\n",
    "        num_rows = (num_subplots + num_cols - 1) // num_cols  \n",
    "        \n",
    "        # For colorbar (collect minimum and maximum values across all tensors)\n",
    "        all_values = np.concatenate([tensor.flatten() for tensor in weights_biases_dict.values()])\n",
    "        min_val = round(all_values.min(), 1)\n",
    "        max_val = round(all_values.max(), 1)\n",
    "        \n",
    "        # Create subplots\n",
    "        fig, axs = plt.subplots(num_rows, num_cols, figsize=(10, 10))\n",
    "        \n",
    "        # Plot images and add values in each subplot\n",
    "        for i, (key, tensor) in enumerate(weights_biases_dict.items()):\n",
    "            row = i // num_cols\n",
    "            col = i % num_cols\n",
    "            ax = axs[row, col]\n",
    "            if 'weight' in key:\n",
    "                im = ax.imshow(tensor, cmap='viridis',vmin=min_val, vmax=max_val, interpolation='none')\n",
    "                ax.set_title(f'{key}')\n",
    "            elif 'bias' in key:\n",
    "                im = ax.imshow(tensor.unsqueeze(0), cmap='viridis', vmin=min_val, vmax=max_val, interpolation='none')\n",
    "                ax.set_title(f'{key}')\n",
    "            #ax.axis('off')\n",
    "            \n",
    "            # If the data is 1D (possibly biases), reshape them to (1, len(data))\n",
    "            if len(tensor.shape) == 1:\n",
    "                tensor = tensor.reshape(1, -1)\n",
    "\n",
    "            # Add xticks and yticks\n",
    "            ax.set_xticks(np.arange(0, tensor.shape[1] , step=1))  \n",
    "            ax.set_yticks(np.arange(0, tensor.shape[0] , step=1))  \n",
    "           \n",
    "            # Add values in the middle of the cell\n",
    "            for y in range(tensor.shape[0]):\n",
    "                for x in range(tensor.shape[1]):\n",
    "                    value = tensor[y, x]\n",
    "                    ax.text(x, y, f'{value:.2f}', fontsize = 8, color='white', ha='center', va='center')\n",
    "            \n",
    "         # Hide extra subplots\n",
    "        for i in range(num_subplots, num_rows * num_cols):\n",
    "            axs.flatten()[i].axis('off')\n",
    "            \n",
    "        # Add a title to the figure\n",
    "        plt.suptitle('Weights and Biases of the Neural Network')\n",
    "        \n",
    "        # Add a colorbar\n",
    "            # Set custom ticks and intervals\n",
    "        tick_interval = 0.2\n",
    "        ticks = np.arange(min_val, max_val, step=tick_interval)\n",
    "\n",
    "        cbar_ax = fig.add_axes([0.25, 0.05, 0.5, 0.01])  # [left, bottom, width, height]\n",
    "        cbar = fig.colorbar(im, cax=cbar_ax, orientation='horizontal', ticks = ticks)\n",
    "        #cbar.set_label('Colorbar Label') Label for the Colorbar\n",
    "        \n",
    "        # Show the plots\n",
    "        plt.show()\n",
    "\n",
    "# Define the neural network architecture\n",
    "input_size = 2\n",
    "hidden_size1 = 4\n",
    "hidden_size2 = 4\n",
    "output_size = 1\n",
    "\n",
    "# Instantiate the PyTorch model\n",
    "model = SimpleNN(input_size, hidden_size1, hidden_size2, output_size)\n",
    "# Call the plot_weights method\n",
    "model.plot_weights()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f69ab5-1321-46bf-91df-13b2506d460c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.fc3 = nn.Linear(hidden_size2, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def plot_weights(self):\n",
    "        state_dict = self.state_dict()\n",
    "        weights_biases_dict = {key: value for key, value in state_dict.items()}\n",
    "        \n",
    "        # Create a figure for visualization\n",
    "        num_plots = len(weights_biases_dict)\n",
    "        fig, axes = plt.subplots(1, num_plots, figsize=(5*num_plots, 5))\n",
    "\n",
    "        # Plot the weights and biases for each layer\n",
    "        #max_intensity = 0  # Track the maximum intensity across all plots\n",
    "        for i, (key, data) in enumerate(weights_biases_dict.items()):\n",
    "            ax = axes[i] if num_plots > 1 else axes\n",
    "            if 'bias' in key:\n",
    "                title = f'{key} (Biases)'\n",
    "            elif 'weight' in key:\n",
    "                title = f'{key} (Weights)'\n",
    "                \n",
    "            if len(data.shape) == 1:\n",
    "                # If the data is 1D (possibly biases), reshape them to (1, len(data))\n",
    "                data = data.reshape(1, -1)\n",
    "                \n",
    "            im = ax.imshow(data.numpy(), cmap='viridis', interpolation='none')\n",
    "\n",
    "            # Add numbers to the cells\n",
    "            for i in range(data.shape[0]):\n",
    "                for j in range(data.shape[1]):\n",
    "                    ax.text(j, i, f'{data[i, j]:.2f}', color='white', ha='center', va='center')\n",
    "\n",
    "            ax.set_title(title)\n",
    "            #max_intensity = max(max_intensity, np.max(np.abs(data.numpy())))  # Update max intensity\n",
    "\n",
    "        # Add a colorbar below all the plots\n",
    "        #divider = make_axes_locatable(ax)\n",
    "        #cax = divider.append_axes(\"bottom\", size=\"5%\", pad=0.5)\n",
    "        #cbar = plt.colorbar(im, cax=cax, orientation='horizontal')\n",
    "        #cbar.set_label('Value')\n",
    "\n",
    "        # Normalize the color bar based on the maximum intensity across all plots\n",
    "        #cbar.set_clim(-max_intensity, max_intensity)\n",
    "\n",
    "        # Show the plots\n",
    "        plt.show()\n",
    "\n",
    "# Define the neural network architecture\n",
    "input_size = 2\n",
    "hidden_size1 = 4\n",
    "hidden_size2 = 4\n",
    "output_size = 1\n",
    "\n",
    "# Instantiate the PyTorch model\n",
    "model = SimpleNN(input_size, hidden_size1, hidden_size2, output_size)\n",
    "\n",
    "# Call the plot_weights method\n",
    "model.plot_weights()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd479b9-cd2b-4335-9160-c3783430e53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.fc3 = nn.Linear(hidden_size2, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def plot_weights(self):\n",
    "        state_dict = self.state_dict()\n",
    "        weights_biases_dict = {key: value for key, value in state_dict.items()}\n",
    "        \n",
    "        # Create a figure for visualization\n",
    "        num_plots = len(weights_biases_dict)\n",
    "        fig, axes = plt.subplots(1, num_plots, figsize=(5*num_plots, 5))\n",
    "\n",
    "        # Plot the weights and biases for each layer\n",
    "        max_intensity = 0  # Track the maximum intensity across all plots\n",
    "        for i, (key, data) in enumerate(weights_biases_dict.items()):\n",
    "            ax = axes[i] if num_plots > 1 else axes\n",
    "            if 'bias' in key:\n",
    "                title = f'{key} (Biases)'\n",
    "            elif 'weight' in key:\n",
    "                title = f'{key} (Weights)'\n",
    "                \n",
    "            if len(data.shape) == 1:\n",
    "                # If the data is 1D (possibly biases), reshape them to (1, len(data))\n",
    "                data = data.reshape(1, -1)\n",
    "                \n",
    "            im = ax.imshow(data.numpy(), cmap='viridis', interpolation='none')\n",
    "\n",
    "            # Add numbers to the cells\n",
    "            for i in range(data.shape[0]):\n",
    "                for j in range(data.shape[1]):\n",
    "                    ax.text(j, i, f'{data[i, j]:.2f}', color='white', ha='center', va='center')\n",
    "\n",
    "            ax.set_title(title)\n",
    "            max_intensity = max(max_intensity, np.max(np.abs(data.numpy())))  # Update max intensity\n",
    "\n",
    "        # Add a colorbar below all the plots\n",
    "        divider = make_axes_locatable(ax)\n",
    "        cax = divider.append_axes(\"bottom\", size=\"5%\", pad=0.5)\n",
    "        cbar = plt.colorbar(im, cax=cax, orientation='horizontal')\n",
    "        cbar.set_label('Value')\n",
    "\n",
    "        # Normalize the color bar based on the maximum intensity across all plots\n",
    "        #cbar.set_clim(-max_intensity, max_intensity)\n",
    "\n",
    "        # Show the plots\n",
    "        plt.show()\n",
    "\n",
    "# Define the neural network architecture\n",
    "input_size = 2\n",
    "hidden_size1 = 4\n",
    "hidden_size2 = 4\n",
    "output_size = 1\n",
    "\n",
    "# Instantiate the PyTorch model\n",
    "model = SimpleNN(input_size, hidden_size1, hidden_size2, output_size)\n",
    "\n",
    "# Call the plot_weights method\n",
    "model.plot_weights()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c597434c-a599-4385-b04e-b8b0fb823d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def visualize_difference(self, original, new, layer_name):\n",
    "        plt.figure(figsize=(10, 4))\n",
    "\n",
    "        # Plot weights\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.title(f'{layer_name} Weights')\n",
    "        plt.imshow(original, cmap='viridis')\n",
    "        plt.colorbar(label='Weight Values')\n",
    "        plt.xlabel('Neurons in Previous Layer')\n",
    "        plt.ylabel('Neurons in Current Layer')\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.title(f'Difference in {layer_name} Weights')\n",
    "        plt.imshow(new - original, cmap='coolwarm')\n",
    "        plt.colorbar(label='Weight Difference')\n",
    "        plt.xlabel('Neurons in Previous Layer')\n",
    "        plt.ylabel('Neurons in Current Layer')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1572688d-29c9-4dec-9758-50c715c9fec2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a10a8f-101c-467b-a329-3236684a38e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to visualize the weights and biases\n",
    "def visualize_difference(original, new, layer_name):\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    \n",
    "    # Plot weights\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title(f'{layer_name} Weights')\n",
    "    plt.imshow(original, cmap='viridis')\n",
    "    plt.colorbar(label='Weight Values')\n",
    "    plt.xlabel('Neurons in Previous Layer')\n",
    "    plt.ylabel('Neurons in Current Layer')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.title(f'Difference in {layer_name} Weights')\n",
    "    plt.imshow(new - original, cmap='coolwarm')\n",
    "    plt.colorbar(label='Weight Difference')\n",
    "    plt.xlabel('Neurons in Previous Layer')\n",
    "    plt.ylabel('Neurons in Current Layer')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Function to create a model with specified initialization\n",
    "def create_model(input_size, hidden_layers, output_size, initialization='he'):\n",
    "    model = nn.Sequential()\n",
    "    \n",
    "    # Add input layer\n",
    "    model.add_module('input_layer', nn.Linear(input_size, hidden_layers[0]))\n",
    "    if initialization == 'he':\n",
    "        init.kaiming_uniform_(model.input_layer.weight.data, mode='fan_in', nonlinearity='relu')\n",
    "        init.zeros_(model.input_layer.bias.data)\n",
    "\n",
    "    # Add hidden layers\n",
    "    for i in range(len(hidden_layers) - 1):\n",
    "        layer = nn.Linear(hidden_layers[i], hidden_layers[i + 1])\n",
    "        model.add_module(f'hidden_layer{i + 1}', layer)\n",
    "        if initialization == 'he':\n",
    "            init.kaiming_uniform_(layer.weight.data, mode='fan_in', nonlinearity='relu')\n",
    "            init.zeros_(layer.bias.data)\n",
    "\n",
    "    # Add output layer\n",
    "    model.add_module('output_layer', nn.Linear(hidden_layers[-1], output_size))\n",
    "    if initialization == 'he':\n",
    "        init.kaiming_uniform_(model.output_layer.weight.data, mode='fan_in', nonlinearity='linear')\n",
    "        init.zeros_(model.output_layer.bias.data)\n",
    "\n",
    "    return model\n",
    "\n",
    "# Step 1: Create the original model\n",
    "original_input_size = 2\n",
    "original_hidden_layers = [8, 8]\n",
    "original_output_size = 1\n",
    "original_model = create_model(original_input_size, original_hidden_layers, original_output_size, initialization='he')\n",
    "\n",
    "# Step 2: Visualize the weights and biases of the original model\n",
    "for layer_name, original_weight in original_model.named_parameters():\n",
    "    visualize_difference(original_weight.detach().numpy(), original_weight.detach().numpy(), layer_name)\n",
    "\n",
    "# Step 3: Create the extended model with 16 neurons in hidden layers\n",
    "extended_input_size = 2\n",
    "extended_hidden_layers = [16, 16]\n",
    "extended_output_size = 1\n",
    "extended_model = create_model(extended_input_size, extended_hidden_layers, extended_output_size, initialization='he')\n",
    "\n",
    "# Step 4: Visualize the weights and biases of the extended model\n",
    "for layer_name, extended_weight in extended_model.named_parameters():\n",
    "    visualize_difference(extended_weight.detach().numpy(), extended_weight.detach().numpy(), layer_name)\n",
    "\n",
    "# Step 5: Overwrite the weights and biases of the extended model with the original model\n",
    "for original_param, extended_param in zip(original_model.parameters(), extended_model.parameters()):\n",
    "    extended_param.data.copy_(original_param.data)\n",
    "\n",
    "# Step 6: Visualize the weights and biases of the extended model after overwriting\n",
    "for layer_name, extended_weight in extended_model.named_parameters():\n",
    "    visualize_difference(original_weight.detach().numpy(), extended_weight.detach().numpy(), layer_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410b3a93-0bbd-4f43-a9d2-b560f9cab46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define a simple fully connected neural network\n",
    "class MyFCNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyFCNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(2, 4)  # Example: input size=10, output size=5\n",
    "        self.fc2 = nn.Linear(4, 1)   # Example: input size=5, output size=2\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Create an instance of the network\n",
    "model = MyFCNet()\n",
    "\n",
    "# Get the state_dict of the model\n",
    "state_dict = model.state_dict()\n",
    "\n",
    "# Extract weights and biases from the state_dict\n",
    "for key, value in state_dict.items():\n",
    "    if 'weight' in key:\n",
    "        print(f\"Layer: {key}, Shape: {value.shape}\")\n",
    "        print(\"Weights:\")\n",
    "        print(value)\n",
    "    elif 'bias' in key:\n",
    "        print(f\"Layer: {key}, Shape: {value.shape}\")\n",
    "        print(\"Biases:\")\n",
    "        print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee472dbf-d387-426f-8942-802ecdae1ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Example tensor with shape (4,)\n",
    "tensor = torch.tensor([1, 2, 3, 4])\n",
    "\n",
    "# Transform the tensor to shape (4, 1)\n",
    "tensor_reshaped = tensor.unsqueeze(0)\n",
    "\n",
    "print(\"Original tensor shape:\", tensor.shape)\n",
    "print(\"Transformed tensor shape:\", tensor_reshaped.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd5880b-7921-4ad0-9cf9-ef7b703b96e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define a simple model using nn.Sequential\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(10, 5),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(5, 1)\n",
    ")\n",
    "\n",
    "# Print the model architecture\n",
    "print(model)\n",
    "\n",
    "# Initialize some input data\n",
    "input_data = torch.randn(3, 10)\n",
    "\n",
    "# Forward pass through the model\n",
    "output = model(input_data)\n",
    "\n",
    "# Print the output\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff3d373-98fd-45e5-b52f-036ce297e068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the model parameters\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef18881-6a28-4ad2-967f-407273319db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7252d31-6e6d-4280-b553-f8ffe5029832",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_layer = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_sizes[0]),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.hidden_layers = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(hidden_sizes[i], hidden_size),\n",
    "                nn.ReLU()\n",
    "            ) for i, hidden_size in enumerate(hidden_sizes[1:])\n",
    "        ])\n",
    "\n",
    "        self.output_layer = nn.Linear(hidden_sizes[-1], output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_layer(x)\n",
    "        for layer in self.hidden_layers:\n",
    "            x = layer(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "# Example usage\n",
    "input_size = 2\n",
    "hidden_sizes = [4, 8, 8, 4]\n",
    "output_size = 1\n",
    "\n",
    "model = SimpleModel(input_size, hidden_sizes, output_size)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da54a9ec-0513-4c65-a53c-73678ada7e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e932e9bb-3f85-4ee7-b67b-97b7035e0e3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d2d73c-3b29-4cb5-a6d2-4839ef00060f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Example weight matrix (2x3) and bias vector (2,)\n",
    "W = torch.randn(2, 3)\n",
    "x = torch.randn(3)\n",
    "b = torch.randn(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea815ca-818c-4400-88b9-06f40a0fb0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5c51ec-fe88-4440-a80e-6481fda30610",
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbbd9c8-b29c-4516-819d-2a6ea91f3f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff30b63-ecd2-4d50-b67d-894bf4552da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform matrix multiplication\n",
    "result = torch.matmul(W, x)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3871d93a-aadf-4e56-800c-bc19857a2319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add bias vector element-wise to each row of the result\n",
    "result_with_bias = result + b\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b012fff-da02-4693-be63-9f492aa77dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Result with bias:\", result_with_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4594b1e5-52f7-410a-8ef9-80495b5c2433",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50db9d4e-0776-4a88-8923-056cdd4226ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8247e4-9b73-4686-8014-3e763b485550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the architecture of the original network\n",
    "original_input_size = 2\n",
    "original_hidden_layers = [8, 8]\n",
    "original_output_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8380b3-6539-4a36-bdb6-c6de593e4233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the original model with Xavier initialization\n",
    "class OriginalModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layers, output_size):\n",
    "        super(OriginalModel, self).__init__()\n",
    "        \n",
    "        # Input layer\n",
    "        self.input_layer = nn.Linear(input_size, hidden_layers[0])\n",
    "        init.xavier_uniform_(self.input_layer.weight.data, gain=nn.init.calculate_gain('relu'))\n",
    "        init.zeros_(self.input_layer.bias.data)\n",
    "        \n",
    "        # Hidden layers with Xavier initialization\n",
    "        self.hidden_layers = nn.ModuleList([\n",
    "            nn.Linear(hidden_layers[i], hidden_layers[i+1]) for i in range(len(hidden_layers)-1)\n",
    "        ])\n",
    "        for layer in self.hidden_layers:\n",
    "            init.xavier_uniform_(layer.weight.data, gain=nn.init.calculate_gain('relu'))\n",
    "            init.zeros_(layer.bias.data)\n",
    "        \n",
    "        # Output layer with Xavier initialization\n",
    "        self.output_layer = nn.Linear(hidden_layers[-1], output_size)\n",
    "        init.xavier_uniform_(self.output_layer.weight.data, gain=nn.init.calculate_gain('linear'))\n",
    "        init.zeros_(self.output_layer.bias.data)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.input_layer(x))\n",
    "        for layer in self.hidden_layers:\n",
    "            x = torch.relu(layer(x))\n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f4ade0-d2a1-4ea8-96cf-cf26a48c74bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create an instance of the original model\n",
    "original_model = OriginalModel(original_input_size, original_hidden_layers, original_output_size)\n",
    "\n",
    "# Define the architecture of the new network with increased neurons\n",
    "new_input_size = 2\n",
    "new_hidden_layers = [16, 16]  # Increase the number of neurons\n",
    "new_output_size = 1\n",
    "\n",
    "# Create a new model with the same architecture as the original one\n",
    "class NewModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layers, output_size):\n",
    "        super(NewModel, self).__init__()\n",
    "        \n",
    "        # Input layer\n",
    "        self.input_layer = nn.Linear(input_size, hidden_layers[0])\n",
    "        \n",
    "        # Hidden layers with weights and biases from the original model\n",
    "        self.hidden_layers = nn.ModuleList([\n",
    "            nn.Linear(hidden_layers[i], hidden_layers[i+1]) for i in range(len(hidden_layers)-1)\n",
    "        ])\n",
    "        \n",
    "        # Output layer\n",
    "        self.output_layer = nn.Linear(hidden_layers[-1], output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.input_layer(x))\n",
    "        for layer in self.hidden_layers:\n",
    "            x = torch.relu(layer(x))\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "# Create an instance of the new model\n",
    "new_model = NewModel(new_input_size, new_hidden_layers, new_output_size)\n",
    "\n",
    "# Set the weights from the original model using He initialization\n",
    "def initialize_weights_he(model):\n",
    "    for layer in model.modules():\n",
    "        if isinstance(layer, nn.Linear):\n",
    "            init.kaiming_uniform_(layer.weight.data, mode='fan_in', nonlinearity='relu')\n",
    "            init.zeros_(layer.bias.data)\n",
    "\n",
    "# Initialize weights of the new model using He initialization\n",
    "initialize_weights_he(new_model)\n",
    "\n",
    "# Visualize the differences in weights and biases\n",
    "def visualize_difference(original, new, layer_name):\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    \n",
    "    # Plot weights\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title(f'{layer_name} Weights')\n",
    "    plt.imshow(original, cmap='viridis')\n",
    "    plt.colorbar(label='Weight Values')\n",
    "    plt.xlabel('Neurons in Previous Layer')\n",
    "    plt.ylabel('Neurons in Current Layer')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.title(f'Difference in {layer_name} Weights')\n",
    "    plt.imshow(new - original, cmap='coolwarm')\n",
    "    plt.colorbar(label='Weight Difference')\n",
    "    plt.xlabel('Neurons in Previous Layer')\n",
    "    plt.ylabel('Neurons in Current Layer')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize differences for each layer\n",
    "for layer_name, original_weight in original_model.state_dict().items():\n",
    "    new_weight = new_model.state_dict()[layer_name]\n",
    "    visualize_difference(original_weight, new_weight, layer_name)\n",
    "\n",
    "# Visualize differences for biases\n",
    "for layer_name, original_bias in original_model.state_dict().items():\n",
    "    if 'bias' in layer_name:\n",
    "        new_bias = new_model.state_dict()[layer_name]\n",
    "        visualize_difference(original_bias.unsqueeze(0), new_bias.unsqueeze(0), layer_name + ' Bias')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afdd9b6-a055-431c-8591-16a4c9a946a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def find_max_min_values(state_dict):\n",
    "    max_value = float('-inf')\n",
    "    min_value = float('inf')\n",
    "\n",
    "    for key, tensor in state_dict.items():\n",
    "        if isinstance(tensor, torch.Tensor):\n",
    "            if tensor.dim() == 0:\n",
    "                tensor_max = tensor.item()\n",
    "                tensor_min = tensor.item()\n",
    "            else:\n",
    "                tensor_max = torch.max(tensor).item()\n",
    "                tensor_min = torch.min(tensor).item()\n",
    "            max_value = max(max_value, tensor_max)\n",
    "            min_value = min(min_value, tensor_min)\n",
    "\n",
    "    return max_value, min_value\n",
    "\n",
    "# Example usage\n",
    "model = YourModel()  # Instantiate your model\n",
    "state_dict = model.state_dict()\n",
    "\n",
    "max_value, min_value = find_max_min_values(state_dict)\n",
    "print(\"Max value:\", max_value)\n",
    "print(\"Min value:\", min_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b316ffe9-fd28-403d-ad7b-dbecfbf5f61e",
   "metadata": {},
   "source": [
    "# AS II Old versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe1003e-bf5a-4777-9cf5-d3d4b1e35741",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "class FCN(nn.Module):\n",
    "    def __init__(self, N_INPUT = 2, hidden_layers = [4], N_OUTPUT = 1,  activation='Tanh', initialization='Xavier'):\n",
    "        super().__init__()\n",
    "\n",
    "        self.activation_functions = nn.ModuleDict([\n",
    "            [\"Tanh\", nn.Tanh()],\n",
    "            [\"ReLU\", nn.ReLU()],\n",
    "            [\"LeakyReLU\", nn.LeakyReLU()],\n",
    "            [\"Sigmoid\", nn.Sigmoid()],\n",
    "            [\"Softmax\", nn.Softmax(dim=-1)],\n",
    "        ])\n",
    "\n",
    "        if activation not in self.activation_functions:\n",
    "            raise ValueError(\"Unsupported activation function\")\n",
    "\n",
    "        self.activation = self.activation_functions[activation]\n",
    "\n",
    "        #self.fci = nn.Sequential(\n",
    "        #    nn.Linear(N_INPUT, hidden_layers[0]),\n",
    "        #    self.activation\n",
    "        #)\n",
    "        self.fci = nn.Linear(N_INPUT, hidden_layers[0])\n",
    "\n",
    "        self.fch = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(hidden_layers[i], hidden_layers[i + 1]),\n",
    "                self.activation\n",
    "            ) for i in range(len(hidden_layers) - 1)\n",
    "        ])\n",
    "\n",
    "        self.fco = nn.Linear(hidden_layers[-1], N_OUTPUT)\n",
    "\n",
    "        self.initialize_parameters(initialization)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.fci(x))\n",
    "        for layer in self.fch:\n",
    "            x = self.activation(layer(x))\n",
    "        x = self.fco(x)\n",
    "        return x\n",
    "    \n",
    "    def initialize_parameters(self, initialization):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                if initialization == 'Uniform':\n",
    "                    init.uniform_(module.weight.data, -0.1, 0.1)\n",
    "                    init.zeros_(module.bias.data)\n",
    "                elif initialization == 'Normal':\n",
    "                    init.normal_(module.weight.data, mean=0, std=0.01)\n",
    "                    init.zeros_(module.bias.data)\n",
    "                elif initialization == 'Xavier':\n",
    "                    init.xavier_uniform_(module.weight.data)\n",
    "                    init.zeros_(module.bias.data)\n",
    "                elif initialization == 'He':\n",
    "                    init.kaiming_uniform_(module.weight.data, mode='fan_in', nonlinearity='relu')\n",
    "                    init.zeros_(module.bias.data)\n",
    "                elif initialization == 'Orthogonal':\n",
    "                    init.orthogonal_(module.weight.data)\n",
    "                    init.zeros_(module.bias.data)\n",
    "                elif initialization == 'Kaiming':\n",
    "                    init.kaiming_uniform_(module.weight.data, mode='fan_in', nonlinearity='leaky_relu')\n",
    "                    init.zeros_(module.bias.data)\n",
    "                else:\n",
    "                    raise ValueError(\"Unsupported initialization type\")\n",
    "\n",
    "    def plot_weights(self, figsize = (10,5)):\n",
    "        self.figsize = figsize\n",
    "        weights_biases_dict = self.state_dict()\n",
    "        #weights_biases_dict = {key: value for key, value in state_dict.items()}\n",
    "        \n",
    "        # Definition of columns, rows and subplots\n",
    "        num_subplots = len(weights_biases_dict)\n",
    "        num_cols = 2\n",
    "        num_rows = (num_subplots + num_cols - 1) // num_cols  \n",
    "        \n",
    "        # For colorbar (collect minimum and maximum values across all tensors)\n",
    "        all_values = np.concatenate([tensor.flatten() for tensor in weights_biases_dict.values()])\n",
    "        min_val = round(all_values.min(), 2)\n",
    "        max_val = round(all_values.max(), 2)\n",
    "        \n",
    "        # Create subplots\n",
    "        fig, axs = plt.subplots(num_rows, num_cols, figsize = self.figsize)\n",
    "        \n",
    "        # Plot images and add values in each subplot\n",
    "        for i, (key, tensor) in enumerate(weights_biases_dict.items()):\n",
    "            row = i // num_cols\n",
    "            col = i % num_cols\n",
    "            ax = axs[row, col]\n",
    "            if 'weight' in key:\n",
    "                im = ax.imshow(tensor, cmap='viridis',vmin=min_val, vmax=max_val, interpolation='none')\n",
    "                ax.set_title(f'{key}', fontsize = 8)\n",
    "            elif 'bias' in key:\n",
    "                im = ax.imshow(tensor.unsqueeze(0), cmap='viridis', vmin=min_val, vmax=max_val, interpolation='none')\n",
    "                ax.set_title(f'{key}', fontsize = 8)\n",
    "            #ax.axis('off')\n",
    "            \n",
    "            # If the data is 1D (possibly biases), reshape them to (1, len(data))\n",
    "            if len(tensor.shape) == 1:\n",
    "                tensor = tensor.reshape(1, -1)\n",
    "\n",
    "            # Add xticks and yticks\n",
    "            ax.set_xticks(np.arange(0, tensor.shape[1] , step=1))  \n",
    "            ax.tick_params(axis='x', labelsize = 8)\n",
    "            ax.set_yticks(np.arange(0, tensor.shape[0] , step=1))\n",
    "            ax.tick_params(axis='y', labelsize = 8)\n",
    "\n",
    "           \n",
    "            # Add values in the middle of the cell\n",
    "            for y in range(tensor.shape[0]):\n",
    "                for x in range(tensor.shape[1]):\n",
    "                    value = tensor[y, x]\n",
    "                    ax.text(x, y, f'{value:.2f}', fontsize = 8, color='white', ha='center', va='center')\n",
    "            \n",
    "         # Hide extra subplots\n",
    "        for i in range(num_subplots, num_rows * num_cols):\n",
    "            axs.flatten()[i].axis('off')\n",
    "            \n",
    "        # Add a title to the figure\n",
    "        plt.suptitle('Weights and Biases of the Neural Network')\n",
    "        \n",
    "        # Add a colorbar\n",
    "            # Set custom ticks and intervals\n",
    "        tick_interval = 0.2\n",
    "        ticks = np.arange(min_val, max_val, step=tick_interval)\n",
    "\n",
    "        cbar_ax = fig.add_axes([0.25, 0.05, 0.5, 0.01])  # [left, bottom, width, height]\n",
    "        cbar = fig.colorbar(im, cax=cbar_ax, orientation='horizontal', ticks = ticks)\n",
    "        cbar.set_label('Range of Weights and Biases') \n",
    "        \n",
    "        # Show the plots\n",
    "        plt.show()\n",
    "\n",
    "# Step 1: Create the original model\n",
    "original_input_size = 2\n",
    "original_hidden_layers = [4,4]\n",
    "original_output_size = 1\n",
    "original_model = FCN(original_input_size, original_hidden_layers, original_output_size, activation='Tanh', initialization='Xavier')\n",
    "original_model.plot_weights()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f043097-7e86-4887-a7c6-8ed63f1e8d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_model.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0326f56d-362a-4349-af0c-c5c5ae09312a",
   "metadata": {},
   "source": [
    "## ASII: extended methods added"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbe6533-4807-430b-b43b-0a9db3982b10",
   "metadata": {},
   "source": [
    "### BACKUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636809c5-91a5-4df4-9d8b-93cb3ff1c581",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "class FCN(nn.Module):\n",
    "    def __init__(self, N_INPUT = 2, hidden_layers = [4], N_OUTPUT = 1, activation='Tanh', initialization='Xavier'):\n",
    "        super().__init__()\n",
    "\n",
    "        self.activation_functions = nn.ModuleDict([\n",
    "            [\"Tanh\", nn.Tanh()],\n",
    "            [\"ReLU\", nn.ReLU()],\n",
    "            [\"LeakyReLU\", nn.LeakyReLU()],\n",
    "            [\"Sigmoid\", nn.Sigmoid()],\n",
    "            [\"Softmax\", nn.Softmax(dim=-1)],\n",
    "        ])\n",
    "\n",
    "        if activation not in self.activation_functions:\n",
    "            raise ValueError(\"Unsupported activation function\")\n",
    "\n",
    "        self.activation = self.activation_functions[activation]\n",
    "\n",
    "        self.fci = nn.Linear(N_INPUT, hidden_layers[0])\n",
    "\n",
    "        self.fch = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(hidden_layers[i], hidden_size),\n",
    "                self.activation\n",
    "            ) for i, hidden_size in enumerate(hidden_layers[1:])\n",
    "        ])\n",
    "\n",
    "        self.fco = nn.Linear(hidden_layers[-1], N_OUTPUT)\n",
    "\n",
    "        self.initialize_parameters(initialization)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.fci(x))\n",
    "        for layer in self.fch:\n",
    "            x = self.activation(layer(x))\n",
    "        x = self.fco(x)\n",
    "        return x\n",
    "        \n",
    "    def initialize_parameters(self, initialization):\n",
    "        \n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                if initialization == 'Uniform':\n",
    "                    init.uniform_(module.weight.data, -0.1, 0.1)\n",
    "                    init.zeros_(module.bias.data)\n",
    "                elif initialization == 'Normal':\n",
    "                    init.normal_(module.weight.data, mean=0, std=0.01)\n",
    "                    init.zeros_(module.bias.data)\n",
    "                elif initialization == 'Xavier':\n",
    "                    init.xavier_uniform_(module.weight.data)\n",
    "                    init.zeros_(module.bias.data)\n",
    "                elif initialization == 'He':\n",
    "                    init.kaiming_uniform_(module.weight.data, mode='fan_in', nonlinearity='relu')\n",
    "                    init.zeros_(module.bias.data)\n",
    "                elif initialization == 'Orthogonal':\n",
    "                    init.orthogonal_(module.weight.data)\n",
    "                    init.zeros_(module.bias.data)\n",
    "                elif initialization == 'Kaiming':\n",
    "                    init.kaiming_uniform_(module.weight.data, mode='fan_in', nonlinearity='leaky_relu')\n",
    "                    init.zeros_(module.bias.data)\n",
    "                else:\n",
    "                    raise ValueError(\"Unsupported initialization type\")\n",
    "\n",
    "    def plot_weights(self, figsize = (10,5)):\n",
    "        \n",
    "        self.figsize = figsize\n",
    "        \n",
    "        weights_biases_dict = self.state_dict()\n",
    "        #weights_biases_dict = {key: value for key, value in state_dict.items()}\n",
    "        \n",
    "        # Definition of columns, rows and subplots\n",
    "        num_subplots = len(weights_biases_dict)\n",
    "        num_cols = 2\n",
    "        num_rows = (num_subplots + num_cols - 1) // num_cols  \n",
    "        \n",
    "        # For colorbar (collect minimum and maximum values across all tensors)\n",
    "        all_values = np.concatenate([tensor.flatten() for tensor in weights_biases_dict.values()])\n",
    "        min_val = round(all_values.min(), 2)\n",
    "        max_val = round(all_values.max(), 2)\n",
    "        \n",
    "        # Create subplots\n",
    "        fig, axs = plt.subplots(num_rows, num_cols, figsize = self.figsize)\n",
    "        \n",
    "        # Plot images and add values in each subplot\n",
    "        for i, (key, tensor) in enumerate(weights_biases_dict.items()):\n",
    "            row = i // num_cols\n",
    "            col = i % num_cols\n",
    "            ax = axs[row, col]\n",
    "            if 'weight' in key:\n",
    "                im = ax.imshow(tensor, cmap='viridis',vmin=min_val, vmax=max_val, interpolation='none')\n",
    "                ax.set_title(f'{key}', fontsize = 8)\n",
    "            elif 'bias' in key:\n",
    "                im = ax.imshow(tensor.unsqueeze(0), cmap='viridis', vmin=min_val, vmax=max_val, interpolation='none')\n",
    "                ax.set_title(f'{key}', fontsize = 8)\n",
    "            #ax.axis('off')\n",
    "            \n",
    "            # If the data is 1D (possibly biases), reshape them to (1, len(data))\n",
    "            if len(tensor.shape) == 1:\n",
    "                tensor = tensor.reshape(1, -1)\n",
    "\n",
    "            # Add xticks and yticks\n",
    "            ax.set_xticks(np.arange(0, tensor.shape[1] , step=1))  \n",
    "            ax.tick_params(axis='x', labelsize = 8)\n",
    "            ax.set_yticks(np.arange(0, tensor.shape[0] , step=1))\n",
    "            ax.tick_params(axis='y', labelsize = 8)\n",
    "\n",
    "           \n",
    "            # Add values in the middle of the cell\n",
    "            for y in range(tensor.shape[0]):\n",
    "                for x in range(tensor.shape[1]):\n",
    "                    value = tensor[y, x]\n",
    "                    ax.text(x, y, f'{value:.2f}', fontsize = 8, color='white', ha='center', va='center')\n",
    "            \n",
    "         # Hide extra subplots\n",
    "        for i in range(num_subplots, num_rows * num_cols):\n",
    "            axs.flatten()[i].axis('off')\n",
    "            \n",
    "        # Add a title to the figure\n",
    "        plt.suptitle('Weights and Biases of the Neural Network')\n",
    "        \n",
    "        # Add a colorbar\n",
    "            # Set custom ticks and intervals\n",
    "        tick_interval = 0.2\n",
    "        ticks = np.arange(min_val, max_val, step=tick_interval)\n",
    "\n",
    "        cbar_ax = fig.add_axes([0.25, 0.05, 0.5, 0.01])  # [left, bottom, width, height]\n",
    "        cbar = fig.colorbar(im, cax=cbar_ax, orientation='horizontal', ticks = ticks)\n",
    "        cbar.set_label('Range of Weights and Biases') \n",
    "        \n",
    "        # Show the plots\n",
    "        plt.show()\n",
    "\n",
    "class FCN_extended(FCN):\n",
    "    def __init__(self, N_INPUT, hidden_layers, N_OUTPUT, activation='Tanh', initialization='Xavier', original_model_path=None):\n",
    "        super().__init__(N_INPUT, hidden_layers, N_OUTPUT, activation, initialization)\n",
    "    \n",
    "        self.original_model_path = original_model_path\n",
    "        \n",
    "        self.load_override_original_weights_biases()\n",
    "\n",
    "    def load_override_original_weights_biases(self):\n",
    "        if self.original_model_path is None:\n",
    "            raise ValueError(\"Path to the original model checkpoint is not provided.\")\n",
    "\n",
    "        if not os.path.exists(self.original_model_path):\n",
    "            raise FileNotFoundError(f\"Provided path '{self.original_model_path}' does not exist.\")\n",
    "\n",
    "        if not os.path.isfile(self.original_model_path):\n",
    "            raise ValueError(f\"Provided path '{self.original_model_path}' is not a file.\")\n",
    "\n",
    "        _, ext = os.path.splitext(self.original_model_path)\n",
    "        if ext not in ['.pt', '.pth']:\n",
    "            raise ValueError(\"Provided file is not a valid checkpoint file.\")\n",
    "        \n",
    "        # Load weights and biases from original_model\n",
    "        original_state_dict = torch.load(self.original_model_path)['model_state_dict']\n",
    "        \n",
    "        # Update extended model state dictionary with original model's parameters\n",
    "        self.load_state_dict(original_state_dict, strict=False)\n",
    "        \n",
    "        # Override weights and biases from original model\n",
    "        for name, param in self.named_parameters():\n",
    "            #print(f\"name: {name}, param: {param}\")\n",
    "            if name in original_state_dict:\n",
    "                param.data.copy_(original_state_dict[name].data)\n",
    "\n",
    "    def extend_hidden_layers(self, num_hidden_layers):\n",
    "        current_hidden_layers = len(self.fch)\n",
    "        if current_hidden_layers > num_hidden_layers:\n",
    "            raise ValueError(\"Cannot reduce the number of hidden layers.\")\n",
    "\n",
    "        for _ in range(num_hidden_layers - current_hidden_layers):\n",
    "            new_layer = nn.Sequential(\n",
    "                nn.Linear(self.hidden_layers[-1], self.hidden_layers[-1]),\n",
    "                self.activation)\n",
    "\n",
    "            self.fch.append(new_layer)\n",
    "\n",
    "    def extend_neurons(self, num_neurons):\n",
    "        if len(self.fch) == 0:\n",
    "            raise ValueError(\"Cannot extend neurons with no hidden layers.\")\n",
    "\n",
    "        for layer in self.fch:\n",
    "            current_neurons = layer[0].in_features\n",
    "            if current_neurons < num_neurons:\n",
    "                new_weights = torch.cat([layer[0].weight.data, torch.randn(num_neurons - current_neurons, current_neurons)], dim=0)\n",
    "                new_biases = torch.cat([layer[0].bias.data, torch.zeros(num_neurons - current_neurons)], dim=0)\n",
    "                layer[0].weight.data = new_weights\n",
    "                layer[0].bias.data = new_biases\n",
    "            elif current_neurons > num_neurons:\n",
    "                layer[0].weight.data = layer[0].weight.data[:num_neurons, :]\n",
    "                layer[0].bias.data = layer[0].bias.data[:num_neurons]\n",
    "\n",
    "class TrainModel:\n",
    "    def __init__(self, model, num_epochs, save_interval, loss_threshold=None):\n",
    "        self.model = model\n",
    "        self.num_epochs = num_epochs\n",
    "        self.save_interval = save_interval\n",
    "        self.loss_threshold = loss_threshold\n",
    "\n",
    "    def train(self):\n",
    "        optimizer = optim.Adam(self.model.parameters())\n",
    "        criterion = nn.MSELoss()\n",
    "\n",
    "        for epoch in range(1, self.num_epochs + 1):\n",
    "            # Train the model\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            outputs = self.model(torch.randn(10, original_input_size))\n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, torch.randn(10, original_output_size))\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Save model checkpoint\n",
    "            if epoch % self.save_interval == 0:\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': self.model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': loss\n",
    "                }, f\"original_model_{epoch}.pt\")\n",
    "\n",
    "            # Check loss threshold\n",
    "            if self.loss_threshold is not None and loss.item() < self.loss_threshold:\n",
    "                print(f\"Loss threshold reached at epoch {epoch}. Stopping training.\")\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94c7c03-1e5f-4596-a754-ff86eac07be0",
   "metadata": {},
   "source": [
    "### Updating the FCN_extended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8113b5f-0306-4ef7-b059-e8f34d192cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BACKUP:\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "class FCN(nn.Module):\n",
    "    def __init__(self, N_INPUT = 2, hidden_layers = [4], N_OUTPUT = 1, activation='Tanh', initialization='Xavier'):\n",
    "        super().__init__()\n",
    "\n",
    "        self.activation_functions = nn.ModuleDict([\n",
    "            [\"Tanh\", nn.Tanh()],\n",
    "            [\"ReLU\", nn.ReLU()],\n",
    "            [\"LeakyReLU\", nn.LeakyReLU()],\n",
    "            [\"Sigmoid\", nn.Sigmoid()],\n",
    "            [\"Softmax\", nn.Softmax(dim=-1)],\n",
    "        ])\n",
    "\n",
    "        if activation not in self.activation_functions:\n",
    "            raise ValueError(\"Unsupported activation function\")\n",
    "\n",
    "        self.activation = self.activation_functions[activation]\n",
    "\n",
    "        # self.fci = nn.Linear(N_INPUT, hidden_layers[0])\n",
    "\n",
    "        # self.fch = nn.ModuleList([\n",
    "        #     nn.Sequential(\n",
    "        #         nn.Linear(hidden_layers[i], hidden_size),\n",
    "        #         self.activation\n",
    "        #     ) for i, hidden_size in enumerate(hidden_layers[1:])    # ) for i, hidden_size in enumerate(hidden_layers[1:])\n",
    "        # ])\n",
    "        self.fch = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(N_INPUT if i == 0 else hidden_sizes[i-1], hidden_size),\n",
    "                self.activation\n",
    "            ) for i, hidden_size in enumerate(hidden_layers[:])    # ) for i, hidden_size in enumerate(hidden_layers[1:])\n",
    "        ])\n",
    "\n",
    "        self.fco = nn.Linear(hidden_layers[-1], N_OUTPUT)\n",
    "\n",
    "        self.initialize_parameters(initialization)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = self.activation(self.fci(x))\n",
    "        for layer in self.fch:\n",
    "            x = self.activation(layer(x))\n",
    "        x = self.fco(x)\n",
    "        return x\n",
    "\n",
    "    def initialize_parameters(self, initialization):\n",
    "        \n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                if initialization == 'Uniform':\n",
    "                    init.uniform_(module.weight.data, -0.1, 0.1)\n",
    "                    init.zeros_(module.bias.data)\n",
    "                elif initialization == 'Normal':\n",
    "                    init.normal_(module.weight.data, mean=0, std=0.01)\n",
    "                    init.zeros_(module.bias.data)\n",
    "                elif initialization == 'Xavier':\n",
    "                    init.xavier_uniform_(module.weight.data)\n",
    "                    init.zeros_(module.bias.data)\n",
    "                elif initialization == 'He':\n",
    "                    init.kaiming_uniform_(module.weight.data, mode='fan_in', nonlinearity='relu')\n",
    "                    init.zeros_(module.bias.data)\n",
    "                elif initialization == 'Orthogonal':\n",
    "                    init.orthogonal_(module.weight.data)\n",
    "                    init.zeros_(module.bias.data)\n",
    "                elif initialization == 'Kaiming':\n",
    "                    init.kaiming_uniform_(module.weight.data, mode='fan_in', nonlinearity='leaky_relu')\n",
    "                    init.zeros_(module.bias.data)\n",
    "                else:\n",
    "                    raise ValueError(\"Unsupported initialization type\")\n",
    "    \n",
    "    def plot_weights(self, figsize = (10,5)):\n",
    "        \n",
    "        self.figsize = figsize\n",
    "        \n",
    "        weights_biases_dict = self.state_dict()\n",
    "        #weights_biases_dict = {key: value for key, value in state_dict.items()}\n",
    "        \n",
    "        # Definition of columns, rows and subplots\n",
    "        num_subplots = len(weights_biases_dict)\n",
    "        num_cols = 2\n",
    "        num_rows = (num_subplots + num_cols - 1) // num_cols  \n",
    "        \n",
    "        # For colorbar (collect minimum and maximum values across all tensors)\n",
    "        all_values = np.concatenate([tensor.flatten() for tensor in weights_biases_dict.values()])\n",
    "        min_val = round(all_values.min(), 2)\n",
    "        max_val = round(all_values.max(), 2)\n",
    "        \n",
    "        # Create subplots\n",
    "        fig, axs = plt.subplots(num_rows, num_cols, figsize = self.figsize)\n",
    "        \n",
    "        # Plot images and add values in each subplot\n",
    "        for i, (key, tensor) in enumerate(weights_biases_dict.items()):\n",
    "            row = i // num_cols\n",
    "            col = i % num_cols\n",
    "            ax = axs[row, col]\n",
    "            if 'weight' in key:\n",
    "                im = ax.imshow(tensor, cmap='viridis',vmin=min_val, vmax=max_val, interpolation='none')\n",
    "                ax.set_title(f'{key}', fontsize = 8)\n",
    "            elif 'bias' in key:\n",
    "                im = ax.imshow(tensor.unsqueeze(0), cmap='viridis', vmin=min_val, vmax=max_val, interpolation='none')\n",
    "                ax.set_title(f'{key}', fontsize = 8)\n",
    "            #ax.axis('off')\n",
    "            \n",
    "            # If the data is 1D (possibly biases), reshape them to (1, len(data))\n",
    "            if len(tensor.shape) == 1:\n",
    "                tensor = tensor.reshape(1, -1)\n",
    "\n",
    "            # Add xticks and yticks\n",
    "            ax.set_xticks(np.arange(0, tensor.shape[1] , step=1))  \n",
    "            ax.tick_params(axis='x', labelsize = 8)\n",
    "            ax.set_yticks(np.arange(0, tensor.shape[0] , step=1))\n",
    "            ax.tick_params(axis='y', labelsize = 8)\n",
    "\n",
    "           \n",
    "            # Add values in the middle of the cell\n",
    "            for y in range(tensor.shape[0]):\n",
    "                for x in range(tensor.shape[1]):\n",
    "                    value = tensor[y, x]\n",
    "                    ax.text(x, y, f'{value:.2f}', fontsize = 8, color='white', ha='center', va='center')\n",
    "            \n",
    "         # Hide extra subplots\n",
    "        for i in range(num_subplots, num_rows * num_cols):\n",
    "            axs.flatten()[i].axis('off')\n",
    "            \n",
    "        # Add a title to the figure\n",
    "        plt.suptitle('Weights and Biases of the Neural Network')\n",
    "        \n",
    "        # Add a colorbar\n",
    "            # Set custom ticks and intervals\n",
    "        tick_interval = 0.2\n",
    "        ticks = np.arange(min_val, max_val, step=tick_interval)\n",
    "\n",
    "        cbar_ax = fig.add_axes([0.25, 0.05, 0.5, 0.01])  # [left, bottom, width, height]\n",
    "        cbar = fig.colorbar(im, cax=cbar_ax, orientation='horizontal', ticks = ticks)\n",
    "        cbar.set_label('Range of Weights and Biases') \n",
    "        \n",
    "        # Show the plots\n",
    "        plt.show()\n",
    "\n",
    "class FCN_extended(FCN):\n",
    "    def __init__(self, N_INPUT, hidden_layers, N_OUTPUT, activation='Tanh', initialization='Xavier', original_model_path=None):\n",
    "        super().__init__(N_INPUT, hidden_layers, N_OUTPUT, activation, initialization)\n",
    "    \n",
    "        self.original_model_path = original_model_path\n",
    "        self.original_neurons =  self.fch[0]0].in_features # assuming that all hidden layers have the same number of neurons \n",
    "        print(f\"Original_neurons in __init__: {self.original_neurons}\")      \n",
    "        self.extended_neurons = hidden_layers[-1] # assuming that all hidden layers have the same number of neurons \n",
    "        print(f\"extended_neurons: {self.extended_neurons}\")\n",
    "        # Check if hidden layers need to be extended\n",
    "        if len(hidden_layers) < len(self.fch): \n",
    "            raise ValueError(\"The number of hidden layers of the extended model must be larger as the original model\")\n",
    "        elif len(hidden_layers) > len(self.fch):\n",
    "            #self.extend_hidden_layers(len(hidden_layers))\n",
    "            self.load_override_original_weights_biases()\n",
    "        else:\n",
    "            self.extend_neurons() \n",
    "            \n",
    "    def load_override_original_weights_biases(self):\n",
    "        if self.original_model_path is None:\n",
    "            raise ValueError(\"Path to the original model checkpoint is not provided.\")\n",
    "\n",
    "        if not os.path.exists(self.original_model_path):\n",
    "            raise FileNotFoundError(f\"Provided path '{self.original_model_path}' does not exist.\")\n",
    "\n",
    "        if not os.path.isfile(self.original_model_path):\n",
    "            raise ValueError(f\"Provided path '{self.original_model_path}' is not a file.\")\n",
    "\n",
    "        _, ext = os.path.splitext(self.original_model_path)\n",
    "        if ext not in ['.pt', '.pth']:\n",
    "            raise ValueError(\"Provided file is not a valid checkpoint file.\")\n",
    "        \n",
    "        # Load weights and biases from original_model\n",
    "        original_state_dict = torch.load(self.original_model_path)['model_state_dict']\n",
    "        \n",
    "        # Update extended model state dictionary with original model's parameters\n",
    "        self.load_state_dict(original_state_dict, strict=False)\n",
    "        \n",
    "        # Override weights and biases from original model\n",
    "        for name, param in self.named_parameters():\n",
    "            if name in original_state_dict:\n",
    "                param.data.copy_(original_state_dict[name].data)\n",
    "\n",
    "    def extend_hidden_layers(self, num_hidden_layers):\n",
    "        current_hidden_layers = len(self.fch)\n",
    "        if current_hidden_layers > num_hidden_layers:\n",
    "            raise ValueError(\"Cannot reduce the number of hidden layers.\")\n",
    "\n",
    "        for _ in range(num_hidden_layers - current_hidden_layers):\n",
    "            new_layer = nn.Sequential(\n",
    "                nn.Linear(self.hidden_layers[-1], self.hidden_layers[-1]),\n",
    "                self.activation)\n",
    "\n",
    "            self.fch.append(new_layer)\n",
    "\n",
    "    def extend_neurons(self):\n",
    "        if len(self.fch) == 0:\n",
    "            raise ValueError(\"Cannot extend neurons with no hidden layers.\")\n",
    "\n",
    "        for layer in self.fch:\n",
    "            original_neurons = layer[0].weight.size(0)\n",
    "            print(f\"Original_neurons: {original_neurons}\")\n",
    "            print(f\"Original neurons in layer {layer[0]} extend_neurons: {original_neurons}\")\n",
    "            # if original_neurons < self.extended_neurons:\n",
    "            #     # Extend weights using the provided initialization\n",
    "            #     init_fn = getattr(init, self.initialization)      # getattr( use to access dynamically the attributes or methods of an object based on a string name )\n",
    "            #     new_weights = init_fn(layer[0].weight.new_empty(self.extended_neurons - original_neurons, original_neurons))\n",
    "            #     new_biases = init.constant_(layer[0].bias.new_empty(self.extended_neurons - original_neurons), 0)\n",
    "            #     layer[0].weight.data = torch.cat([layer[0].weight.data, new_weights], dim=0)\n",
    "            #     layer[0].bias.data = torch.cat([layer[0].bias.data, new_biases], dim=0)\n",
    "            # else:\n",
    "            #     raise ValueError(\"Maintaining or reducing the number of neurons is not allowed\")\n",
    "class TrainModel:\n",
    "    def __init__(self, model, num_epochs, save_interval, loss_threshold=None):\n",
    "        self.model = model\n",
    "        self.num_epochs = num_epochs\n",
    "        self.save_interval = save_interval\n",
    "        self.loss_threshold = loss_threshold\n",
    "\n",
    "    def train(self):\n",
    "        optimizer = optim.Adam(self.model.parameters())\n",
    "        criterion = nn.MSELoss()\n",
    "\n",
    "        for epoch in range(1, self.num_epochs + 1):\n",
    "            # Train the model\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            outputs = self.model(torch.randn(10, original_input_size))\n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, torch.randn(10, original_output_size))\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Save model checkpoint\n",
    "            if epoch % self.save_interval == 0:\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': self.model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': loss\n",
    "                }, f\"original_model_{epoch}.pt\")\n",
    "\n",
    "            # Check loss threshold\n",
    "            if self.loss_threshold is not None and loss.item() < self.loss_threshold:\n",
    "                print(f\"Loss threshold reached at epoch {epoch}. Stopping training.\")\n",
    "                break\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc03e1f4-6aaf-4a37-aacf-f35d923a968e",
   "metadata": {},
   "source": [
    "## New version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467b3653-531a-41ca-9cbf-f25779eaf9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "class FCN(nn.Module):\n",
    "    def __init__(self, N_INPUT = 2, hidden_layers = [4], N_OUTPUT = 1, activation='Tanh', initialization='Xavier'):\n",
    "        super().__init__()\n",
    "\n",
    "        self.activation_functions = nn.ModuleDict([\n",
    "            [\"Tanh\", nn.Tanh()],\n",
    "            [\"ReLU\", nn.ReLU()],\n",
    "            [\"LeakyReLU\", nn.LeakyReLU()],\n",
    "            [\"Sigmoid\", nn.Sigmoid()],\n",
    "            [\"Softmax\", nn.Softmax(dim=-1)],\n",
    "        ])\n",
    "\n",
    "        if activation not in self.activation_functions:\n",
    "            raise ValueError(\"Unsupported activation function\")\n",
    "\n",
    "        self.activation = self.activation_functions[activation]\n",
    "\n",
    "        # self.fci = nn.Linear(N_INPUT, hidden_layers[0])\n",
    "\n",
    "        # self.fch = nn.ModuleList([\n",
    "        #     nn.Sequential(\n",
    "        #         nn.Linear(hidden_layers[i], hidden_size),\n",
    "        #         self.activation\n",
    "        #     ) for i, hidden_size in enumerate(hidden_layers[1:])    # ) for i, hidden_size in enumerate(hidden_layers[1:])\n",
    "        # ])\n",
    "        self.fch = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(N_INPUT if i == 0 else hidden_size, hidden_size),\n",
    "                self.activation\n",
    "            ) for i, hidden_size in enumerate(hidden_layers[:])    # ) for i, hidden_size in enumerate(hidden_layers[1:])\n",
    "        ])\n",
    "\n",
    "        self.fco = nn.Linear(hidden_layers[-1], N_OUTPUT)\n",
    "\n",
    "        self.initialize_parameters(initialization)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = self.activation(self.fci(x))\n",
    "        for layer in self.fch:\n",
    "            x = self.activation(layer(x))\n",
    "        x = self.fco(x)\n",
    "        return x\n",
    "\n",
    "    def initialize_parameters(self, initialization):\n",
    "        \n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                if initialization == 'Uniform':\n",
    "                    init.uniform_(module.weight.data, -0.1, 0.1)\n",
    "                    init.zeros_(module.bias.data)\n",
    "                elif initialization == 'Normal':\n",
    "                    init.normal_(module.weight.data, mean=0, std=0.01)\n",
    "                    init.zeros_(module.bias.data)\n",
    "                elif initialization == 'Xavier':\n",
    "                    init.xavier_uniform_(module.weight.data)\n",
    "                    init.zeros_(module.bias.data)\n",
    "                elif initialization == 'He':\n",
    "                    init.kaiming_uniform_(module.weight.data, mode='fan_in', nonlinearity='relu')\n",
    "                    init.zeros_(module.bias.data)\n",
    "                elif initialization == 'Orthogonal':\n",
    "                    init.orthogonal_(module.weight.data)\n",
    "                    init.zeros_(module.bias.data)\n",
    "                elif initialization == 'Kaiming':\n",
    "                    init.kaiming_uniform_(module.weight.data, mode='fan_in', nonlinearity='leaky_relu')\n",
    "                    init.zeros_(module.bias.data)\n",
    "                else:\n",
    "                    raise ValueError(\"Unsupported initialization type\")\n",
    "    \n",
    "    def plot_weights(self, figsize = (10,5)):\n",
    "        \n",
    "        self.figsize = figsize\n",
    "        \n",
    "        weights_biases_dict = self.state_dict()\n",
    "        #weights_biases_dict = {key: value for key, value in state_dict.items()}\n",
    "        \n",
    "        # Definition of columns, rows and subplots\n",
    "        num_subplots = len(weights_biases_dict)\n",
    "        num_cols = 2\n",
    "        num_rows = (num_subplots + num_cols - 1) // num_cols  \n",
    "        \n",
    "        # For colorbar (collect minimum and maximum values across all tensors)\n",
    "        all_values = np.concatenate([tensor.flatten() for tensor in weights_biases_dict.values()])\n",
    "        min_val = round(all_values.min(), 2)\n",
    "        max_val = round(all_values.max(), 2)\n",
    "        \n",
    "        # Create subplots\n",
    "        fig, axs = plt.subplots(num_rows, num_cols, figsize = self.figsize)\n",
    "        \n",
    "        # Plot images and add values in each subplot\n",
    "        for i, (key, tensor) in enumerate(weights_biases_dict.items()):\n",
    "            row = i // num_cols\n",
    "            col = i % num_cols\n",
    "            ax = axs[row, col]\n",
    "            if 'weight' in key:\n",
    "                im = ax.imshow(tensor, cmap='viridis',vmin=min_val, vmax=max_val, interpolation='none')\n",
    "                ax.set_title(f'{key}', fontsize = 8)\n",
    "            elif 'bias' in key:\n",
    "                im = ax.imshow(tensor.unsqueeze(0), cmap='viridis', vmin=min_val, vmax=max_val, interpolation='none')\n",
    "                ax.set_title(f'{key}', fontsize = 8)\n",
    "            #ax.axis('off')\n",
    "            \n",
    "            # If the data is 1D (possibly biases), reshape them to (1, len(data))\n",
    "            if len(tensor.shape) == 1:\n",
    "                tensor = tensor.reshape(1, -1)\n",
    "\n",
    "            # Add xticks and yticks\n",
    "            ax.set_xticks(np.arange(0, tensor.shape[1] , step=1))  \n",
    "            ax.tick_params(axis='x', labelsize = 8)\n",
    "            ax.set_yticks(np.arange(0, tensor.shape[0] , step=1))\n",
    "            ax.tick_params(axis='y', labelsize = 8)\n",
    "\n",
    "           \n",
    "            # Add values in the middle of the cell\n",
    "            for y in range(tensor.shape[0]):\n",
    "                for x in range(tensor.shape[1]):\n",
    "                    value = tensor[y, x]\n",
    "                    ax.text(x, y, f'{value:.2f}', fontsize = 8, color='white', ha='center', va='center')\n",
    "            \n",
    "         # Hide extra subplots\n",
    "        for i in range(num_subplots, num_rows * num_cols):\n",
    "            axs.flatten()[i].axis('off')\n",
    "            \n",
    "        # Add a title to the figure\n",
    "        plt.suptitle('Weights and Biases of the Neural Network')\n",
    "        \n",
    "        # Add a colorbar\n",
    "            # Set custom ticks and intervals\n",
    "        tick_interval = 0.2\n",
    "        ticks = np.arange(min_val, max_val, step=tick_interval)\n",
    "\n",
    "        cbar_ax = fig.add_axes([0.25, 0.05, 0.5, 0.01])  # [left, bottom, width, height]\n",
    "        cbar = fig.colorbar(im, cax=cbar_ax, orientation='horizontal', ticks = ticks)\n",
    "        cbar.set_label('Range of Weights and Biases') \n",
    "        \n",
    "        # Show the plots\n",
    "        plt.show()\n",
    "\n",
    "class FCN_extended(FCN):\n",
    "    def __init__(self, N_INPUT, hidden_layers, N_OUTPUT, activation='Tanh', initialization='Xavier', original_model_path=None):\n",
    "        super().__init__(N_INPUT, hidden_layers, N_OUTPUT, activation, initialization)\n",
    "    \n",
    "        self.original_model_path = original_model_path\n",
    "        self.original_state_dict = self.load_original_state_dict()\n",
    "        self.original_state_dict_keys = list(self.original_state_dict.keys()) \n",
    "        self.original_hidden_layers_neurons = self.original_state_dict[self.original_state_dict_keys[-4]].size(0)\n",
    "        self.original_hidden_layers_keys = [key for key in self.original_state_dict_keys if \"fch\" in key and \"weight\" in key]\n",
    "        self.original_hidden_layers_num = len(set([key.split('.')[-1] for key in self.original_hidden_layers_keys]))\n",
    "\n",
    "        self.extended_neurons = hidden_layers[-1] # assuming that all hidden layers have the same number of neurons\n",
    "        #print(self.original_state_dict_keys)\n",
    "        #print(self.original_hidden_layers_keys)\n",
    "        #print(len(hidden_layers))\n",
    "        #print(self.original_hidden_layers_num)\n",
    "        # len(set([key.split('.')[1] for key in hidden_layers_keys]))\n",
    "        \n",
    "        # Check if hidden layers need to be extended\n",
    "        if len(hidden_layers) < self.original_hidden_layers_num: \n",
    "            raise ValueError(f\"The number of hidden layers of the extended model({len(hidden_layers)}) must be larger as from the original model({self.original_hidden_layers_num})\")\n",
    "        elif len(hidden_layers) > self.original_hidden_layers_num:\n",
    "            self.extend_hidden_layers()\n",
    "        else:\n",
    "            self.extend_neurons() \n",
    "            \n",
    "    def load_original_state_dict(self):\n",
    "        if self.original_model_path is None:\n",
    "            raise ValueError(\"Path to the original model checkpoint is not provided.\")\n",
    "\n",
    "        if not os.path.exists(self.original_model_path):\n",
    "            raise FileNotFoundError(f\"Provided path '{self.original_model_path}' does not exist.\")\n",
    "\n",
    "        if not os.path.isfile(self.original_model_path):\n",
    "            raise ValueError(f\"Provided path '{self.original_model_path}' is not a file.\")\n",
    "\n",
    "        _, ext = os.path.splitext(self.original_model_path)\n",
    "        if ext not in ['.pt', '.pth']:\n",
    "            raise ValueError(\"Provided file is not a valid checkpoint file.\")\n",
    "        \n",
    "        # Load weights and biases from original_model\n",
    "        return torch.load(self.original_model_path)['model_state_dict']\n",
    "\n",
    "    def extend_hidden_layers(self):        \n",
    "        # Update extended model state dictionary with original model's parameters\n",
    "        self.load_state_dict(self.original_state_dict, strict=False)\n",
    "        \n",
    "        # Transfer weights and biases from original model to extended model\n",
    "        for name, param in self.named_parameters():\n",
    "            if name in self.original_state_dict:\n",
    "                param.data.copy_(self.original_state_dict[name].data)\n",
    "\n",
    "    def extend_hidden_layers_old(self, num_hidden_layers):\n",
    "        current_hidden_layers = len(self.fch)\n",
    "        if current_hidden_layers > num_hidden_layers:\n",
    "            raise ValueError(\"Cannot reduce the number of hidden layers.\")\n",
    "\n",
    "        for _ in range(num_hidden_layers - current_hidden_layers):\n",
    "            new_layer = nn.Sequential(\n",
    "                nn.Linear(self.hidden_layers[-1], self.hidden_layers[-1]),\n",
    "                self.activation)\n",
    "\n",
    "            self.fch.append(new_layer)\n",
    "\n",
    "    def extend_neurons(self):\n",
    "        if len(self.fch) == 0:\n",
    "            raise ValueError(\"Cannot extend neurons with no hidden layers.\")\n",
    "\n",
    "        for ((name, param)) in zip(self.named_parameters(), self.original_state_dict.items()):\n",
    "            print(f\"Current model - {name}: shape {param.shape}\")\n",
    "            print(f\"Old model - {name}: shape {param.shape}\")\n",
    "        # for  ((),())layer_extended, layer_original in zip(self.fch, :\n",
    "        #     #original_neurons = layer[0].weight.size(0)\n",
    "        #     #print(f\"Original_neurons: {original_neurons}\")\n",
    "        #     #print(f\"Original neurons in layer {layer[0]} extend_neurons: {original_neurons}\")\n",
    "        #     if self.original_hidden_layers_neurons < self.extended_neurons:\n",
    "        #         # self.initialize_parameters(self.initialization)\n",
    "        #         # # Extend weights using the provided initialization\n",
    "        #         # init_fn = getattr(init, self.initialization)      # getattr( use to access dynamically the attributes or methods of an object based on a string name )\n",
    "        #         # new_weights = init_fn(layer[0].weight.new_empty(self.extended_neurons - self.original_neurons_hidden_layers, self.original_neurons_hidden_layers))\n",
    "        #         # new_biases = init.constant_(layer[0].bias.new_empty(self.extended_neurons - self.original_neurons_hidden_layers), 0)\n",
    "        #         # layer[0].weight.data = torch.cat([layer[0].weight.data, new_weights], dim=0)\n",
    "        #         # layer[0].bias.data = torch.cat([layer[0].bias.data, new_biases], dim=0)\n",
    "        #         # Initialize new weights for additional neurons\n",
    "        #         #print(self.fch.keys())\n",
    "        #         print(layer[0].weight)\n",
    "        #         new_weights = torch.cat([layer[0].weight, layer[0].weight.data.new_empty(self.extended_neurons - self.original_hidden_layers_neurons, self.original_hidden_layers_neurons)], dim=0)\n",
    "\n",
    "        #         #new_weights = torch.cat([layer[0].weight.data, layer[0].weight.data.new_empty(self.extended_neurons - self.original_hidden_layers_neurons, self.original_hidden_layers_neurons)], dim=0)\n",
    "        #         #new_biases = torch.cat([layer[0].bias.data, layer[0].bias.data.new_empty(self.extended_neurons - self.original_hidden_layers_neurons)], dim=0)\n",
    "        #         # Update the layer's weights and biases\n",
    "        #         #layer[0].weight.data = new_weights\n",
    "        #         #layer[0].bias.data = new_biases\n",
    "        #     else:\n",
    "        #         raise ValueError(\"Maintaining or reducing the number of neurons is not allowed\")\n",
    "class TrainModel:\n",
    "    def __init__(self, model, num_epochs, save_interval, loss_threshold=None):\n",
    "        self.model = model\n",
    "        self.num_epochs = num_epochs\n",
    "        self.save_interval = save_interval\n",
    "        self.loss_threshold = loss_threshold\n",
    "\n",
    "    def train(self):\n",
    "        optimizer = optim.Adam(self.model.parameters())\n",
    "        criterion = nn.MSELoss()\n",
    "\n",
    "        for epoch in range(1, self.num_epochs + 1):\n",
    "            # Train the model\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            outputs = self.model(torch.randn(10, original_input_size))\n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, torch.randn(10, original_output_size))\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Save model checkpoint\n",
    "            if epoch % self.save_interval == 0:\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': self.model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': loss\n",
    "                }, f\"original_model_{epoch}.pt\")\n",
    "\n",
    "            # Check loss threshold\n",
    "            if self.loss_threshold is not None and loss.item() < self.loss_threshold:\n",
    "                print(f\"Loss threshold reached at epoch {epoch}. Stopping training.\")\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099f4a1f-37a5-46d1-b12b-2233732a84be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "original = torch.rand(1,4)\n",
    "original.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da63d95c-c6ff-443c-a9b7-3c6003a7f1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "extended = torch.rand(1,8)\n",
    "extended.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435f1de9-af25-4b06-a82c-61cd2797082d",
   "metadata": {},
   "outputs": [],
   "source": [
    "original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3980874-a0ff-4079-bd3b-401c008a00b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "extended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1231f15-a408-452d-80a5-0ac3e7ef8afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "extended[0,:original.size(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a58676-45ec-4166-b315-f15e6cbfe28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "extended[0,:original.size(1)].copy_(original)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f315ff37-edf2-480c-8a30-08836a987862",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [\"a\", \"b\",\"c\"]\n",
    "test[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96087aa4-a0d0-4faf-bf44-96ed65345ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "param.data[:old_param.size(0),:].copy_(old_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5333639d-b2e5-4de7-90d3-3ce68e0242ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "class FCN(nn.Module):\n",
    "    def __init__(self, N_INPUT = 2, hidden_layers = [4], N_OUTPUT = 1, activation='Tanh', initialization='Xavier'):\n",
    "        super().__init__()\n",
    "\n",
    "        self.activation_functions = nn.ModuleDict([\n",
    "            [\"Tanh\", nn.Tanh()],\n",
    "            [\"ReLU\", nn.ReLU()],\n",
    "            [\"LeakyReLU\", nn.LeakyReLU()],\n",
    "            [\"Sigmoid\", nn.Sigmoid()],\n",
    "            [\"Softmax\", nn.Softmax(dim=-1)],\n",
    "        ])\n",
    "\n",
    "        if activation not in self.activation_functions:\n",
    "            raise ValueError(\"Unsupported activation function\")\n",
    "\n",
    "        self.activation = self.activation_functions[activation]\n",
    "\n",
    "        # self.fci = nn.Linear(N_INPUT, hidden_layers[0])\n",
    "\n",
    "        # self.fch = nn.ModuleList([\n",
    "        #     nn.Sequential(\n",
    "        #         nn.Linear(hidden_layers[i], hidden_size),\n",
    "        #         self.activation\n",
    "        #     ) for i, hidden_size in enumerate(hidden_layers[1:])    # ) for i, hidden_size in enumerate(hidden_layers[1:])\n",
    "        # ])\n",
    "        self.fch = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(N_INPUT if i == 0 else hidden_size, hidden_size),\n",
    "                self.activation\n",
    "            ) for i, hidden_size in enumerate(hidden_layers[:])    # ) for i, hidden_size in enumerate(hidden_layers[1:])\n",
    "        ])\n",
    "\n",
    "        self.fco = nn.Linear(hidden_layers[-1], N_OUTPUT)\n",
    "\n",
    "        self.initialize_parameters(initialization)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = self.activation(self.fci(x))\n",
    "        for layer in self.fch:\n",
    "            x = self.activation(layer(x))\n",
    "        x = self.fco(x)\n",
    "        return x\n",
    "\n",
    "    def initialize_parameters(self, initialization):\n",
    "        \n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                if initialization == 'Uniform':\n",
    "                    init.uniform_(module.weight.data, -0.1, 0.1)\n",
    "                    init.zeros_(module.bias.data)\n",
    "                elif initialization == 'Normal':\n",
    "                    init.normal_(module.weight.data, mean=0, std=0.01)\n",
    "                    init.zeros_(module.bias.data)\n",
    "                elif initialization == 'Xavier':\n",
    "                    init.xavier_uniform_(module.weight.data)\n",
    "                    init.zeros_(module.bias.data)\n",
    "                elif initialization == 'He':\n",
    "                    init.kaiming_uniform_(module.weight.data, mode='fan_in', nonlinearity='relu')\n",
    "                    init.zeros_(module.bias.data)\n",
    "                elif initialization == 'Orthogonal':\n",
    "                    init.orthogonal_(module.weight.data)\n",
    "                    init.zeros_(module.bias.data)\n",
    "                elif initialization == 'Kaiming':\n",
    "                    init.kaiming_uniform_(module.weight.data, mode='fan_in', nonlinearity='leaky_relu')\n",
    "                    init.zeros_(module.bias.data)\n",
    "                else:\n",
    "                    raise ValueError(\"Unsupported initialization type\")\n",
    "    \n",
    "    def plot_weights(self, figsize = (10,5)):\n",
    "        \n",
    "        self.figsize = figsize\n",
    "        \n",
    "        weights_biases_dict = self.state_dict()\n",
    "        #weights_biases_dict = {key: value for key, value in state_dict.items()}\n",
    "        \n",
    "        # Definition of columns, rows and subplots\n",
    "        num_subplots = len(weights_biases_dict)\n",
    "        num_cols = 2\n",
    "        num_rows = (num_subplots + num_cols - 1) // num_cols  \n",
    "        \n",
    "        # For colorbar (collect minimum and maximum values across all tensors)\n",
    "        all_values = np.concatenate([tensor.flatten() for tensor in weights_biases_dict.values()])\n",
    "        min_val = round(all_values.min(), 2)\n",
    "        max_val = round(all_values.max(), 2)\n",
    "        \n",
    "        # Create subplots\n",
    "        fig, axs = plt.subplots(num_rows, num_cols, figsize = self.figsize)\n",
    "        \n",
    "        # Plot images and add values in each subplot\n",
    "        for i, (key, tensor) in enumerate(weights_biases_dict.items()):\n",
    "            row = i // num_cols\n",
    "            col = i % num_cols\n",
    "            ax = axs[row, col]\n",
    "            if 'weight' in key:\n",
    "                im = ax.imshow(tensor, cmap='viridis',vmin=min_val, vmax=max_val, interpolation='none')\n",
    "                ax.set_title(f'{key}', fontsize = 8)\n",
    "            elif 'bias' in key:\n",
    "                im = ax.imshow(tensor.unsqueeze(0), cmap='viridis', vmin=min_val, vmax=max_val, interpolation='none')\n",
    "                ax.set_title(f'{key}', fontsize = 8)\n",
    "            #ax.axis('off')\n",
    "            \n",
    "            # If the data is 1D (possibly biases), reshape them to (1, len(data))\n",
    "            if len(tensor.shape) == 1:\n",
    "                tensor = tensor.reshape(1, -1)\n",
    "\n",
    "            # Add xticks and yticks\n",
    "            ax.set_xticks(np.arange(0, tensor.shape[1] , step=1))  \n",
    "            ax.tick_params(axis='x', labelsize = 8)\n",
    "            ax.set_yticks(np.arange(0, tensor.shape[0] , step=1))\n",
    "            ax.tick_params(axis='y', labelsize = 8)\n",
    "\n",
    "           \n",
    "            # Add values in the middle of the cell\n",
    "            for y in range(tensor.shape[0]):\n",
    "                for x in range(tensor.shape[1]):\n",
    "                    value = tensor[y, x]\n",
    "                    ax.text(x, y, f'{value:.2f}', fontsize = 8, color='white', ha='center', va='center')\n",
    "            \n",
    "         # Hide extra subplots\n",
    "        for i in range(num_subplots, num_rows * num_cols):\n",
    "            axs.flatten()[i].axis('off')\n",
    "            \n",
    "        # Add a title to the figure\n",
    "        plt.suptitle('Weights and Biases of the Neural Network')\n",
    "        \n",
    "        # Add a colorbar\n",
    "            # Set custom ticks and intervals\n",
    "        tick_interval = 0.2\n",
    "        ticks = np.arange(min_val, max_val, step=tick_interval)\n",
    "\n",
    "        cbar_ax = fig.add_axes([0.25, 0.05, 0.5, 0.01])  # [left, bottom, width, height]\n",
    "        cbar = fig.colorbar(im, cax=cbar_ax, orientation='horizontal', ticks = ticks)\n",
    "        cbar.set_label('Range of Weights and Biases') \n",
    "        \n",
    "        # Show the plots\n",
    "        plt.show()\n",
    "\n",
    "class FCN_extended(FCN):\n",
    "    def __init__(self, N_INPUT, hidden_layers, N_OUTPUT, activation='Tanh', initialization='Xavier', original_model_path=None):\n",
    "        super().__init__(N_INPUT, hidden_layers, N_OUTPUT, activation, initialization)\n",
    "\n",
    "        self.initialize_parameters(initialization)\n",
    "        \n",
    "        self.original_model_path = original_model_path\n",
    "        self.original_state_dict = self.load_original_state_dict()\n",
    "        self.original_state_dict_keys = list(self.original_state_dict.keys()) \n",
    "        self.original_hidden_layers_neurons = self.original_state_dict[self.original_state_dict_keys[-4]].size(0)\n",
    "        self.original_hidden_layers_keys = [key for key in self.original_state_dict_keys if \"fch\" in key and \"weight\" in key]\n",
    "        self.original_hidden_layers_num = len(set([key.split('.')[-1] for key in self.original_hidden_layers_keys]))\n",
    "\n",
    "        #self.extended_neurons = hidden_layers[-1] # assuming that all hidden layers have the same number of neurons\n",
    "\n",
    "        self.copy_and_initialize_parameters()\n",
    "        \n",
    "        # # Check if hidden layers need to be extended\n",
    "        # if len(hidden_layers) < self.original_hidden_layers_num: \n",
    "        #     raise ValueError(f\"The number of hidden layers of the extended model({len(hidden_layers)}) must be larger as from the original model({self.original_hidden_layers_num})\")\n",
    "        # elif len(hidden_layers) > self.original_hidden_layers_num:\n",
    "        #     self.extend_hidden_layers()\n",
    "        # else:\n",
    "        #     self.extend_neurons() \n",
    "\n",
    "\n",
    "    def copy_and_initialize_parameters(self):\n",
    "        #current_model.initialize_parameters(initialization)  # Initialize current model first\n",
    "        for name, param in self.named_parameters():\n",
    "            print(\"next iteration\")\n",
    "            if name in self.original_state_dict:\n",
    "                print(name)\n",
    "                old_param = self.original_state_dict[name]\n",
    "                if param.shape == old_param.shape:\n",
    "                    print(\"same shape\")\n",
    "                    param.data.copy_(old_param)                    \n",
    "                else:\n",
    "                    if \"weight\" in name: \n",
    "                        if param.shape != old_param.shape:  # Mismatch in dimension 0: increasing neurons\n",
    "                            # Copy matching portion of old weights\n",
    "                            print(f\"weight in name and different size(0)\")\n",
    "                            print(f\"Name: {name}, extended:{param.shape}, original: {old_param.shape}\")\n",
    "                            \n",
    "                            param.data[:old_param.size(0), :old_param.size(1)].copy_(old_param)\n",
    "                        elif param.size(0) != old_param.size(0):  # Mismatch in dimension 0: increasing neurons\n",
    "                            # Copy matching portion of old weights\n",
    "                            print(f\"weight in name and different size(0)\")\n",
    "                            print(f\"Name: {name}, extended:{param.shape}, original: {old_param.shape}\")\n",
    "                            \n",
    "                            param.data[:old_param.size(0), :].copy_(old_param)                           \n",
    "                        elif param.size(0) != old_param.size(0) and \"fc0\" in name:\n",
    "                            print(f\"weight in name BUT equal size(0)\")\n",
    "                            print(f\"Name: {name}, extended:{param.shape}, original: {old_param.shape}\")\n",
    "                            \n",
    "                            param.data[0,:old_param.size(1)].copy_(old_param[0]) \n",
    "               \n",
    "                    if \"bias\" in name: \n",
    "                        print(\"bias in name\")\n",
    "                        if param.size(0) != old_param.size(0):  # Mismatch in dimension 0: increasing neurons\n",
    "                            print(f\"bias in name and different size(0)\")\n",
    "                            print(f\"Name: {name}, extended:{param.shape}, original: {old_param.shape}\")\n",
    "                            # Copy matching portion of old weights\n",
    "\n",
    "                            param.data[old_param.size(0):].copy_(old_param)\n",
    "                        else:\n",
    "                            print(f\"bias in name BUT equal size(0)\")\n",
    "                            print(f\"Name: {name}, extended:{param.shape}, original: {old_param.shape}\")\n",
    "\n",
    "                            param.data.copy_(old_param)\n",
    "\n",
    "            print(\"end iteration\")\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # for name, param in self.named_parameters():\n",
    "        #     print(\"next iteration\")\n",
    "        #     if name in self.original_state_dict:\n",
    "        #         print(name)\n",
    "        #         old_param = self.original_state_dict[name]\n",
    "        #         if param.shape == old_param.shape:\n",
    "        #             print(\"same shape\")\n",
    "        #             param.data.copy_(old_param)                    \n",
    "        #         else:\n",
    "        #             print(\"different shape\")\n",
    "        #             print(name)\n",
    "        #             if \"weight\" in name: \n",
    "        #                 if param.size(0) != old_param.size(0):  # Mismatch in dimension 0: increasing neurons\n",
    "        #                     # Copy matching portion of old weights\n",
    "        #                     print(f\"weight in name and different size(0)\")\n",
    "        #                     print(f\"Name: {name}, extended:{param.shape}, original: {old_param.shape}\")\n",
    "                            \n",
    "        #                     param.data[:old_param.size(0), :].copy_(old_param)\n",
    "        #                 else:\n",
    "        #                     print(f\"weight in name BUT equal size(0)\")\n",
    "        #                     print(f\"Name: {name}, extended:{param.shape}, original: {old_param.shape}\")\n",
    "                            \n",
    "        #                     param.data[0,:old_param.size(1)].copy_(old_param[0])\n",
    "        #             if \"bias\" in name: \n",
    "        #                 print(\"bias in name\")\n",
    "        #                 if param.size(0) != old_param.size(0):  # Mismatch in dimension 0: increasing neurons\n",
    "        #                     print(f\"bias in name and different size(0)\")\n",
    "        #                     print(f\"Name: {name}, extended:{param.shape}, original: {old_param.shape}\")\n",
    "        #                     # Copy matching portion of old weights\n",
    "\n",
    "        #                     param.data[old_param.size(0):].copy_(old_param)\n",
    "        #                 else:\n",
    "        #                     print(f\"bias in name BUT equal size(0)\")\n",
    "        #                     print(f\"Name: {name}, extended:{param.shape}, original: {old_param.shape}\")\n",
    "\n",
    "        #                     param.data.copy_(old_param)\n",
    "\n",
    "        #     print(\"end iteration\")\n",
    "\n",
    "                    \n",
    "                    # if param.size(0) != old_param.size(0) and \"weight\" in name:  # Mismatch in dimension 0: increasing neurons\n",
    "                    #     # Copy matching portion of old weights\n",
    "                    #     print(f\"Name: {name}, extended:{param.shape}, original: {old_param.shape}\")\n",
    "                    #     param.data[:old_param.size(0), :].copy_(old_param)\n",
    "                    # if param.size(0) != old_param.size(0) and \"bias\" in name:  # Mismatch in dimension 0: increasing neurons\n",
    "                    #     print(f\"Name: {name}, extended:{param.shape}, original: {old_param.shape}\")\n",
    "                    #     # Copy matching portion of old weights\n",
    "                    #     param.data[:old_param.size(0)].copy_(old_param)\n",
    "                    #     # The rest of the weight tensor has been initialized by initialize_parameters\n",
    "                    # if param.size(1) != old_param.size(1):\n",
    "                    #     param.data[:, :old_param.size(1)].copy_(old_param)\n",
    "                    #     print(f\"Name: {name}, extended:{param.shape}, original: {old_param.shape}\")\n",
    "                    # print(\"end iteration\")                    \n",
    "                    # if param.size(0) != old_param.size(0):# in \"weight\" in name:  # Mismatch in dimension 0: increasing neurons\n",
    "                    #     # Copy matching portion of old weights\n",
    "                    #     print(f\"Name: {name}, extended:{param.shape}, original: {old_param.shape}\")\n",
    "                    #     param.data[:old_param.size(0):, :].copy_(old_param)\n",
    "                    # # if param.size(0) != old_param.size(0) in \"bias\" in name:  # Mismatch in dimension 0: increasing neurons\n",
    "                    # #     # Copy matching portion of old weights\n",
    "                    # #     param.data[:old_param.size(0):, :].copy_(old_param)\n",
    "                    #     # The rest of the weight tensor has been initialized by initialize_parameters\n",
    "                    # if param.size(1) != old_param.size(1):\n",
    "                    #     param.data[:, :old_param.size(1)].copy_(old_param)\n",
    "\n",
    "\n",
    "            \n",
    "            #: and 'weight' in name:  # Check for matching weight parameters\n",
    "            # if name in self.original_state_dict and 'weight' in name:  # Check for matching weight parameters\n",
    "            #     old_param_weights = self.original_state_dict[name]\n",
    "            #     if param.size(0) != old_param_weights.size(0):  # Mismatch in dimension 0 for weights\n",
    "            #         # Copy matching portion of old weights\n",
    "            #         param.data[:old_param_weights.size(0)].copy_(old_param_weights)\n",
    "            #         # The rest of the weight tensor has been initialized by initialize_parameters\n",
    "            #     else:\n",
    "            #         # Direct copy if sizes match\n",
    "            #         print(param.shape)\n",
    "            #         param.data.copy_(old_param_weights)\n",
    "            # elif name in self.original_state_dict and 'bias' in name:  # Directly copy biases if present\n",
    "            #     old_param_biases = self.original_state_dict[name]\n",
    "            #     if param.size(0) != old_param_biases.size(0):  # Mismatch in dimension 1 for biases\n",
    "            #         # Copy matching portion of old biases\n",
    "            #         param.data[:old_param_biases.size(0)].copy_(old_param_biases)\n",
    "            #         # The rest of the biases tensor has been initialized by initialize_parameters\n",
    "            #     else:\n",
    "            #         # Direct copy if sizes match\n",
    "            #         param.data.copy_(old_param_biases)\n",
    "\n",
    "    \n",
    "    def load_original_state_dict(self):\n",
    "        if self.original_model_path is None:\n",
    "            raise ValueError(\"Path to the original model checkpoint is not provided.\")\n",
    "\n",
    "        if not os.path.exists(self.original_model_path):\n",
    "            raise FileNotFoundError(f\"Provided path '{self.original_model_path}' does not exist.\")\n",
    "\n",
    "        if not os.path.isfile(self.original_model_path):\n",
    "            raise ValueError(f\"Provided path '{self.original_model_path}' is not a file.\")\n",
    "\n",
    "        _, ext = os.path.splitext(self.original_model_path)\n",
    "        if ext not in ['.pt', '.pth']:\n",
    "            raise ValueError(\"Provided file is not a valid checkpoint file.\")\n",
    "        \n",
    "        # Load weights and biases from original_model\n",
    "        return torch.load(self.original_model_path)['model_state_dict']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class TrainModel:\n",
    "    def __init__(self, model, num_epochs, save_interval, loss_threshold=None):\n",
    "        self.model = model\n",
    "        self.num_epochs = num_epochs\n",
    "        self.save_interval = save_interval\n",
    "        self.loss_threshold = loss_threshold\n",
    "\n",
    "    def train(self):\n",
    "        optimizer = optim.Adam(self.model.parameters())\n",
    "        criterion = nn.MSELoss()\n",
    "\n",
    "        for epoch in range(1, self.num_epochs + 1):\n",
    "            # Train the model\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            outputs = self.model(torch.randn(10, original_input_size))\n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, torch.randn(10, original_output_size))\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Save model checkpoint\n",
    "            if epoch % self.save_interval == 0:\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': self.model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': loss\n",
    "                }, f\"original_model_{epoch}.pt\")\n",
    "\n",
    "            # Check loss threshold\n",
    "            if self.loss_threshold is not None and loss.item() < self.loss_threshold:\n",
    "                print(f\"Loss threshold reached at epoch {epoch}. Stopping training.\")\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b6b963-09d3-434c-ba2e-0c499a33fda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage example\n",
    "original_input_size = 2\n",
    "original_hidden_layers = [4]\n",
    "original_output_size = 1\n",
    "original_model = FCN(original_input_size, original_hidden_layers, original_output_size, activation='Tanh', initialization='Xavier')\n",
    "original_model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c0b50b-6b6f-4f7b-b96a-3bc3e43e870d",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_model.plot_weights(figsize = (10,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4751f596-47d2-4ebe-8ec2-cb901dfd504f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train original model\n",
    "train_model = TrainModel(original_model, num_epochs=1000, save_interval=100, loss_threshold=0.01)\n",
    "train_model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5921546-fb93-4c09-9d9c-3aa45df3ddc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_model.plot_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768c8c86-fa7c-4476-8e76-23e276bbd7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240abfb4-05d3-4b28-b178-5a3c17873450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create extended model\n",
    "extended_input_size = 2\n",
    "extended_hidden_layers = [4,4]\n",
    "extended_output_size = 1\n",
    "extended_model = FCN_extended(extended_input_size, extended_hidden_layers, extended_output_size, activation='Tanh', initialization='Xavier', original_model_path='original_model_1000.pt')\n",
    "extended_model.plot_weights((15,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513f8bc5-00e3-4e5b-9244-bcac35bf0451",
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7167143a-60e7-4c69-8859-8377d133b0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train original model\n",
    "train_extended_model = TrainModel(extended_model, num_epochs=1000, save_interval=100, loss_threshold=0.01)\n",
    "train_extended_model.train()\n",
    "extended_model.plot_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2fe25b-8eb9-4f7c-9e4e-991b3a2aea64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load original model state and override weights\n",
    "#extended_model.load_override_original_weights_biases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4995617-0e1b-4ae9-a911-72912a7ac308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extend neurons in hidden layers\n",
    "#extended_model.extend_neurons(num_neurons=8)\n",
    "#extended_model.plot_weights((15,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ac7a3c-c2b5-432a-ad3d-24eab392d447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create extended model\n",
    "extended1_input_size = 2\n",
    "extended1_hidden_layers = [8,8]\n",
    "extended1_output_size = 1\n",
    "extended1_model = FCN_extended(extended1_input_size, extended1_hidden_layers, extended1_output_size, activation='Tanh', initialization='Xavier', original_model_path='original_model_1000.pt')\n",
    "extended1_model.plot_weights((15,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3542a78f-a754-4d56-a056-c71cca91ed34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train original model\n",
    "train_extended1_model = TrainModel(extended1_model, num_epochs=1000, save_interval=100, loss_threshold=0.01)\n",
    "train_extended1_model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397acfc9-67c9-4c0f-889b-625b198c1bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "extended1_model.plot_weights(figsize = (15,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5930132-a188-48ad-a309-5f9c668b07ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create extended model\n",
    "extended2_input_size = 2\n",
    "extended2_hidden_layers = [16,16,16,16]\n",
    "extended2_output_size = 1\n",
    "extended2_model = FCN_extended(extended2_input_size, extended2_hidden_layers, extended2_output_size, activation='Tanh', initialization='Xavier', original_model_path='original_model_1000.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765b1c04-3b6a-4dea-b5d9-f1928e4757c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "extended2_model.plot_weights((40,20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9cb00b-5f6f-4300-b4f0-d26366e124d5",
   "metadata": {},
   "source": [
    "## Others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8f9582-03b1-457e-86eb-f2af1f18069e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267ad520-0a86-4253-982d-9b861eaf85f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(original_model.fch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b61696-2035-483f-bfb8-34d0f1a45b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if fch is not empty\n",
    "if original_model.fch:\n",
    "    # Print the length of fch\n",
    "    print(\"Number of modules in fch:\", len(original_model.fch))\n",
    "    \n",
    "    # Loop through each module in fch\n",
    "    for i, module in enumerate(original_model.fch):\n",
    "        print(f\"Module {i}:\")\n",
    "        # Check if the module is not empty\n",
    "        if module:\n",
    "            # Print the number of layers in the module\n",
    "            print(\"  Number of layers:\", len(module))\n",
    "        else:\n",
    "            print(\"  Empty module\")\n",
    "else:\n",
    "    print(\"fch is empty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cf8f82-4cc4-4c8a-bb09-bfdc7100a04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_INPUT= 2\n",
    "hidden_layers = [4,4]\n",
    "for i , hidden_size in enumerate(hidden_layers[:]):\n",
    "    print(f\"nn.Linear(n: {i}, size: {hidden_size}, lenght of hidden_layers: {len(hidden_layers)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5e9c04-312f-4700-b782-53a85f34af95",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_INPUT = 2\n",
    "hidden_layers = [4,4]\n",
    "for i, hidden_size in enumerate(hidden_layers[:]):\n",
    "    input_size = N_INPUT if i == 0 else hidden_size\n",
    "    output_size = hidden_size\n",
    "    print( f\" nn.Linear({input_size}, {output_size})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2741ff7b-4a91-42b8-bbcd-898ad134c96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_layers = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(input_size if i == 0 else hidden_sizes[i-1], hidden_size),\n",
    "                nn.ReLU()\n",
    "            ) for i, hidden_size in enumerate(hidden_sizes)\n",
    "        ])\n",
    "\n",
    "        self.output_layer = nn.Linear(hidden_sizes[-1], output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.hidden_layers:\n",
    "            x = layer(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "# Example usage\n",
    "input_size = 2\n",
    "hidden_sizes = [4,4]\n",
    "output_size = 1\n",
    "\n",
    "model = SimpleModel(input_size, hidden_sizes, output_size)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae2d230-2413-4c0e-8ff4-68be701e1dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4f1d9b-315a-425b-b4e0-3079be875793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the state_dict without initializing the model first\n",
    "#loaded_state_dict = torch.load('simple_model_state_dict.pth')\n",
    "\n",
    "# Infer the length of hidden layers\n",
    "# Keys will be in the format of 'hidden_layers.0.0.weight', 'hidden_layers.1.0.weight', etc.\n",
    "hidden_layers_keys = [key for key in model.state_dict().keys() if 'hidden_layers' in key and  'weight' in key]\n",
    "hidden_layers_count = len(set([key.split('.')[1] for key in hidden_layers_keys]))\n",
    "#hidden_layers_count = set([key.split('.')[-1] for key in hidden_layers_keys])\n",
    "\n",
    "#hidden_layers_count = [key.split('.')[1] for key in hidden_layers_keys]\n",
    "\n",
    "print(f\"Length of hidden layers: {hidden_layers_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c224f3cf-9e02-458d-9eb1-3e59d52a54eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(hidden_layers_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1eafdf-961c-48b6-9f4e-f26a51b2bba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921e7240-2fa6-4da9-ab7d-5e63a291bddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_keys = list(state_dict.keys())\n",
    "layer_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1bf50e-aa33-48c7-a709-0a9a2ba33e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_keys[-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14ecfed-86bb-4a58-9dc0-7b1b70f3079a",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict[layer_keys[-4]].size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ab3513-a727-4814-a0cd-92dde60e9040",
   "metadata": {},
   "outputs": [],
   "source": [
    "visitor_ids = ['123', '356', '123', '501', '356', '123', '501', '789', '356']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ec4334-930a-4c7e-be98-0b27e9657893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list of visitor IDs to a set to remove duplicates\n",
    "unique_visitors = set(visitor_ids)\n",
    "\n",
    "# Now, unique_visitors contains only unique IDs\n",
    "print(f\"Unique visitor IDs: {unique_visitors}\")\n",
    "\n",
    "# The number of unique visitors\n",
    "print(f\"Number of unique visitors: {len(unique_visitors)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57eb37a-779b-4b31-8e1f-b51de0bb2990",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the neural network architecture\n",
    "class MyNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyNN, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(2, 4),  # Input layer to hidden layer\n",
    "            nn.Sigmoid(),     # Apply activation function (e.g., sigmoid)\n",
    "            nn.Linear(4, 1)   # Hidden layer to output layer\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Instantiate the neural network model\n",
    "model = MyNN()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e6e318-598a-43ef-8ad7-ecfcf0114b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fa47b4-c8f5-449a-ad42-cc222af59c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the state dictionary of the model\n",
    "state_dict = model.state_dict()\n",
    "\n",
    "import numpy \n",
    "# Access the weights and biases\n",
    "weights_hidden = state_dict['model.0.weight'].numpy()\n",
    "biases_hidden = state_dict['model.0.bias'].numpy()\n",
    "weights_output = state_dict['model.1.weight'].numpy()\n",
    "biases_output = state_dict['model.1.bias'].numpy()\n",
    "\n",
    "# Print weights and biases\n",
    "print(\"Weights of Hidden Layer:\")\n",
    "print(weights_hidden)\n",
    "print(\"\\nBiases of Hidden Layer:\")\n",
    "print(biases_hidden)\n",
    "print(\"\\nWeights of Output Layer:\")\n",
    "print(weights_output)\n",
    "print(\"\\nBiases of Output Layer:\")\n",
    "print(biases_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba2e6af-df2c-4dd6-8f88-a89352f975c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d256c5-56c5-4196-a2c8-cdc39d0ddb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class FlexibleNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size):\n",
    "        super(FlexibleNN, self).__init__()\n",
    "        layers = []\n",
    "\n",
    "        # Input layer to first hidden layer\n",
    "        layers.append(nn.Linear(input_size, hidden_sizes[0]))\n",
    "        layers.append(nn.Tanh())\n",
    "\n",
    "        # Adding variable number of hidden layers\n",
    "        for i in range(1, len(hidden_sizes)):\n",
    "            layers.append(nn.Linear(hidden_sizes[i-1], hidden_sizes[i]))\n",
    "            layers.append(nn.Tanh())\n",
    "\n",
    "        # Adding the final output layer\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        self.output_layer = nn.Linear(hidden_sizes[-1], output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "# Example usage\n",
    "input_size = 2  # Input features\n",
    "hidden_sizes = [4, 3, 2]  # Sizes of hidden layers\n",
    "output_size = 1  # Output features\n",
    "\n",
    "# Create an instance of the network with the specified architecture\n",
    "net = FlexibleNN(input_size, hidden_sizes, output_size)\n",
    "\n",
    "# Example input\n",
    "example_input = torch.rand(1, input_size)\n",
    "\n",
    "# Forward pass through the network\n",
    "output = net(example_input)\n",
    "\n",
    "print(\"Output:\", output)\n",
    "\n",
    "# Displaying the weights and biases\n",
    "for name, param in net.named_parameters():\n",
    "    print(name, param.data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fcba57-f254-48dc-8a21-b6ad68683f58",
   "metadata": {},
   "source": [
    "# Variance of the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec76e7a-8605-46cd-b761-8a3ad38fcd51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe418c5-24ba-47da-a5ba-55fb88b9ead3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.init as init\n",
    "\n",
    "# Function to initialize tensors and calculate variance\n",
    "def initialize_and_calculate_variance(initialization_fn):\n",
    "    # Initialize tensor\n",
    "    tensor = torch.empty(8, 2)\n",
    "    initialization_fn(tensor)\n",
    "\n",
    "    # Calculate variance of the tensor\n",
    "    variance = torch.var(tensor)\n",
    "\n",
    "    return tensor, variance.item()\n",
    "\n",
    "# Function to trim tensors and calculate variance\n",
    "def trim_and_calculate_variance(tensor):\n",
    "    # Trim tensor to last 4 rows\n",
    "    trimmed_tensor = tensor[-4:, :]\n",
    "\n",
    "    # Calculate variance of the trimmed tensor\n",
    "    variance = torch.var(trimmed_tensor)\n",
    "\n",
    "    return trimmed_tensor, variance.item()\n",
    "\n",
    "# List of initialization methods\n",
    "initialization_methods = [\n",
    "    init.uniform_,\n",
    "    init.normal_,\n",
    "    init.xavier_uniform_,\n",
    "    init.orthogonal_,\n",
    "    init.kaiming_uniform_\n",
    "]\n",
    "\n",
    "# Initialize and calculate variance for each method\n",
    "initial_tensors = []\n",
    "initial_variances = []\n",
    "trimmed_tensors = []\n",
    "trimmed_variances = []\n",
    "\n",
    "for initialization_method in initialization_methods:\n",
    "    # Initialize tensor and calculate variance\n",
    "    tensor, variance = initialize_and_calculate_variance(initialization_method)\n",
    "    initial_tensors.append(tensor)\n",
    "    initial_variances.append(variance)\n",
    "\n",
    "    # Trim tensor and calculate variance\n",
    "    trimmed_tensor, trimmed_variance = trim_and_calculate_variance(tensor)\n",
    "    trimmed_tensors.append(trimmed_tensor)\n",
    "    trimmed_variances.append(trimmed_variance)\n",
    "\n",
    "# Print initial variances\n",
    "print(\"Initial variances:\")\n",
    "for method, variance in zip(initialization_methods, initial_variances):\n",
    "    print(f\"{method.__name__}: {variance:.4f}\")\n",
    "\n",
    "# Print trimmed variances\n",
    "print(\"\\nTrimmed variances:\")\n",
    "for method, variance in zip(initialization_methods, trimmed_variances):\n",
    "    print(f\"{method.__name__}: {variance:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3c11b6-48ee-4352-84a2-1533f7d8e156",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "\n",
    "# Function to initialize tensors and calculate variance\n",
    "def initialize_and_calculate_variance(initialization_fn):\n",
    "    # Initialize tensor\n",
    "    tensor = torch.empty(8, 2)\n",
    "    initialization_fn(tensor)\n",
    "\n",
    "    # Apply Tanh activation function\n",
    "    tensor = F.tanh(tensor)\n",
    "\n",
    "    # Calculate variance of the tensor\n",
    "    variance = torch.var(tensor)\n",
    "\n",
    "    return tensor, variance.item()\n",
    "\n",
    "# Function to trim tensors and calculate variance\n",
    "def trim_and_calculate_variance(tensor):\n",
    "    # Trim tensor to last 4 rows\n",
    "    trimmed_tensor = tensor[-4:, :]\n",
    "\n",
    "    # Calculate variance of the trimmed tensor\n",
    "    variance = torch.var(trimmed_tensor)\n",
    "\n",
    "    return trimmed_tensor, variance.item()\n",
    "\n",
    "# List of initialization methods\n",
    "initialization_methods = [\n",
    "    init.uniform_,\n",
    "    init.normal_,\n",
    "    init.xavier_uniform_,\n",
    "    init.orthogonal_,\n",
    "    init.kaiming_uniform_\n",
    "]\n",
    "\n",
    "# Initialize and calculate variance for each method\n",
    "initial_tensors = []\n",
    "initial_variances = []\n",
    "trimmed_tensors = []\n",
    "trimmed_variances = []\n",
    "\n",
    "for initialization_method in initialization_methods:\n",
    "    # Initialize tensor and calculate variance\n",
    "    tensor, variance = initialize_and_calculate_variance(initialization_method)\n",
    "    initial_tensors.append(tensor)\n",
    "    initial_variances.append(variance)\n",
    "\n",
    "    # Trim tensor and calculate variance\n",
    "    trimmed_tensor, trimmed_variance = trim_and_calculate_variance(tensor)\n",
    "    trimmed_tensors.append(trimmed_tensor)\n",
    "    trimmed_variances.append(trimmed_variance)\n",
    "\n",
    "# Print initial variances\n",
    "print(\"Initial variances (with Tanh activation):\")\n",
    "for method, variance in zip(initialization_methods, initial_variances):\n",
    "    print(f\"{method.__name__}: {variance:.4f}\")\n",
    "\n",
    "# Print trimmed variances\n",
    "print(\"\\nTrimmed variances (with Tanh activation):\")\n",
    "for method, variance in zip(initialization_methods, trimmed_variances):\n",
    "    print(f\"{method.__name__}: {variance:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b82d527-e46c-45a7-8620-217cf1958c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    if i % 2 == 0:\n",
    "        continue  # Skip even numbers\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80eb7999-05ab-476d-a148-58cb299ecd4e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-26T20:59:06.097861Z",
     "iopub.status.busy": "2024-02-26T20:59:06.097475Z",
     "iopub.status.idle": "2024-02-26T20:59:07.162298Z",
     "shell.execute_reply": "2024-02-26T20:59:07.160787Z",
     "shell.execute_reply.started": "2024-02-26T20:59:06.097826Z"
    }
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Failed to import any of the following Qt binding modules: PyQt5, PySide2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Switch to an interactive backend (e.g., 'qt5')\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_line_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmatplotlib\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mqt5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Your plotting code\u001b[39;00m\n\u001b[1;32m     10\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:2432\u001b[0m, in \u001b[0;36mInteractiveShell.run_line_magic\u001b[0;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[1;32m   2430\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocal_ns\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_local_scope(stack_depth)\n\u001b[1;32m   2431\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[0;32m-> 2432\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2434\u001b[0m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[1;32m   2435\u001b[0m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[1;32m   2436\u001b[0m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[1;32m   2437\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic\u001b[38;5;241m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/IPython/core/magics/pylab.py:99\u001b[0m, in \u001b[0;36mPylabMagics.matplotlib\u001b[0;34m(self, line)\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAvailable matplotlib backends: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m backends_list)\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 99\u001b[0m     gui, backend \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshell\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menable_matplotlib\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgui\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgui\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgui\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_show_matplotlib_backend(args\u001b[38;5;241m.\u001b[39mgui, backend)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3621\u001b[0m, in \u001b[0;36mInteractiveShell.enable_matplotlib\u001b[0;34m(self, gui)\u001b[0m\n\u001b[1;32m   3617\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWarning: Cannot change to a different GUI toolkit: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   3618\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m Using \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (gui, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpylab_gui_select))\n\u001b[1;32m   3619\u001b[0m         gui, backend \u001b[38;5;241m=\u001b[39m pt\u001b[38;5;241m.\u001b[39mfind_gui_and_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpylab_gui_select)\n\u001b[0;32m-> 3621\u001b[0m \u001b[43mpt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactivate_matplotlib\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3622\u001b[0m configure_inline_support(\u001b[38;5;28mself\u001b[39m, backend)\n\u001b[1;32m   3624\u001b[0m \u001b[38;5;66;03m# Now we must activate the gui pylab wants to use, and fix %run to take\u001b[39;00m\n\u001b[1;32m   3625\u001b[0m \u001b[38;5;66;03m# plot updates into account\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/IPython/core/pylabtools.py:368\u001b[0m, in \u001b[0;36mactivate_matplotlib\u001b[0;34m(backend)\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;66;03m# Due to circular imports, pyplot may be only partially initialised\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;66;03m# when this function runs.\u001b[39;00m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;66;03m# So avoid needing matplotlib attribute-lookup to access pyplot.\u001b[39;00m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pyplot \u001b[38;5;28;01mas\u001b[39;00m plt\n\u001b[0;32m--> 368\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mswitch_backend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    370\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow\u001b[38;5;241m.\u001b[39m_needmain \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;66;03m# We need to detect at runtime whether show() is called by the user.\u001b[39;00m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;66;03m# For this, we wrap it into a decorator which adds a 'called' flag.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/matplotlib/pyplot.py:342\u001b[0m, in \u001b[0;36mswitch_backend\u001b[0;34m(newbackend)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;66;03m# have to escape the switch on access logic\u001b[39;00m\n\u001b[1;32m    340\u001b[0m old_backend \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(rcParams, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbackend\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 342\u001b[0m module \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcbook\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend_module_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnewbackend\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    343\u001b[0m canvas_class \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39mFigureCanvas\n\u001b[1;32m    345\u001b[0m required_framework \u001b[38;5;241m=\u001b[39m canvas_class\u001b[38;5;241m.\u001b[39mrequired_interactive_framework\n",
      "File \u001b[0;32m/usr/lib/python3.10/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:883\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/matplotlib/backends/backend_qt5agg.py:7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backends\n\u001b[1;32m      6\u001b[0m backends\u001b[38;5;241m.\u001b[39m_QT_FORCE_QT5_BINDING \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend_qtagg\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (    \u001b[38;5;66;03m# noqa: F401, E402 # pylint: disable=W0611\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     _BackendQTAgg, FigureCanvasQTAgg, FigureManagerQT, NavigationToolbar2QT,\n\u001b[1;32m      9\u001b[0m     FigureCanvasAgg, FigureCanvasQT)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;129m@_BackendQTAgg\u001b[39m\u001b[38;5;241m.\u001b[39mexport\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01m_BackendQT5Agg\u001b[39;00m(_BackendQTAgg):\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/matplotlib/backends/backend_qtagg.py:9\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mctypes\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Bbox\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mqt_compat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m QT_API, QtCore, QtGui\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend_agg\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FigureCanvasAgg\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend_qt\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _BackendQT, FigureCanvasQT\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/matplotlib/backends/qt_compat.py:133\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 133\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m    134\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import any of the following Qt binding modules: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    135\u001b[0m             \u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([QT_API \u001b[38;5;28;01mfor\u001b[39;00m _, QT_API \u001b[38;5;129;01min\u001b[39;00m _candidates]))\n\u001b[1;32m    136\u001b[0m         )\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# We should not get there.\u001b[39;00m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected QT_API: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mQT_API\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mImportError\u001b[0m: Failed to import any of the following Qt binding modules: PyQt5, PySide2"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Switch to an interactive backend (e.g., 'qt5')\n",
    "%matplotlib qt5\n",
    "\n",
    "# Your plotting code\n",
    "plt.figure()\n",
    "t = np.arange(0.0, 2.0, 0.01)\n",
    "s = 1 + np.sin(2 * np.pi * t)\n",
    "plt.plot(t, s)\n",
    "\n",
    "plt.xlabel('time (s)')\n",
    "plt.ylabel('voltage (mV)')\n",
    "plt.title('About as simple as it gets, folks')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb73b6b-fc6d-4590-acb6-6fd4a069890c",
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.get_backend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cba6e0-a645-412a-b5cc-3b876bf2c643",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install PyQt5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "972daa6c-75c8-44fa-9276-d81273efc6eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-26T21:00:34.088443Z",
     "iopub.status.busy": "2024-02-26T21:00:34.087988Z",
     "iopub.status.idle": "2024-02-26T21:00:36.625925Z",
     "shell.execute_reply": "2024-02-26T21:00:36.624113Z",
     "shell.execute_reply.started": "2024-02-26T21:00:34.088408Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: PyQt5 in /home/luis/.local/lib/python3.10/site-packages (5.15.10)\n",
      "Requirement already satisfied: PyQt5-sip<13,>=12.13 in /home/luis/.local/lib/python3.10/site-packages (from PyQt5) (12.13.0)\n",
      "Requirement already satisfied: PyQt5-Qt5>=5.15.2 in /home/luis/.local/lib/python3.10/site-packages (from PyQt5) (5.15.2)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: PySide2 in /home/luis/.local/lib/python3.10/site-packages (5.15.2.1)\n",
      "Requirement already satisfied: shiboken2==5.15.2.1 in /home/luis/.local/lib/python3.10/site-packages (from PySide2) (5.15.2.1)\n"
     ]
    }
   ],
   "source": [
    "# Install PyQt5\n",
    "!pip install PyQt5\n",
    "# or install PySide2\n",
    "!pip install PySide2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e7d24d5-4db1-489c-b589-6044f5aa0f96",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-26T21:02:35.067960Z",
     "iopub.status.busy": "2024-02-26T21:02:35.067457Z",
     "iopub.status.idle": "2024-02-26T21:02:35.075202Z",
     "shell.execute_reply": "2024-02-26T21:02:35.073941Z",
     "shell.execute_reply.started": "2024-02-26T21:02:35.067921Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fb473fe-c2ad-453f-8c1c-55fcd03db383",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-26T21:02:48.620284Z",
     "iopub.status.busy": "2024-02-26T21:02:48.619856Z",
     "iopub.status.idle": "2024-02-26T21:02:48.627486Z",
     "shell.execute_reply": "2024-02-26T21:02:48.625853Z",
     "shell.execute_reply.started": "2024-02-26T21:02:48.620249Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/python3\n"
     ]
    }
   ],
   "source": [
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b373948-6fde-435a-853e-8926eac3a0f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-26T21:03:15.978986Z",
     "iopub.status.busy": "2024-02-26T21:03:15.978549Z",
     "iopub.status.idle": "2024-02-26T21:03:16.017099Z",
     "shell.execute_reply": "2024-02-26T21:03:16.015371Z",
     "shell.execute_reply.started": "2024-02-26T21:03:15.978951Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'PyQt5' has no attribute '__version__'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mPyQt5\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mPyQt5\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__version__\u001b[49m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'PyQt5' has no attribute '__version__'"
     ]
    }
   ],
   "source": [
    "import PyQt5\n",
    "print(PyQt5.__version__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
